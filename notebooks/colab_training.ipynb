{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IntelHealth - Colab Training (Per-Agent)\n",
    "\n",
    "This notebook is designed for **one agent per run**.\n",
    "Pick `AGENT_NAME`, then run all cells.\n",
    "\n",
    "All agents use `supervised_finetuning.py` with Qwen ChatML template.\n",
    "- 4 grader/normalizer agents → Qwen3-0.5B-Instruct (loss on response only)\n",
    "- diagnosis_generator → Qwen3-1.8B-Instruct (loss on all tokens via `--train_on_inputs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zfzgyuh4yie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo + pull latest\n",
    "import os\n",
    "repo_dir = '/content/Intel_Health'\n",
    "if not os.path.exists(repo_dir):\n",
    "    !git clone https://github.com/DemonRain7/Intel_Health.git {repo_dir}\n",
    "else:\n",
    "    !git -C {repo_dir} pull\n",
    "%cd {repo_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip -q install torch transformers datasets peft accelerate bitsandbytes sentencepiece loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drive_training_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and set persistent paths\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/Code_Project/IntelHealth'\n",
    "SFT_DATA_ROOT = f'{DRIVE_ROOT}/datasets/agent_sft'\n",
    "ADAPTER_OUTPUT_ROOT = f'{DRIVE_ROOT}/models/adapters'\n",
    "os.makedirs(SFT_DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(ADAPTER_OUTPUT_ROOT, exist_ok=True)\n",
    "print('SFT_DATA_ROOT:', SFT_DATA_ROOT)\n",
    "print('ADAPTER_OUTPUT_ROOT:', ADAPTER_OUTPUT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a252b",
   "metadata": {},
   "source": [
    "## 1) Select Agent (single run)\n",
    "\n",
    "Supported training agents:\n",
    "- symptom_normalizer (SFT, 0.5B)\n",
    "- symptom_quality_grader (SFT, 0.5B)\n",
    "- rag_relevance_grader (SFT, 0.5B)\n",
    "- drug_evidence_grader (SFT, 0.5B)\n",
    "- diagnosis_generator (SFT + train_on_inputs, 1.8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec49031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shlex\n",
    "from pathlib import Path\n",
    "\n",
    "AGENT_NAME = \"symptom_quality_grader\"  # <-- change this each run\n",
    "\n",
    "MODEL_BY_AGENT = {\n",
    "    \"symptom_normalizer\":     \"Qwen/Qwen3-0.5B-Instruct\",\n",
    "    \"symptom_quality_grader\": \"Qwen/Qwen3-0.5B-Instruct\",\n",
    "    \"rag_relevance_grader\":   \"Qwen/Qwen3-0.5B-Instruct\",\n",
    "    \"drug_evidence_grader\":   \"Qwen/Qwen3-0.5B-Instruct\",\n",
    "    \"diagnosis_generator\":    \"Qwen/Qwen3-1.8B-Instruct\",\n",
    "}\n",
    "\n",
    "# --train_file_dir expects a DIRECTORY (globs *.jsonl inside)\n",
    "DATA_DIR_BY_AGENT = {\n",
    "    \"symptom_normalizer\":     f\"{SFT_DATA_ROOT}/symptom_normalizer\",\n",
    "    \"symptom_quality_grader\": f\"{SFT_DATA_ROOT}/symptom_quality_grader\",\n",
    "    \"rag_relevance_grader\":   f\"{SFT_DATA_ROOT}/rag_relevance_grader\",\n",
    "    \"drug_evidence_grader\":   f\"{SFT_DATA_ROOT}/drug_evidence_grader\",\n",
    "    \"diagnosis_generator\":    f\"{SFT_DATA_ROOT}/diagnosis_generator\",\n",
    "}\n",
    "\n",
    "OUTPUT_BY_AGENT = {\n",
    "    \"symptom_normalizer\":     f\"{ADAPTER_OUTPUT_ROOT}/symptom_normalizer\",\n",
    "    \"symptom_quality_grader\": f\"{ADAPTER_OUTPUT_ROOT}/symptom_quality_grader\",\n",
    "    \"rag_relevance_grader\":   f\"{ADAPTER_OUTPUT_ROOT}/rag_relevance_grader\",\n",
    "    \"drug_evidence_grader\":   f\"{ADAPTER_OUTPUT_ROOT}/drug_evidence_grader\",\n",
    "    \"diagnosis_generator\":    f\"{ADAPTER_OUTPUT_ROOT}/diagnosis_generator\",\n",
    "}\n",
    "\n",
    "assert AGENT_NAME in MODEL_BY_AGENT, f\"Unknown agent: {AGENT_NAME}\"\n",
    "\n",
    "MODEL_NAME = MODEL_BY_AGENT[AGENT_NAME]\n",
    "TRAIN_DIR  = DATA_DIR_BY_AGENT[AGENT_NAME]\n",
    "OUTPUT_DIR = OUTPUT_BY_AGENT[AGENT_NAME]\n",
    "\n",
    "print(f\"AGENT_NAME: {AGENT_NAME}\")\n",
    "print(f\"MODEL_NAME: {MODEL_NAME}\")\n",
    "print(f\"TRAIN_DIR:  {TRAIN_DIR}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c6614",
   "metadata": {},
   "source": [
    "## 2) Build command\n",
    "\n",
    "All agents use `supervised_finetuning.py` with Qwen ChatML template.\n",
    "- `diagnosis_generator` adds `--train_on_inputs` (loss on all tokens, DAPT-like behavior)\n",
    "- Others only compute loss on model response tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SFT_SCRIPT = Path(\"training/supervised_finetuning.py\")\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "EPOCHS     = 3\n",
    "LR         = \"2e-4\"\n",
    "BATCH      = \"2\"\n",
    "GRAD_ACC   = \"8\"           # effective batch = 2 * 8 = 16\n",
    "LORA_R     = \"64\"\n",
    "LORA_ALPHA = \"128\"         # scaling = alpha/rank = 2.0\n",
    "LORA_DROPOUT = \"0.05\"\n",
    "MAX_LEN    = \"512\"         # max sequence length\n",
    "\n",
    "cmd = [\n",
    "    \"python\", str(SFT_SCRIPT),\n",
    "    \"--model_name_or_path\",        MODEL_NAME,\n",
    "    \"--tokenizer_name_or_path\",    MODEL_NAME,\n",
    "    \"--train_file_dir\",            TRAIN_DIR,\n",
    "    \"--output_dir\",                OUTPUT_DIR,\n",
    "    \"--template_name\",             \"qwen\",\n",
    "    \"--do_train\",\n",
    "    \"--fp16\",\n",
    "    \"--gradient_checkpointing\",\n",
    "    \"--per_device_train_batch_size\", BATCH,\n",
    "    \"--gradient_accumulation_steps\", GRAD_ACC,\n",
    "    \"--num_train_epochs\",          str(1 if AGENT_NAME == \"diagnosis_generator\" else EPOCHS),\n",
    "    \"--learning_rate\",             LR,\n",
    "    \"--lora_rank\",                 LORA_R,\n",
    "    \"--lora_alpha\",                LORA_ALPHA,\n",
    "    \"--lora_dropout\",              LORA_DROPOUT,\n",
    "    \"--model_max_length\",          MAX_LEN,\n",
    "    \"--logging_steps\",             \"10\",\n",
    "    \"--save_strategy\",             \"epoch\",\n",
    "    \"--overwrite_output_dir\",\n",
    "]\n",
    "\n",
    "# diagnosis_generator: train on ALL tokens (including the question)\n",
    "if AGENT_NAME == \"diagnosis_generator\":\n",
    "    cmd.append(\"--train_on_inputs\")\n",
    "\n",
    "print(\"Command:\")\n",
    "print(\" \".join(shlex.quote(x) for x in cmd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b97eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ca9a5",
   "metadata": {},
   "source": [
    "## 3) Next agent\n",
    "\n",
    "After this run finishes:\n",
    "1. Change `AGENT_NAME` in cell 1\n",
    "2. **Runtime → Restart runtime** (free GPU memory)\n",
    "3. Run all cells again\n",
    "\n",
    "Trained LoRA adapters are saved to Google Drive: `models/adapters/<agent>/`\n",
    "\n",
    "### Recommended training order\n",
    "1. `symptom_normalizer` (0.5B, fast)\n",
    "2. `symptom_quality_grader` (0.5B, fast)\n",
    "3. `rag_relevance_grader` (0.5B, fast)\n",
    "4. `drug_evidence_grader` (0.5B, fast)\n",
    "5. `diagnosis_generator` (1.8B, slower)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
