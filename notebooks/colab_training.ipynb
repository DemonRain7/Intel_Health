{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IntelHealth - Colab Training (Per-Agent)\n",
    "\n",
    "This notebook is designed for **one agent per run**.\n",
    "Pick `AGENT_NAME`, then run all cells.\n",
    "\n",
    "All agents use `supervised_finetuning.py` with Qwen ChatML template.\n",
    "- 4 grader/normalizer agents → Qwen3-0.5B-Instruct (loss on response only)\n",
    "- diagnosis_generator → Qwen3-1.8B-Instruct (loss on all tokens via `--train_on_inputs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zfzgyuh4yie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo + pull latest\n",
    "import os\n",
    "repo_dir = '/content/Intel_Health'\n",
    "if not os.path.exists(repo_dir):\n",
    "    !git clone https://github.com/DemonRain7/Intel_Health.git {repo_dir}\n",
    "else:\n",
    "    !git -C {repo_dir} pull\n",
    "%cd {repo_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip -q install torch transformers datasets peft accelerate bitsandbytes sentencepiece loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drive_training_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and set persistent paths\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/Code_Project/IntelHealth'\n",
    "SFT_DATA_ROOT = f'{DRIVE_ROOT}/datasets/agent_sft'\n",
    "ADAPTER_OUTPUT_ROOT = f'{DRIVE_ROOT}/models/adapters'\n",
    "os.makedirs(SFT_DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(ADAPTER_OUTPUT_ROOT, exist_ok=True)\n",
    "print('SFT_DATA_ROOT:', SFT_DATA_ROOT)\n",
    "print('ADAPTER_OUTPUT_ROOT:', ADAPTER_OUTPUT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a252b",
   "metadata": {},
   "source": [
    "## 1) Select Agent (single run)\n",
    "\n",
    "Supported training agents:\n",
    "- symptom_normalizer (SFT, 0.5B)\n",
    "- symptom_quality_grader (SFT, 0.5B)\n",
    "- rag_relevance_grader (SFT, 0.5B)\n",
    "- drug_evidence_grader (SFT, 0.5B)\n",
    "- diagnosis_generator (SFT + train_on_inputs, 1.8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec49031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess, shlex\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "AGENT_NAME = \"symptom_quality_grader\"  # <-- change this each run\n",
    "\n",
    "MODEL_BY_AGENT = {\n",
    "    \"symptom_normalizer\":     \"Qwen/Qwen3-0.5B-Instruct\",\n",
    "    \"symptom_quality_grader\": \"Qwen/Qwen3-0.5B-Instruct\",\n",
    "    \"rag_relevance_grader\":   \"Qwen/Qwen3-0.5B-Instruct\",\n",
    "    \"drug_evidence_grader\":   \"Qwen/Qwen3-0.5B-Instruct\",\n",
    "    \"diagnosis_generator\":    \"Qwen/Qwen3-1.8B-Instruct\",\n",
    "}\n",
    "\n",
    "# --train_file_dir expects a DIRECTORY (globs *.jsonl inside)\n",
    "DATA_DIR_BY_AGENT = {\n",
    "    \"symptom_normalizer\":     f\"{SFT_DATA_ROOT}/symptom_normalizer\",\n",
    "    \"symptom_quality_grader\": f\"{SFT_DATA_ROOT}/symptom_quality_grader\",\n",
    "    \"rag_relevance_grader\":   f\"{SFT_DATA_ROOT}/rag_relevance_grader\",\n",
    "    \"drug_evidence_grader\":   f\"{SFT_DATA_ROOT}/drug_evidence_grader\",\n",
    "    \"diagnosis_generator\":    f\"{SFT_DATA_ROOT}/diagnosis_generator\",\n",
    "}\n",
    "\n",
    "# Keep each agent under its existing folder; create a new run subfolder each time\n",
    "OUTPUT_BY_AGENT = {\n",
    "    \"symptom_normalizer\":     f\"{ADAPTER_OUTPUT_ROOT}/symptom_normalizer\",\n",
    "    \"symptom_quality_grader\": f\"{ADAPTER_OUTPUT_ROOT}/symptom_quality_grader\",\n",
    "    \"rag_relevance_grader\":   f\"{ADAPTER_OUTPUT_ROOT}/rag_relevance_grader\",\n",
    "    \"drug_evidence_grader\":   f\"{ADAPTER_OUTPUT_ROOT}/drug_evidence_grader\",\n",
    "    \"diagnosis_generator\":    f\"{ADAPTER_OUTPUT_ROOT}/diagnosis_generator\",\n",
    "}\n",
    "\n",
    "assert AGENT_NAME in MODEL_BY_AGENT, f\"Unknown agent: {AGENT_NAME}\"\n",
    "\n",
    "# Ensure all agent output directories exist\n",
    "for _agent_dir in OUTPUT_BY_AGENT.values():\n",
    "    os.makedirs(_agent_dir, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = MODEL_BY_AGENT[AGENT_NAME]\n",
    "TRAIN_DIR  = DATA_DIR_BY_AGENT[AGENT_NAME]\n",
    "OUTPUT_BASE_DIR = OUTPUT_BY_AGENT[AGENT_NAME]\n",
    "RUN_NAME = datetime.now().strftime(\"run_%Y%m%d_%H%M%S_%f\")\n",
    "OUTPUT_DIR = f\"{OUTPUT_BASE_DIR}/{RUN_NAME}\"\n",
    "\n",
    "print(f\"AGENT_NAME: {AGENT_NAME}\")\n",
    "print(f\"MODEL_NAME: {MODEL_NAME}\")\n",
    "print(f\"TRAIN_DIR:  {TRAIN_DIR}\")\n",
    "print(f\"OUTPUT_BASE_DIR: {OUTPUT_BASE_DIR}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c6614",
   "metadata": {},
   "source": [
    "## 2) Build command\n",
    "\n",
    "All agents use `supervised_finetuning.py` with Qwen ChatML template.\n",
    "- `diagnosis_generator` adds `--train_on_inputs` (loss on all tokens, DAPT-like behavior)\n",
    "- Others only compute loss on model response tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SFT_SCRIPT = Path(\"training/supervised_finetuning.py\")\n",
    "\n",
    "# =============================================================\n",
    "# Per-agent hyperparameters (edit freely, auto-selected by AGENT_NAME)\n",
    "# =============================================================\n",
    "AGENT_HP = {\n",
    "    # DAPT: 1.8B model, 5000 MCQ samples, output=1 token\n",
    "    \"diagnosis_generator\": {\n",
    "        \"epochs\":     2,\n",
    "        \"lr\":         \"1e-4\",\n",
    "        \"lora_r\":     \"32\",\n",
    "        \"lora_alpha\": \"64\",\n",
    "        \"max_len\":    \"256\",\n",
    "    },\n",
    "    # SFT: 0.5B, 1500 samples, long input + long JSON output\n",
    "    \"symptom_normalizer\": {\n",
    "        \"epochs\":     3,\n",
    "        \"lr\":         \"2e-4\",\n",
    "        \"lora_r\":     \"64\",\n",
    "        \"lora_alpha\": \"128\",\n",
    "        \"max_len\":    \"512\",\n",
    "    },\n",
    "    # SFT: 0.5B, ~1500 samples, moderate input + short JSON output\n",
    "    \"symptom_quality_grader\": {\n",
    "        \"epochs\":     3,\n",
    "        \"lr\":         \"2e-4\",\n",
    "        \"lora_r\":     \"64\",\n",
    "        \"lora_alpha\": \"128\",\n",
    "        \"max_len\":    \"256\",\n",
    "    },\n",
    "    \"rag_relevance_grader\": {\n",
    "        \"epochs\":     3,\n",
    "        \"lr\":         \"2e-4\",\n",
    "        \"lora_r\":     \"64\",\n",
    "        \"lora_alpha\": \"128\",\n",
    "        \"max_len\":    \"256\",\n",
    "    },\n",
    "    \"drug_evidence_grader\": {\n",
    "        \"epochs\":     3,\n",
    "        \"lr\":         \"2e-4\",\n",
    "        \"lora_r\":     \"64\",\n",
    "        \"lora_alpha\": \"128\",\n",
    "        \"max_len\":    \"256\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# =============================================================\n",
    "# Shared hyperparameters (A100-40GB)\n",
    "# =============================================================\n",
    "BATCH        = \"8\"\n",
    "GRAD_ACC     = \"2\"\n",
    "LORA_DROPOUT = \"0.05\"\n",
    "\n",
    "hp = AGENT_HP[AGENT_NAME]\n",
    "\n",
    "cmd = [\n",
    "    \"python\", str(SFT_SCRIPT),\n",
    "    \"--model_name_or_path\",          MODEL_NAME,\n",
    "    \"--tokenizer_name_or_path\",      MODEL_NAME,\n",
    "    \"--train_file_dir\",              TRAIN_DIR,\n",
    "    \"--output_dir\",                  OUTPUT_DIR,\n",
    "    \"--template_name\",               \"qwen\",\n",
    "    \"--do_train\",\n",
    "    \"--fp16\",\n",
    "    \"--gradient_checkpointing\",\n",
    "    \"--per_device_train_batch_size\", BATCH,\n",
    "    \"--gradient_accumulation_steps\", GRAD_ACC,\n",
    "    \"--num_train_epochs\",            str(hp[\"epochs\"]),\n",
    "    \"--learning_rate\",               hp[\"lr\"],\n",
    "    \"--lora_rank\",                   hp[\"lora_r\"],\n",
    "    \"--lora_alpha\",                  hp[\"lora_alpha\"],\n",
    "    \"--lora_dropout\",                LORA_DROPOUT,\n",
    "    \"--model_max_length\",            hp[\"max_len\"],\n",
    "    \"--logging_steps\",               \"10\",\n",
    "    \"--save_strategy\",               \"epoch\",\n",
    "]\n",
    "\n",
    "# diagnosis_generator: loss on ALL tokens (DAPT-like)\n",
    "if AGENT_NAME == \"diagnosis_generator\":\n",
    "    cmd.append(\"--train_on_inputs\")\n",
    "\n",
    "print(f\"Agent: {AGENT_NAME}\")\n",
    "print(f\"Hyperparameters: {hp}\")\n",
    "print(f\"Batch: {BATCH} x Grad_Acc: {GRAD_ACC} = Effective: {int(BATCH) * int(GRAD_ACC)}\")\n",
    "print(f\"\\nCommand:\\n{' ' .join(shlex.quote(x) for x in cmd)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b97eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ca9a5",
   "metadata": {},
   "source": [
    "## 3) Next agent\n",
    "\n",
    "After this run finishes:\n",
    "1. Change `AGENT_NAME` in cell 1\n",
    "2. **Runtime → Restart runtime** (free GPU memory)\n",
    "3. Run all cells again\n",
    "\n",
    "Trained LoRA adapters are saved to Google Drive: `models/adapters/<agent>/run_YYYYmmdd_HHMMSS_ffffff/`\n",
    "\n",
    "### Recommended training order\n",
    "1. `symptom_normalizer` (0.5B, fast)\n",
    "2. `symptom_quality_grader` (0.5B, fast)\n",
    "3. `rag_relevance_grader` (0.5B, fast)\n",
    "4. `drug_evidence_grader` (0.5B, fast)\n",
    "5. `diagnosis_generator` (1.8B, slower)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}