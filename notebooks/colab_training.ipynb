{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# IntelHealth - Colab Training (Per-Agent)\n\nThis notebook is designed for **one agent per run**.\nPick `AGENT_NAME`, then run all cells.\n\nAll agents use `supervised_finetuning.py` with Qwen ChatML template.\n- 4 grader/normalizer agents → Qwen3-0.6B (loss on response only)\n- diagnosis_generator → Qwen3-1.7B (loss on all tokens via `--train_on_inputs`)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zfzgyuh4yie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo + pull latest\n",
    "import os\n",
    "repo_dir = '/content/Intel_Health'\n",
    "if not os.path.exists(repo_dir):\n",
    "    !git clone https://github.com/DemonRain7/Intel_Health.git {repo_dir}\n",
    "else:\n",
    "    !git -C {repo_dir} pull\n",
    "%cd {repo_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a189e0",
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies (need transformers>=4.46 for Qwen3 support)\n!pip -q install torch \"transformers>=4.46\" datasets peft accelerate bitsandbytes sentencepiece loguru\n\n# HuggingFace login (Qwen3 requires authentication)\n# Option 1: Colab Secrets (recommended, persistent across sessions)\n#   Click the key icon in the left sidebar -> Add secret:\n#   Name: HF_TOKEN    Value: your token from https://huggingface.co/settings/tokens\n#   Then toggle \"Notebook access\" ON\n# Option 2: Falls back to interactive login() if secret not found\nimport os\ntry:\n    from google.colab import userdata\n    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n    print(\"HF_TOKEN loaded from Colab Secrets\")\nexcept (ImportError, userdata.SecretNotFoundError):\n    from huggingface_hub import login\n    login()  # interactive fallback"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drive_training_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and set persistent paths\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/Code_Project/IntelHealth'\n",
    "SFT_DATA_ROOT = f'{DRIVE_ROOT}/datasets/agent_sft'\n",
    "ADAPTER_OUTPUT_ROOT = f'{DRIVE_ROOT}/models/adapters'\n",
    "os.makedirs(SFT_DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(ADAPTER_OUTPUT_ROOT, exist_ok=True)\n",
    "print('SFT_DATA_ROOT:', SFT_DATA_ROOT)\n",
    "print('ADAPTER_OUTPUT_ROOT:', ADAPTER_OUTPUT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a252b",
   "metadata": {},
   "source": "## 1) Select Agent (single run)\n\nSupported training agents:\n- symptom_normalizer (SFT, 0.6B)\n- symptom_quality_grader (SFT, 0.6B)\n- rag_relevance_grader (SFT, 0.6B)\n- drug_evidence_grader (SFT, 0.6B)\n- diagnosis_generator (SFT + train_on_inputs, 1.7B)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec49031",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport subprocess, shlex\nfrom pathlib import Path\nfrom datetime import datetime\n\nAGENT_NAME = \"symptom_quality_grader\"  # <-- change this each run\n\nMODEL_BY_AGENT = {\n    \"symptom_normalizer\":     \"Qwen/Qwen3-0.6B\",\n    \"symptom_quality_grader\": \"Qwen/Qwen3-0.6B\",\n    \"rag_relevance_grader\":   \"Qwen/Qwen3-0.6B\",\n    \"drug_evidence_grader\":   \"Qwen/Qwen3-0.6B\",\n    \"diagnosis_generator\":    \"Qwen/Qwen3-1.7B\",\n}\n\n# --train_file_dir expects a DIRECTORY (globs *.jsonl inside)\nDATA_DIR_BY_AGENT = {\n    \"symptom_normalizer\":     f\"{SFT_DATA_ROOT}/symptom_normalizer\",\n    \"symptom_quality_grader\": f\"{SFT_DATA_ROOT}/symptom_quality_grader\",\n    \"rag_relevance_grader\":   f\"{SFT_DATA_ROOT}/rag_relevance_grader\",\n    \"drug_evidence_grader\":   f\"{SFT_DATA_ROOT}/drug_evidence_grader\",\n    \"diagnosis_generator\":    f\"{SFT_DATA_ROOT}/diagnosis_generator\",\n}\n\nOUTPUT_BY_AGENT = {\n    \"symptom_normalizer\":     f\"{ADAPTER_OUTPUT_ROOT}/symptom_normalizer\",\n    \"symptom_quality_grader\": f\"{ADAPTER_OUTPUT_ROOT}/symptom_quality_grader\",\n    \"rag_relevance_grader\":   f\"{ADAPTER_OUTPUT_ROOT}/rag_relevance_grader\",\n    \"drug_evidence_grader\":   f\"{ADAPTER_OUTPUT_ROOT}/drug_evidence_grader\",\n    \"diagnosis_generator\":    f\"{ADAPTER_OUTPUT_ROOT}/diagnosis_generator\",\n}\n\nassert AGENT_NAME in MODEL_BY_AGENT, f\"Unknown agent: {AGENT_NAME}\"\n\nfor _agent_dir in OUTPUT_BY_AGENT.values():\n    os.makedirs(_agent_dir, exist_ok=True)\n\nMODEL_NAME = MODEL_BY_AGENT[AGENT_NAME]\nTRAIN_DIR  = DATA_DIR_BY_AGENT[AGENT_NAME]\nOUTPUT_BASE_DIR = OUTPUT_BY_AGENT[AGENT_NAME]\nRUN_NAME = datetime.now().strftime(\"run_%Y%m%d_%H%M%S_%f\")\nOUTPUT_DIR = f\"{OUTPUT_BASE_DIR}/{RUN_NAME}\"\n\nprint(f\"AGENT_NAME: {AGENT_NAME}\")\nprint(f\"MODEL_NAME: {MODEL_NAME}\")\nprint(f\"TRAIN_DIR:  {TRAIN_DIR}\")\nprint(f\"OUTPUT_DIR: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "id": "b09c6614",
   "metadata": {},
   "source": [
    "## 2) Build command\n",
    "\n",
    "All agents use `supervised_finetuning.py` with Qwen ChatML template.\n",
    "- `diagnosis_generator` adds `--train_on_inputs` (loss on all tokens, DAPT-like behavior)\n",
    "- Others only compute loss on model response tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84a4d1",
   "metadata": {},
   "outputs": [],
   "source": "SFT_SCRIPT = Path(\"training/supervised_finetuning.py\")\n\n# =============================================================\n# Per-agent hyperparameters (edit freely, auto-selected by AGENT_NAME)\n# =============================================================\nAGENT_HP = {\n    # DAPT: 1.7B model, 5000 MCQ samples, output=1 token\n    \"diagnosis_generator\": {\n        \"epochs\":     2,\n        \"lr\":         \"1e-4\",\n        \"lora_r\":     \"32\",\n        \"lora_alpha\": \"64\",\n        \"max_len\":    \"256\",\n    },\n    # SFT: 0.6B, 1500 samples, long input + long JSON output\n    \"symptom_normalizer\": {\n        \"epochs\":     3,\n        \"lr\":         \"2e-4\",\n        \"lora_r\":     \"64\",\n        \"lora_alpha\": \"128\",\n        \"max_len\":    \"512\",\n    },\n    # SFT: 0.6B, ~1500 samples, moderate input + short JSON output\n    \"symptom_quality_grader\": {\n        \"epochs\":     3,\n        \"lr\":         \"2e-4\",\n        \"lora_r\":     \"64\",\n        \"lora_alpha\": \"128\",\n        \"max_len\":    \"256\",\n    },\n    \"rag_relevance_grader\": {\n        \"epochs\":     3,\n        \"lr\":         \"2e-4\",\n        \"lora_r\":     \"64\",\n        \"lora_alpha\": \"128\",\n        \"max_len\":    \"256\",\n    },\n    \"drug_evidence_grader\": {\n        \"epochs\":     3,\n        \"lr\":         \"2e-4\",\n        \"lora_r\":     \"64\",\n        \"lora_alpha\": \"128\",\n        \"max_len\":    \"256\",\n    },\n}\n\n# =============================================================\n# Shared hyperparameters (A100-40GB optimized)\n# =============================================================\nBATCH = \"8\"\nGRAD_ACC = \"2\"\nLORA_DROPOUT = \"0.05\"\n\nhp = AGENT_HP[AGENT_NAME]\n\ncmd = [\n    \"python\", str(SFT_SCRIPT),\n    \"--model_name_or_path\",          MODEL_NAME,\n    \"--tokenizer_name_or_path\",      MODEL_NAME,\n    \"--train_file_dir\",              TRAIN_DIR,\n    \"--output_dir\",                  OUTPUT_DIR,\n    \"--template_name\",               \"qwen\",\n    \"--do_train\",\n    \"--fp16\",\n    \"--gradient_checkpointing\",\n    \"--per_device_train_batch_size\", BATCH,\n    \"--gradient_accumulation_steps\", GRAD_ACC,\n    \"--num_train_epochs\",            str(hp[\"epochs\"]),\n    \"--learning_rate\",               hp[\"lr\"],\n    \"--lora_rank\",                   hp[\"lora_r\"],\n    \"--lora_alpha\",                  hp[\"lora_alpha\"],\n    \"--lora_dropout\",                LORA_DROPOUT,\n    \"--model_max_length\",            hp[\"max_len\"],\n    \"--logging_steps\",               \"10\",\n    \"--save_strategy\",               \"epoch\",\n]\n\n# diagnosis_generator: loss on ALL tokens (DAPT-like)\nif AGENT_NAME == \"diagnosis_generator\":\n    cmd.append(\"--train_on_inputs\")\n\nprint(f\"Agent: {AGENT_NAME}\")\nprint(f\"Hyperparameters: {hp}\")\nprint(f\"Batch: {BATCH} x Grad_Acc: {GRAD_ACC} = Effective: {int(BATCH) * int(GRAD_ACC)}\")\nprint(f\"\\nCommand:\\n{' ' .join(shlex.quote(x) for x in cmd)}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b97eed",
   "metadata": {},
   "outputs": [],
   "source": "# Run training (real-time output)\nimport sys, subprocess, time as _time\n\n_t0 = _time.time()\nproc = subprocess.Popen(\n    cmd,\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,   # merge stderr into stdout\n    text=True,\n    bufsize=1,                  # line-buffered\n)\n\n# Stream output line by line\nfor line in proc.stdout:\n    print(line, end=\"\", flush=True)\n\nret = proc.wait()\n_elapsed = _time.time() - _t0\n\nif ret != 0:\n    raise RuntimeError(f\"Training failed with exit code {ret}\")\nprint(f\"\\nTraining complete in {_elapsed/60:.1f} min! Adapter saved to: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "code",
   "id": "ciw5kij9zqc",
   "source": "# ============================================================\n# Post-training analysis: loss curve + GPU stats + save report\n# ============================================================\nimport json, torch\nimport matplotlib.pyplot as plt\n\n# --- 1. Read trainer_state.json (contains per-step logs) ---\nstate_path = f\"{OUTPUT_DIR}/trainer_state.json\"\nwith open(state_path) as f:\n    state = json.load(f)\n\nlogs = [e for e in state[\"log_history\"] if \"loss\" in e]\nsteps = [e[\"step\"] for e in logs]\nlosses = [e[\"loss\"] for e in logs]\nlrs = [e.get(\"learning_rate\", 0) for e in logs]\n\n# Final metrics\nfinal_loss = losses[-1] if losses else None\ntotal_steps = state.get(\"max_steps\", steps[-1] if steps else 0)\nbest_step = min(logs, key=lambda e: e[\"loss\"])[\"step\"] if logs else 0\n\n# --- 2. GPU memory stats ---\ngpu_info = {}\nif torch.cuda.is_available():\n    props = torch.cuda.get_device_properties(0)\n    gpu_info = {\n        \"gpu_name\": torch.cuda.get_device_name(0),\n        \"allocated_gb\": round(torch.cuda.max_memory_allocated(0) / 1e9, 2),\n        \"reserved_gb\": round(torch.cuda.max_memory_reserved(0) / 1e9, 2),\n        \"total_gb\": round(props.total_memory / 1e9, 2),\n    }\n\n# --- 3. Read train_results.json ---\nresults_path = f\"{OUTPUT_DIR}/train_results.json\"\ntrain_results = {}\nif os.path.exists(results_path):\n    with open(results_path) as f:\n        train_results = json.load(f)\n\n# --- 4. Print summary ---\nprint(f\"{'='*60}\")\nprint(f\"  Training Report: {AGENT_NAME}\")\nprint(f\"{'='*60}\")\nprint(f\"  Model:           {MODEL_NAME}\")\nprint(f\"  LoRA rank:       {hp['lora_r']}, alpha: {hp['lora_alpha']}\")\nprint(f\"  Epochs:          {hp['epochs']}\")\nprint(f\"  Total steps:     {total_steps}\")\nprint(f\"  Final loss:      {final_loss:.4f}\" if final_loss else \"  Final loss:      N/A\")\nprint(f\"  Best loss:       {best_step} step = {min(losses):.4f}\" if losses else \"\")\nprint(f\"  Training time:   {train_results.get('train_runtime', 0)/60:.1f} min\")\nprint(f\"  Samples/sec:     {train_results.get('train_samples_per_second', 0):.1f}\")\nif gpu_info:\n    print(f\"  GPU:             {gpu_info['gpu_name']}\")\n    print(f\"  Peak VRAM:       {gpu_info['allocated_gb']} GB / {gpu_info['total_gb']} GB\")\nprint(f\"  Output:          {OUTPUT_DIR}\")\nprint(f\"{'='*60}\")\n\n# --- 5. Plot loss curve ---\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nax1.plot(steps, losses, \"b-\", linewidth=1.5, label=\"Training Loss\")\nax1.set_xlabel(\"Step\")\nax1.set_ylabel(\"Loss\")\nax1.set_title(f\"{AGENT_NAME} - Training Loss\")\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.plot(steps, lrs, \"r-\", linewidth=1.5, label=\"Learning Rate\")\nax2.set_xlabel(\"Step\")\nax2.set_ylabel(\"Learning Rate\")\nax2.set_title(f\"{AGENT_NAME} - LR Schedule\")\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplot_path = f\"{OUTPUT_DIR}/training_curves.png\"\nfig.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\nplt.show()\nprint(f\"Plot saved to: {plot_path}\")\n\n# --- 6. Save summary JSON (for cross-agent comparison later) ---\nsummary = {\n    \"agent\": AGENT_NAME,\n    \"model\": MODEL_NAME,\n    \"hyperparameters\": hp,\n    \"batch_size\": int(BATCH),\n    \"grad_acc\": int(GRAD_ACC),\n    \"effective_batch\": int(BATCH) * int(GRAD_ACC),\n    \"final_loss\": final_loss,\n    \"best_loss\": min(losses) if losses else None,\n    \"total_steps\": total_steps,\n    \"training_time_min\": round(train_results.get(\"train_runtime\", 0) / 60, 1),\n    \"samples_per_second\": train_results.get(\"train_samples_per_second\", 0),\n    \"gpu\": gpu_info,\n    \"output_dir\": OUTPUT_DIR,\n}\nsummary_path = f\"{OUTPUT_DIR}/training_summary.json\"\nwith open(summary_path, \"w\") as f:\n    json.dump(summary, f, indent=2, ensure_ascii=False)\nprint(f\"Summary saved to: {summary_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "311ca9a5",
   "metadata": {},
   "source": "## 3) Next agent\n\nAfter this run finishes:\n1. Change `AGENT_NAME` in cell 1\n2. **Runtime → Restart runtime** (free GPU memory)\n3. Run all cells again\n\nTrained LoRA adapters are saved to Google Drive: `models/adapters/<agent>/run_YYYYmmdd_HHMMSS/`\n\n### Recommended training order\n1. `symptom_normalizer` (0.6B, fast)\n2. `symptom_quality_grader` (0.6B, fast)\n3. `rag_relevance_grader` (0.6B, fast)\n4. `drug_evidence_grader` (0.6B, fast)\n5. `diagnosis_generator` (1.7B, slower)\n\n## 4) All-agents comparison (run after ALL agents are trained)"
  },
  {
   "cell_type": "code",
   "id": "v0fys4z56la",
   "source": "# ============================================================\n# Cross-agent comparison (run AFTER all 5 agents are trained)\n# Reads training_summary.json from each agent's latest run\n# ============================================================\nimport json, glob, os\nimport matplotlib.pyplot as plt\n\nAGENTS = [\"symptom_normalizer\", \"symptom_quality_grader\", \"rag_relevance_grader\",\n          \"drug_evidence_grader\", \"diagnosis_generator\"]\n\nsummaries = []\nfor agent in AGENTS:\n    agent_dir = f\"{ADAPTER_OUTPUT_ROOT}/{agent}\"\n    # Find latest run\n    runs = sorted(glob.glob(f\"{agent_dir}/run_*\"))\n    if not runs:\n        print(f\"[SKIP] {agent}: no training runs found\")\n        continue\n    latest = runs[-1]\n    summary_file = f\"{latest}/training_summary.json\"\n    if not os.path.exists(summary_file):\n        print(f\"[SKIP] {agent}: no training_summary.json in {latest}\")\n        continue\n    with open(summary_file) as f:\n        summaries.append(json.load(f))\n    print(f\"[OK] {agent}: loaded from {os.path.basename(latest)}\")\n\nif len(summaries) < 2:\n    print(\"\\nNeed at least 2 agents trained to compare. Train more agents first.\")\nelse:\n    # --- Comparison table ---\n    print(f\"\\n{'='*90}\")\n    print(f\"{'Agent':<25} {'Model':<16} {'Loss':>8} {'Steps':>7} {'Time':>8} {'VRAM':>8} {'Samples/s':>10}\")\n    print(f\"{'-'*90}\")\n    for s in summaries:\n        vram = f\"{s['gpu'].get('allocated_gb', '?')}GB\" if s.get('gpu') else \"N/A\"\n        print(f\"{s['agent']:<25} {s['model']:<16} {s.get('final_loss', 0):>8.4f} \"\n              f\"{s['total_steps']:>7} {s['training_time_min']:>7.1f}m {vram:>8} \"\n              f\"{s.get('samples_per_second', 0):>10.1f}\")\n    print(f\"{'='*90}\")\n\n    # --- Comparison charts ---\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    names = [s[\"agent\"].replace(\"_\", \"\\n\") for s in summaries]\n    colors = [\"#4CAF50\", \"#2196F3\", \"#FF9800\", \"#9C27B0\", \"#F44336\"][:len(summaries)]\n\n    # Final loss\n    axes[0].bar(names, [s.get(\"final_loss\", 0) for s in summaries], color=colors)\n    axes[0].set_title(\"Final Training Loss\")\n    axes[0].set_ylabel(\"Loss\")\n\n    # Training time\n    axes[1].bar(names, [s.get(\"training_time_min\", 0) for s in summaries], color=colors)\n    axes[1].set_title(\"Training Time (min)\")\n    axes[1].set_ylabel(\"Minutes\")\n\n    # Peak VRAM\n    vrams = [s.get(\"gpu\", {}).get(\"allocated_gb\", 0) for s in summaries]\n    axes[2].bar(names, vrams, color=colors)\n    axes[2].set_title(\"Peak GPU VRAM (GB)\")\n    axes[2].set_ylabel(\"GB\")\n\n    plt.tight_layout()\n    comp_path = f\"{ADAPTER_OUTPUT_ROOT}/all_agents_comparison.png\"\n    fig.savefig(comp_path, dpi=150, bbox_inches=\"tight\")\n    plt.show()\n    print(f\"\\nComparison chart saved to: {comp_path}\")\n\n    # --- All loss curves in one plot ---\n    fig2, ax = plt.subplots(figsize=(12, 6))\n    for i, agent in enumerate(AGENTS):\n        agent_dir = f\"{ADAPTER_OUTPUT_ROOT}/{agent}\"\n        runs = sorted(glob.glob(f\"{agent_dir}/run_*\"))\n        if not runs:\n            continue\n        state_file = f\"{runs[-1]}/trainer_state.json\"\n        if not os.path.exists(state_file):\n            continue\n        with open(state_file) as f:\n            state = json.load(f)\n        logs = [e for e in state[\"log_history\"] if \"loss\" in e]\n        if logs:\n            # Normalize steps to epoch fraction for comparison\n            max_step = max(e[\"step\"] for e in logs)\n            epochs_total = state.get(\"epoch\", 3)\n            x = [e[\"step\"] / max_step * epochs_total for e in logs]\n            y = [e[\"loss\"] for e in logs]\n            ax.plot(x, y, linewidth=1.5, label=agent, color=colors[i] if i < len(colors) else None)\n\n    ax.set_xlabel(\"Epoch\")\n    ax.set_ylabel(\"Loss\")\n    ax.set_title(\"All Agents - Training Loss Curves\")\n    ax.legend(fontsize=9)\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    curves_path = f\"{ADAPTER_OUTPUT_ROOT}/all_agents_loss_curves.png\"\n    fig2.savefig(curves_path, dpi=150, bbox_inches=\"tight\")\n    plt.show()\n    print(f\"Loss curves saved to: {curves_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}