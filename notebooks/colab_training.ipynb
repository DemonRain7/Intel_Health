{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IntelHealth - Colab Training (Per-Agent)\n",
    "\n",
    "This notebook is designed for **one agent per run**.\n",
    "Pick `AGENT_NAME`, then run all cells.\n",
    "\n",
    "All agents use `supervised_finetuning.py` with Qwen ChatML template.\n",
    "- 4 grader/normalizer agents → Qwen3-0.5B-Instruct (loss on response only)\n",
    "- diagnosis_generator → Qwen3-1.8B-Instruct (loss on all tokens via `--train_on_inputs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zfzgyuh4yie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo + pull latest\n",
    "import os\n",
    "repo_dir = '/content/Intel_Health'\n",
    "if not os.path.exists(repo_dir):\n",
    "    !git clone https://github.com/DemonRain7/Intel_Health.git {repo_dir}\n",
    "else:\n",
    "    !git -C {repo_dir} pull\n",
    "%cd {repo_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip -q install torch transformers datasets peft accelerate bitsandbytes sentencepiece loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drive_training_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and set persistent paths\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/Code_Project/IntelHealth'\n",
    "SFT_DATA_ROOT = f'{DRIVE_ROOT}/datasets/agent_sft'\n",
    "ADAPTER_OUTPUT_ROOT = f'{DRIVE_ROOT}/models/adapters'\n",
    "os.makedirs(SFT_DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(ADAPTER_OUTPUT_ROOT, exist_ok=True)\n",
    "print('SFT_DATA_ROOT:', SFT_DATA_ROOT)\n",
    "print('ADAPTER_OUTPUT_ROOT:', ADAPTER_OUTPUT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a252b",
   "metadata": {},
   "source": [
    "## 1) Select Agent (single run)\n",
    "\n",
    "Supported training agents:\n",
    "- symptom_normalizer (SFT, 0.5B)\n",
    "- symptom_quality_grader (SFT, 0.5B)\n",
    "- rag_relevance_grader (SFT, 0.5B)\n",
    "- drug_evidence_grader (SFT, 0.5B)\n",
    "- diagnosis_generator (SFT + train_on_inputs, 1.8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec49031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shlex\n",
    "from pathlib import Path\n",
    "\n",
    "AGENT_NAME = \"symptom_quality_grader\"  # <-- change this each run\n",
    "\n",
    "MODEL_BY_AGENT = {\n",
    "    \"symptom_normalizer\":     \"Qwen/Qwen3-0.5B-Instruct\",\n",
    "    \"symptom_quality_grader\": \"Qwen/Qwen3-0.5B-Instruct\",\n",
    "    \"rag_relevance_grader\":   \"Qwen/Qwen3-0.5B-Instruct\",\n",
    "    \"drug_evidence_grader\":   \"Qwen/Qwen3-0.5B-Instruct\",\n",
    "    \"diagnosis_generator\":    \"Qwen/Qwen3-1.8B-Instruct\",\n",
    "}\n",
    "\n",
    "# --train_file_dir expects a DIRECTORY (globs *.jsonl inside)\n",
    "DATA_DIR_BY_AGENT = {\n",
    "    \"symptom_normalizer\":     f\"{SFT_DATA_ROOT}/symptom_normalizer\",\n",
    "    \"symptom_quality_grader\": f\"{SFT_DATA_ROOT}/symptom_quality_grader\",\n",
    "    \"rag_relevance_grader\":   f\"{SFT_DATA_ROOT}/rag_relevance_grader\",\n",
    "    \"drug_evidence_grader\":   f\"{SFT_DATA_ROOT}/drug_evidence_grader\",\n",
    "    \"diagnosis_generator\":    f\"{SFT_DATA_ROOT}/diagnosis_generator\",\n",
    "}\n",
    "\n",
    "OUTPUT_BY_AGENT = {\n",
    "    \"symptom_normalizer\":     f\"{ADAPTER_OUTPUT_ROOT}/symptom_normalizer\",\n",
    "    \"symptom_quality_grader\": f\"{ADAPTER_OUTPUT_ROOT}/symptom_quality_grader\",\n",
    "    \"rag_relevance_grader\":   f\"{ADAPTER_OUTPUT_ROOT}/rag_relevance_grader\",\n",
    "    \"drug_evidence_grader\":   f\"{ADAPTER_OUTPUT_ROOT}/drug_evidence_grader\",\n",
    "    \"diagnosis_generator\":    f\"{ADAPTER_OUTPUT_ROOT}/diagnosis_generator\",\n",
    "}\n",
    "\n",
    "assert AGENT_NAME in MODEL_BY_AGENT, f\"Unknown agent: {AGENT_NAME}\"\n",
    "\n",
    "MODEL_NAME = MODEL_BY_AGENT[AGENT_NAME]\n",
    "TRAIN_DIR  = DATA_DIR_BY_AGENT[AGENT_NAME]\n",
    "OUTPUT_DIR = OUTPUT_BY_AGENT[AGENT_NAME]\n",
    "\n",
    "print(f\"AGENT_NAME: {AGENT_NAME}\")\n",
    "print(f\"MODEL_NAME: {MODEL_NAME}\")\n",
    "print(f\"TRAIN_DIR:  {TRAIN_DIR}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c6614",
   "metadata": {},
   "source": [
    "## 2) Build command\n",
    "\n",
    "All agents use `supervised_finetuning.py` with Qwen ChatML template.\n",
    "- `diagnosis_generator` adds `--train_on_inputs` (loss on all tokens, DAPT-like behavior)\n",
    "- Others only compute loss on model response tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84a4d1",
   "metadata": {},
   "outputs": [],
   "source": "SFT_SCRIPT = Path(\"training/supervised_finetuning.py\")\n\n# ═══════════════════════════════════════════════════════════════\n#  Per-agent hyperparameters  (edit freely, auto-selected by AGENT_NAME)\n# ═══════════════════════════════════════════════════════════════\nAGENT_HP = {\n    # ── DAPT: 1.8B model, 5000 MCQ samples, output=1 token ──\n    \"diagnosis_generator\": {\n        \"epochs\":     2,       # 75万 tokens 较少, LoRA 不易过拟合, 2 轮吸收知识\n        \"lr\":         \"1e-4\",  # 1.8B 模型 + 小数据, 保守学习率\n        \"lora_r\":     \"32\",    # 5 分类任务 pattern 简单, 低秩够用\n        \"lora_alpha\": \"64\",    # alpha/rank = 2.0\n        \"max_len\":    \"256\",   # 样本最长 ~200 tokens\n    },\n    # ── SFT: 0.5B, 1500 samples, long input + long JSON output ──\n    \"symptom_normalizer\": {\n        \"epochs\":     3,       # 1500 条, 需要多轮学习复杂的结构化生成\n        \"lr\":         \"2e-4\",  # 0.5B 小模型承受得住\n        \"lora_r\":     \"64\",    # 生成医学术语 + RAG 关键词, 需要高表达力\n        \"lora_alpha\": \"128\",   # alpha/rank = 2.0\n        \"max_len\":    \"512\",   # 输入+输出最长 ~390 tokens, 必须 512\n    },\n    # ── SFT: 0.5B, ~1500 samples, moderate input + short JSON output ──\n    \"symptom_quality_grader\": {\n        \"epochs\":     3,\n        \"lr\":         \"2e-4\",\n        \"lora_r\":     \"64\",\n        \"lora_alpha\": \"128\",\n        \"max_len\":    \"256\",   # 样本最长 ~190 tokens\n    },\n    \"rag_relevance_grader\": {\n        \"epochs\":     3,\n        \"lr\":         \"2e-4\",\n        \"lora_r\":     \"64\",\n        \"lora_alpha\": \"128\",\n        \"max_len\":    \"256\",   # 样本最长 ~175 tokens\n    },\n    \"drug_evidence_grader\": {\n        \"epochs\":     3,\n        \"lr\":         \"2e-4\",\n        \"lora_r\":     \"64\",\n        \"lora_alpha\": \"128\",\n        \"max_len\":    \"256\",   # 样本最长 ~225 tokens\n    },\n}\n\n# ═══════════════════════════════════════════════════════════════\n#  Shared hyperparameters\n# ═══════════════════════════════════════════════════════════════\nBATCH        = \"2\"\nGRAD_ACC     = \"8\"      # effective batch = 2 × 8 = 16\nLORA_DROPOUT = \"0.05\"\n\n# ═══════════════════════════════════════════════════════════════\nhp = AGENT_HP[AGENT_NAME]\n\ncmd = [\n    \"python\", str(SFT_SCRIPT),\n    \"--model_name_or_path\",          MODEL_NAME,\n    \"--tokenizer_name_or_path\",      MODEL_NAME,\n    \"--train_file_dir\",              TRAIN_DIR,\n    \"--output_dir\",                  OUTPUT_DIR,\n    \"--template_name\",               \"qwen\",\n    \"--do_train\",\n    \"--fp16\",\n    \"--gradient_checkpointing\",\n    \"--per_device_train_batch_size\", BATCH,\n    \"--gradient_accumulation_steps\", GRAD_ACC,\n    \"--num_train_epochs\",            str(hp[\"epochs\"]),\n    \"--learning_rate\",               hp[\"lr\"],\n    \"--lora_rank\",                   hp[\"lora_r\"],\n    \"--lora_alpha\",                  hp[\"lora_alpha\"],\n    \"--lora_dropout\",                LORA_DROPOUT,\n    \"--model_max_length\",            hp[\"max_len\"],\n    \"--logging_steps\",               \"10\",\n    \"--save_strategy\",               \"epoch\",\n    \"--overwrite_output_dir\",\n]\n\n# diagnosis_generator: loss on ALL tokens (DAPT-like)\nif AGENT_NAME == \"diagnosis_generator\":\n    cmd.append(\"--train_on_inputs\")\n\nprint(f\"Agent: {AGENT_NAME}\")\nprint(f\"Hyperparameters: {hp}\")\nprint(f\"\\nCommand:\\n{' '.join(shlex.quote(x) for x in cmd)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b97eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ca9a5",
   "metadata": {},
   "source": [
    "## 3) Next agent\n",
    "\n",
    "After this run finishes:\n",
    "1. Change `AGENT_NAME` in cell 1\n",
    "2. **Runtime → Restart runtime** (free GPU memory)\n",
    "3. Run all cells again\n",
    "\n",
    "Trained LoRA adapters are saved to Google Drive: `models/adapters/<agent>/`\n",
    "\n",
    "### Recommended training order\n",
    "1. `symptom_normalizer` (0.5B, fast)\n",
    "2. `symptom_quality_grader` (0.5B, fast)\n",
    "3. `rag_relevance_grader` (0.5B, fast)\n",
    "4. `drug_evidence_grader` (0.5B, fast)\n",
    "5. `diagnosis_generator` (1.8B, slower)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}