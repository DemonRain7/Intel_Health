{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# IntelHealth - Colab Training (Per-Agent)\n\nThis notebook is designed for **one agent per run**.\nPick `AGENT_NAME`, then run all cells.\n\nAll agents use `supervised_finetuning.py` with Qwen ChatML template.\n- 4 grader/normalizer agents → Qwen3-0.6B (loss on response only)\n- diagnosis_generator → Qwen3-1.7B (loss on all tokens via `--train_on_inputs`)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zfzgyuh4yie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo + pull latest\n",
    "import os\n",
    "repo_dir = '/content/Intel_Health'\n",
    "if not os.path.exists(repo_dir):\n",
    "    !git clone https://github.com/DemonRain7/Intel_Health.git {repo_dir}\n",
    "else:\n",
    "    !git -C {repo_dir} pull\n",
    "%cd {repo_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a189e0",
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies (need transformers>=4.46 for Qwen3 support)\n!pip -q install torch \"transformers>=4.46\" datasets peft accelerate bitsandbytes sentencepiece loguru\n\n# HuggingFace login (Qwen3 requires authentication)\n# Option 1: Colab Secrets (recommended, persistent across sessions)\n#   Click the key icon in the left sidebar -> Add secret:\n#   Name: HF_TOKEN    Value: your token from https://huggingface.co/settings/tokens\n#   Then toggle \"Notebook access\" ON\n# Option 2: Falls back to interactive login() if secret not found\nimport os\ntry:\n    from google.colab import userdata\n    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n    print(\"HF_TOKEN loaded from Colab Secrets\")\nexcept (ImportError, userdata.SecretNotFoundError):\n    from huggingface_hub import login\n    login()  # interactive fallback"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drive_training_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and set persistent paths\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/Code_Project/IntelHealth'\n",
    "SFT_DATA_ROOT = f'{DRIVE_ROOT}/datasets/agent_sft'\n",
    "ADAPTER_OUTPUT_ROOT = f'{DRIVE_ROOT}/models/adapters'\n",
    "os.makedirs(SFT_DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(ADAPTER_OUTPUT_ROOT, exist_ok=True)\n",
    "print('SFT_DATA_ROOT:', SFT_DATA_ROOT)\n",
    "print('ADAPTER_OUTPUT_ROOT:', ADAPTER_OUTPUT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a252b",
   "metadata": {},
   "source": "## 1) Select Agent (single run)\n\nSupported training agents:\n- symptom_normalizer (SFT, 0.6B)\n- symptom_quality_grader (SFT, 0.6B)\n- rag_relevance_grader (SFT, 0.6B)\n- drug_evidence_grader (SFT, 0.6B)\n- diagnosis_generator (SFT + train_on_inputs, 1.7B)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec49031",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport subprocess, shlex\nfrom pathlib import Path\nfrom datetime import datetime\n\nAGENT_NAME = \"symptom_quality_grader\"  # <-- change this each run\n\nMODEL_BY_AGENT = {\n    \"symptom_normalizer\":     \"Qwen/Qwen3-0.6B\",\n    \"symptom_quality_grader\": \"Qwen/Qwen3-0.6B\",\n    \"rag_relevance_grader\":   \"Qwen/Qwen3-0.6B\",\n    \"drug_evidence_grader\":   \"Qwen/Qwen3-0.6B\",\n    \"diagnosis_generator\":    \"Qwen/Qwen3-1.7B\",\n}\n\n# --train_file_dir expects a DIRECTORY (globs *.jsonl inside)\nDATA_DIR_BY_AGENT = {\n    \"symptom_normalizer\":     f\"{SFT_DATA_ROOT}/symptom_normalizer\",\n    \"symptom_quality_grader\": f\"{SFT_DATA_ROOT}/symptom_quality_grader\",\n    \"rag_relevance_grader\":   f\"{SFT_DATA_ROOT}/rag_relevance_grader\",\n    \"drug_evidence_grader\":   f\"{SFT_DATA_ROOT}/drug_evidence_grader\",\n    \"diagnosis_generator\":    f\"{SFT_DATA_ROOT}/diagnosis_generator\",\n}\n\nOUTPUT_BY_AGENT = {\n    \"symptom_normalizer\":     f\"{ADAPTER_OUTPUT_ROOT}/symptom_normalizer\",\n    \"symptom_quality_grader\": f\"{ADAPTER_OUTPUT_ROOT}/symptom_quality_grader\",\n    \"rag_relevance_grader\":   f\"{ADAPTER_OUTPUT_ROOT}/rag_relevance_grader\",\n    \"drug_evidence_grader\":   f\"{ADAPTER_OUTPUT_ROOT}/drug_evidence_grader\",\n    \"diagnosis_generator\":    f\"{ADAPTER_OUTPUT_ROOT}/diagnosis_generator\",\n}\n\nassert AGENT_NAME in MODEL_BY_AGENT, f\"Unknown agent: {AGENT_NAME}\"\n\nfor _agent_dir in OUTPUT_BY_AGENT.values():\n    os.makedirs(_agent_dir, exist_ok=True)\n\nMODEL_NAME = MODEL_BY_AGENT[AGENT_NAME]\nTRAIN_DIR  = DATA_DIR_BY_AGENT[AGENT_NAME]\nOUTPUT_BASE_DIR = OUTPUT_BY_AGENT[AGENT_NAME]\nRUN_NAME = datetime.now().strftime(\"run_%Y%m%d_%H%M%S_%f\")\nOUTPUT_DIR = f\"{OUTPUT_BASE_DIR}/{RUN_NAME}\"\n\nprint(f\"AGENT_NAME: {AGENT_NAME}\")\nprint(f\"MODEL_NAME: {MODEL_NAME}\")\nprint(f\"TRAIN_DIR:  {TRAIN_DIR}\")\nprint(f\"OUTPUT_DIR: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "id": "b09c6614",
   "metadata": {},
   "source": [
    "## 2) Build command\n",
    "\n",
    "All agents use `supervised_finetuning.py` with Qwen ChatML template.\n",
    "- `diagnosis_generator` adds `--train_on_inputs` (loss on all tokens, DAPT-like behavior)\n",
    "- Others only compute loss on model response tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84a4d1",
   "metadata": {},
   "outputs": [],
   "source": "SFT_SCRIPT = Path(\"training/supervised_finetuning.py\")\n\n# =============================================================\n# Per-agent hyperparameters (edit freely, auto-selected by AGENT_NAME)\n# =============================================================\nAGENT_HP = {\n    # DAPT: 1.7B model, 5000 MCQ samples, output=1 token\n    \"diagnosis_generator\": {\n        \"epochs\":     2,\n        \"lr\":         \"1e-4\",\n        \"lora_r\":     \"32\",\n        \"lora_alpha\": \"64\",\n        \"max_len\":    \"256\",\n    },\n    # SFT: 0.6B, 1500 samples, long input + long JSON output\n    \"symptom_normalizer\": {\n        \"epochs\":     3,\n        \"lr\":         \"2e-4\",\n        \"lora_r\":     \"64\",\n        \"lora_alpha\": \"128\",\n        \"max_len\":    \"512\",\n    },\n    # SFT: 0.6B, ~1500 samples, moderate input + short JSON output\n    \"symptom_quality_grader\": {\n        \"epochs\":     3,\n        \"lr\":         \"2e-4\",\n        \"lora_r\":     \"64\",\n        \"lora_alpha\": \"128\",\n        \"max_len\":    \"256\",\n    },\n    \"rag_relevance_grader\": {\n        \"epochs\":     3,\n        \"lr\":         \"2e-4\",\n        \"lora_r\":     \"64\",\n        \"lora_alpha\": \"128\",\n        \"max_len\":    \"256\",\n    },\n    \"drug_evidence_grader\": {\n        \"epochs\":     3,\n        \"lr\":         \"2e-4\",\n        \"lora_r\":     \"64\",\n        \"lora_alpha\": \"128\",\n        \"max_len\":    \"256\",\n    },\n}\n\n# =============================================================\n# Shared hyperparameters (A100-40GB optimized)\n# =============================================================\nBATCH = \"8\"\nGRAD_ACC = \"2\"\nLORA_DROPOUT = \"0.05\"\n\nhp = AGENT_HP[AGENT_NAME]\n\ncmd = [\n    \"python\", str(SFT_SCRIPT),\n    \"--model_name_or_path\",          MODEL_NAME,\n    \"--tokenizer_name_or_path\",      MODEL_NAME,\n    \"--train_file_dir\",              TRAIN_DIR,\n    \"--output_dir\",                  OUTPUT_DIR,\n    \"--template_name\",               \"qwen\",\n    \"--do_train\",\n    \"--fp16\",\n    \"--gradient_checkpointing\",\n    \"--per_device_train_batch_size\", BATCH,\n    \"--gradient_accumulation_steps\", GRAD_ACC,\n    \"--num_train_epochs\",            str(hp[\"epochs\"]),\n    \"--learning_rate\",               hp[\"lr\"],\n    \"--lora_rank\",                   hp[\"lora_r\"],\n    \"--lora_alpha\",                  hp[\"lora_alpha\"],\n    \"--lora_dropout\",                LORA_DROPOUT,\n    \"--model_max_length\",            hp[\"max_len\"],\n    \"--logging_steps\",               \"10\",\n    \"--save_strategy\",               \"epoch\",\n]\n\n# diagnosis_generator: loss on ALL tokens (DAPT-like)\nif AGENT_NAME == \"diagnosis_generator\":\n    cmd.append(\"--train_on_inputs\")\n\nprint(f\"Agent: {AGENT_NAME}\")\nprint(f\"Hyperparameters: {hp}\")\nprint(f\"Batch: {BATCH} x Grad_Acc: {GRAD_ACC} = Effective: {int(BATCH) * int(GRAD_ACC)}\")\nprint(f\"\\nCommand:\\n{' ' .join(shlex.quote(x) for x in cmd)}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b97eed",
   "metadata": {},
   "outputs": [],
   "source": "# Run training (real-time output)\nimport sys, subprocess\n\nproc = subprocess.Popen(\n    cmd,\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,   # merge stderr into stdout\n    text=True,\n    bufsize=1,                  # line-buffered\n)\n\n# Stream output line by line\nfor line in proc.stdout:\n    print(line, end=\"\", flush=True)\n\nret = proc.wait()\nif ret != 0:\n    raise RuntimeError(f\"Training failed with exit code {ret}\")\nprint(f\"\\nTraining complete! Adapter saved to: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "id": "311ca9a5",
   "metadata": {},
   "source": "## 3) Next agent\n\nAfter this run finishes:\n1. Change `AGENT_NAME` in cell 1\n2. **Runtime → Restart runtime** (free GPU memory)\n3. Run all cells again\n\nTrained LoRA adapters are saved to Google Drive: `models/adapters/<agent>/run_YYYYmmdd_HHMMSS/`\n\n### Recommended training order\n1. `symptom_normalizer` (0.6B, fast)\n2. `symptom_quality_grader` (0.6B, fast)\n3. `rag_relevance_grader` (0.6B, fast)\n4. `drug_evidence_grader` (0.6B, fast)\n5. `diagnosis_generator` (1.7B, slower)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}