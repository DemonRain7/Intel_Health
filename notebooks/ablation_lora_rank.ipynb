{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Rank Ablation Study\n",
    "\n",
    "对 `diagnosis_generator`（Qwen3-1.7B）做 LoRA rank 消融实验：\n",
    "- 对比 rank = 8, 16, 32, 64, 128\n",
    "- alpha = rank × 2（保持比值 2.0 不变）\n",
    "- 其余超参固定（lr=1e-4, batch=2, grad_acc=8, epochs=2）\n",
    "\n",
    "评估指标：\n",
    "1. Training loss 曲线\n",
    "2. JSON 格式正确率\n",
    "3. 字段完整性（results/recommendations/recomm_short）\n",
    "4. 概率合理性（3 个疾病概率之和 ≈ 1.0）\n",
    "5. 推理速度\n",
    "6. 训练显存占用\n",
    "\n",
    "**注意**: 每个 rank 训练完后需要 **重启 Runtime** 释放显存，再训练下一个。\n",
    "建议按 rank 从小到大依次训练。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 0. Setup\nimport os\nrepo_dir = '/content/Intel_Health'\nif not os.path.exists(repo_dir):\n    !git clone https://github.com/DemonRain7/Intel_Health.git {repo_dir}\nelse:\n    !git -C {repo_dir} pull\n%cd {repo_dir}\n\n# Colab 已预装 torch，不要重装，否则会循环导入报错\n!pip -q install \"transformers>=4.46\" datasets peft accelerate bitsandbytes sentencepiece loguru\n\ntry:\n    from google.colab import userdata\n    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n    print(\"HF_TOKEN loaded from Colab Secrets\")\nexcept (ImportError, Exception):\n    from huggingface_hub import login\n    login()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1. Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/Code_Project/IntelHealth'\n",
    "SFT_DATA_DIR = f'{DRIVE_ROOT}/datasets/agent_sft/diagnosis_generator'\n",
    "ABLATION_OUTPUT_ROOT = f'{DRIVE_ROOT}/models/adapters/ablation'\n",
    "ABLATION_MERGED_ROOT = f'{DRIVE_ROOT}/models/merged/ablation'\n",
    "\n",
    "os.makedirs(ABLATION_OUTPUT_ROOT, exist_ok=True)\n",
    "os.makedirs(ABLATION_MERGED_ROOT, exist_ok=True)\n",
    "\n",
    "# 检查训练数据\n",
    "data_files = [f for f in os.listdir(SFT_DATA_DIR) if f.endswith('.jsonl')]\n",
    "print(f'SFT data dir: {SFT_DATA_DIR}')\n",
    "print(f'Data files: {data_files}')\n",
    "for f in data_files:\n",
    "    path = os.path.join(SFT_DATA_DIR, f)\n",
    "    with open(path) as fp:\n",
    "        count = sum(1 for _ in fp)\n",
    "    print(f'  {f}: {count} samples')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Training（每个 rank 跑一次，跑完重启 Runtime）\n",
    "\n",
    "**修改下面的 `CURRENT_RANK` 后运行此 cell 和下一个 cell。**\n",
    "\n",
    "训练顺序建议：8 → 16 → 32 → 64 → 128（每个跑完后重启 Runtime 再跑下一个）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2. 选择当前要训练的 rank\n",
    "# ============================\n",
    "# 每次只训练一个 rank，训练完重启 Runtime，修改此值再跑\n",
    "CURRENT_RANK = 64  # <-- 修改这里: 8, 16, 32, 64, 128\n",
    "# ============================\n",
    "\n",
    "CURRENT_ALPHA = CURRENT_RANK * 2\n",
    "MODEL_NAME = 'Qwen/Qwen3-1.7B'\n",
    "OUTPUT_DIR = f'{ABLATION_OUTPUT_ROOT}/rank{CURRENT_RANK}'\n",
    "\n",
    "# 检查是否已经训练过\n",
    "if os.path.isdir(OUTPUT_DIR) and os.listdir(OUTPUT_DIR):\n",
    "    print(f'WARNING: {OUTPUT_DIR} already exists and is not empty!')\n",
    "    print('Contents:', os.listdir(OUTPUT_DIR))\n",
    "    print('If you want to retrain, delete the directory first.')\n",
    "else:\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f'\\n=== Ablation Config ===')\n",
    "print(f'Rank:       {CURRENT_RANK}')\n",
    "print(f'Alpha:      {CURRENT_ALPHA}')\n",
    "print(f'Alpha/Rank: {CURRENT_ALPHA / CURRENT_RANK}')\n",
    "print(f'Model:      {MODEL_NAME}')\n",
    "print(f'Data:       {SFT_DATA_DIR}')\n",
    "print(f'Output:     {OUTPUT_DIR}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 3. 训练\nimport subprocess, shlex, sys, time as _time, os\n\ncmd = [\n    'python', 'training/supervised_finetuning.py',\n    '--model_name_or_path',          MODEL_NAME,\n    '--tokenizer_name_or_path',      MODEL_NAME,\n    '--train_file_dir',              SFT_DATA_DIR,\n    '--output_dir',                  OUTPUT_DIR,\n    '--template_name',               'qwen',\n    '--do_train',\n    '--fp16',\n    '--gradient_checkpointing',\n    '--per_device_train_batch_size', '2',\n    '--gradient_accumulation_steps',  '8',\n    '--num_train_epochs',            '2',\n    '--learning_rate',               '1e-4',\n    '--lora_rank',                   str(CURRENT_RANK),\n    '--lora_alpha',                  str(CURRENT_ALPHA),\n    '--lora_dropout',                '0.05',\n    '--model_max_length',            '256',\n    '--logging_steps',               '5',\n    '--save_strategy',               'epoch',\n]\n\nprint(f'Training rank={CURRENT_RANK}, alpha={CURRENT_ALPHA}...')\nprint(f'Command: {\" \".join(shlex.quote(x) for x in cmd)}')\nprint('=' * 60)\n\n# ????????????????? dapt/jsonl\ntrain_jsonl_files = sorted([f for f in os.listdir(SFT_DATA_DIR) if f.endswith('.jsonl')])\nprint(f'Training JSONL files ({len(train_jsonl_files)}): {train_jsonl_files}')\nif any('dapt' in f.lower() for f in train_jsonl_files):\n    print('[WARN] ??? dapt ??????? rank ablation ????')\n\n_t0 = _time.time()\nproc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)\nfor line in proc.stdout:\n    print(line, end='', flush=True)\nret = proc.wait()\n_elapsed = _time.time() - _t0\n\nif ret != 0:\n    raise RuntimeError(f'Training failed with exit code {ret}')\nprint(f'\\nTraining rank={CURRENT_RANK} complete in {_elapsed/60:.1f} min!')\nprint(f'Adapter saved to: {OUTPUT_DIR}')\nchild_gpu_stats_path = f'{OUTPUT_DIR}/gpu_stats.json'\nif os.path.exists(child_gpu_stats_path):\n    print(f'Child GPU stats found: {child_gpu_stats_path}')\nelse:\n    print(f'[WARN] Child GPU stats not found: {child_gpu_stats_path}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 4. ????? GPU ????????????\n",
    "import json, os\n",
    "\n",
    "child_stats_path = f'{OUTPUT_DIR}/gpu_stats.json'\n",
    "gpu_stats = {\n",
    "    'rank': CURRENT_RANK,\n",
    "    'alpha': CURRENT_ALPHA,\n",
    "    'training_time_min': round(_elapsed / 60, 1),\n",
    "}\n",
    "\n",
    "if os.path.exists(child_stats_path):\n",
    "    with open(child_stats_path) as f:\n",
    "        child_stats = json.load(f)\n",
    "    gpu_stats.update({\n",
    "        'gpu_name': child_stats.get('gpu_name'),\n",
    "        'total_gb': child_stats.get('total_gb'),\n",
    "        'peak_allocated_gb': child_stats.get('peak_allocated_gb'),\n",
    "        'peak_reserved_gb': child_stats.get('peak_reserved_gb'),\n",
    "        'stats_source': 'training/supervised_finetuning.py',\n",
    "    })\n",
    "else:\n",
    "    print(f'[WARN] ??? GPU ?????: {child_stats_path}')\n",
    "    gpu_stats['stats_source'] = 'fallback_no_child_stats'\n",
    "\n",
    "stats_path = f'{OUTPUT_DIR}/ablation_gpu_stats.json'\n",
    "with open(stats_path, 'w') as f:\n",
    "    json.dump(gpu_stats, f, indent=2)\n",
    "print(f'GPU stats saved to {stats_path}')\n",
    "print(json.dumps(gpu_stats, indent=2))\n",
    "\n",
    "print(f'\\n>>> ?????? Runtime ? Restart runtime????? CURRENT_RANK ?????')\n",
    "print(f'>>> ?? 5 ? rank ???????? Part B ? merge + ???')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B: Merge + Evaluate（全部 rank 训练完后运行）\n",
    "\n",
    "确保 5 个 rank（8/16/32/64/128）都训练完毕，adapter 保存在 Google Drive 上。\n",
    "\n",
    "以下步骤：\n",
    "1. 逐个 merge adapter 到 base model\n",
    "2. 对每个 merged 模型做推理评估\n",
    "3. 生成对比报告和可视化"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 5. 检查哪些 rank 已训练完毕\n",
    "import os, json\n",
    "\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/Code_Project/IntelHealth'\n",
    "ABLATION_OUTPUT_ROOT = f'{DRIVE_ROOT}/models/adapters/ablation'\n",
    "ABLATION_MERGED_ROOT = f'{DRIVE_ROOT}/models/merged/ablation'\n",
    "\n",
    "RANKS = [8, 16, 32, 64, 128]\n",
    "MODEL_NAME = 'Qwen/Qwen3-1.7B'\n",
    "\n",
    "print('Checking trained adapters...')\n",
    "trained_ranks = []\n",
    "for rank in RANKS:\n",
    "    adapter_dir = f'{ABLATION_OUTPUT_ROOT}/rank{rank}'\n",
    "    if os.path.isdir(adapter_dir):\n",
    "        has_adapter = any(f.startswith('adapter') for f in os.listdir(adapter_dir))\n",
    "        status = 'READY' if has_adapter else 'NO ADAPTER'\n",
    "        if has_adapter:\n",
    "            trained_ranks.append(rank)\n",
    "    else:\n",
    "        status = 'NOT FOUND'\n",
    "    print(f'  rank={rank:>3}: {status}  ({adapter_dir})')\n",
    "\n",
    "print(f'\\nReady for merge: {trained_ranks}')\n",
    "if len(trained_ranks) < len(RANKS):\n",
    "    missing = set(RANKS) - set(trained_ranks)\n",
    "    print(f'WARNING: Missing ranks: {missing}. Go back to Part A to train them.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 6. Merge adapters??? rank?\n",
    "import os, shutil, subprocess\n",
    "\n",
    "FORCE_REMERGE = True  # True: ????? merge??????? merged ??\n",
    "\n",
    "for rank in trained_ranks:\n",
    "    adapter_dir = f'{ABLATION_OUTPUT_ROOT}/rank{rank}'\n",
    "    merged_dir = f'{ABLATION_MERGED_ROOT}/rank{rank}'\n",
    "\n",
    "    if FORCE_REMERGE and os.path.isdir(merged_dir):\n",
    "        shutil.rmtree(merged_dir)\n",
    "\n",
    "    # ????? merge\n",
    "    if os.path.isdir(merged_dir) and any(\n",
    "        f.endswith('.safetensors') or f.endswith('.bin')\n",
    "        for f in os.listdir(merged_dir)\n",
    "    ):\n",
    "        print(f'rank={rank}: already merged, skipping')\n",
    "        continue\n",
    "\n",
    "    print(f'\\nMerging rank={rank}...')\n",
    "    cmd = [\n",
    "        'python', 'training/merge_peft_adapter.py',\n",
    "        '--base_model', MODEL_NAME,\n",
    "        '--lora_model', adapter_dir,\n",
    "        '--output_dir', merged_dir,\n",
    "    ]\n",
    "    ret = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if ret.returncode != 0:\n",
    "        print(f'  ERROR: {ret.stderr[-500:]}')\n",
    "    else:\n",
    "        print(f'  Merged to {merged_dir}')\n",
    "\n",
    "print('\\nMerge complete!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 7. ???????????\n",
    "import json, random\n",
    "\n",
    "SFT_DATA_DIR = f'{DRIVE_ROOT}/datasets/agent_sft/diagnosis_generator'\n",
    "\n",
    "# ????? diagnosis_sft.jsonl????? diagnosis_dapt.jsonl\n",
    "preferred_file = os.path.join(SFT_DATA_DIR, 'diagnosis_sft.jsonl')\n",
    "if os.path.exists(preferred_file):\n",
    "    eval_files = [preferred_file]\n",
    "else:\n",
    "    eval_files = [\n",
    "        os.path.join(SFT_DATA_DIR, f)\n",
    "        for f in sorted(os.listdir(SFT_DATA_DIR))\n",
    "        if f.endswith('.jsonl')\n",
    "    ]\n",
    "\n",
    "print(f'Eval JSONL files ({len(eval_files)}): {[os.path.basename(f) for f in eval_files]}')\n",
    "if any('dapt' in os.path.basename(f).lower() for f in eval_files):\n",
    "    print('[WARN] ?????? dapt ??????????? JSON ?????')\n",
    "\n",
    "# ???????? 20 ?????\n",
    "all_samples = []\n",
    "for fp in eval_files:\n",
    "    with open(fp) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                all_samples.append(json.loads(line))\n",
    "\n",
    "print(f'Total candidate samples: {len(all_samples)}')\n",
    "\n",
    "# ??? 20 ???? user prompt ??????\n",
    "random.seed(42)\n",
    "test_samples = random.sample(all_samples, min(20, len(all_samples)))\n",
    "\n",
    "test_inputs = []\n",
    "for sample in test_samples:\n",
    "    convs = sample.get('conversations', [])\n",
    "    user_msg = next((c['value'] for c in convs if c['from'] == 'human'), '')\n",
    "    if user_msg:\n",
    "        test_inputs.append(user_msg)\n",
    "\n",
    "print(f'Test inputs prepared: {len(test_inputs)}')\n",
    "print(f'Sample input (truncated): {test_inputs[0][:200]}...')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 8. ??????? rank ?????????\n",
    "import gc, time, re, json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def evaluate_model(model_path, test_inputs, max_new_tokens=1024):\n",
    "    \"\"\"????????????????????\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path, torch_dtype=torch.float16, device_map='cuda', trust_remote_code=True\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    mem_loaded = torch.cuda.memory_allocated() / 1024**3\n",
    "\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else eos_id\n",
    "\n",
    "    results = []\n",
    "    total_time = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for user_input in test_inputs:\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': '??????????????????JSON????JSON?????????'},\n",
    "            {'role': 'user', 'content': user_input},\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        input_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                num_beams=1,\n",
    "                eos_token_id=eos_id,\n",
    "                pad_token_id=pad_id,\n",
    "            )\n",
    "        dt = time.time() - t0\n",
    "        total_time += dt\n",
    "\n",
    "        gen_tokens = output_ids.shape[1] - input_len\n",
    "        total_tokens += gen_tokens\n",
    "\n",
    "        output_text = tokenizer.decode(output_ids[0][input_len:], skip_special_tokens=True)\n",
    "\n",
    "        # ?? <think> ??? markdown ??\n",
    "        cleaned = re.sub(r'<think>[\\s\\S]*?</think>\\s*', '', output_text)\n",
    "        cleaned = re.sub(r'```(?:json)?|```', '', cleaned).strip()\n",
    "        # ???? JSON\n",
    "        json_match = re.search(r'\\{[\\s\\S]*\\}', cleaned)\n",
    "\n",
    "        result = {\n",
    "            'raw_output': output_text[:500],\n",
    "            'gen_tokens': gen_tokens,\n",
    "            'hit_max_new_tokens': gen_tokens >= max_new_tokens,\n",
    "            'time_s': round(dt, 2),\n",
    "            'json_valid': False,\n",
    "            'has_results': False,\n",
    "            'has_recommendations': False,\n",
    "            'has_recomm_short': False,\n",
    "            'results_count': 0,\n",
    "            'prob_sum': 0.0,\n",
    "        }\n",
    "\n",
    "        if json_match:\n",
    "            try:\n",
    "                parsed = json.loads(json_match.group())\n",
    "                result['json_valid'] = True\n",
    "\n",
    "                if isinstance(parsed.get('results'), list) and len(parsed['results']) > 0:\n",
    "                    result['has_results'] = True\n",
    "                    result['results_count'] = len(parsed['results'])\n",
    "                    result['prob_sum'] = sum(\n",
    "                        r.get('probability', 0) for r in parsed['results']\n",
    "                        if isinstance(r, dict)\n",
    "                    )\n",
    "\n",
    "                if isinstance(parsed.get('recommendations'), list) and len(parsed['recommendations']) > 0:\n",
    "                    result['has_recommendations'] = True\n",
    "\n",
    "                if isinstance(parsed.get('recomm_short'), list) and len(parsed['recomm_short']) > 0:\n",
    "                    result['has_recomm_short'] = True\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    mem_peak = torch.cuda.max_memory_allocated() / 1024**3\n",
    "\n",
    "    # ??\n",
    "    del model, tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # ??\n",
    "    n = len(results)\n",
    "    json_valid_rate = sum(1 for r in results if r['json_valid']) / n\n",
    "    has_results_rate = sum(1 for r in results if r['has_results']) / n\n",
    "    has_recs_rate = sum(1 for r in results if r['has_recommendations']) / n\n",
    "    has_short_rate = sum(1 for r in results if r['has_recomm_short']) / n\n",
    "    results_3_rate = sum(1 for r in results if r['results_count'] >= 3) / n\n",
    "    hit_max_new_tokens_rate = sum(1 for r in results if r['hit_max_new_tokens']) / n\n",
    "\n",
    "    prob_valid = [r for r in results if r['has_results'] and r['prob_sum'] > 0]\n",
    "    prob_close_to_1 = sum(1 for r in prob_valid if abs(r['prob_sum'] - 1.0) <= 0.05) / max(len(prob_valid), 1)\n",
    "\n",
    "    return {\n",
    "        'json_valid_rate': round(json_valid_rate, 3),\n",
    "        'has_results_rate': round(has_results_rate, 3),\n",
    "        'results_3_rate': round(results_3_rate, 3),\n",
    "        'has_recs_rate': round(has_recs_rate, 3),\n",
    "        'has_short_rate': round(has_short_rate, 3),\n",
    "        'prob_close_to_1_rate': round(prob_close_to_1, 3),\n",
    "        'hit_max_new_tokens_rate': round(hit_max_new_tokens_rate, 3),\n",
    "        'avg_time_s': round(total_time / n, 2),\n",
    "        'avg_tokens': round(total_tokens / n, 1),\n",
    "        'tok_per_s': round(total_tokens / max(total_time, 0.01), 1),\n",
    "        'vram_loaded_gb': round(mem_loaded, 2),\n",
    "        'vram_peak_gb': round(mem_peak, 2),\n",
    "        'details': results,\n",
    "    }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 9. ??????? rank?\n",
    "eval_results = {}\n",
    "\n",
    "for rank in trained_ranks:\n",
    "    merged_dir = f'{ABLATION_MERGED_ROOT}/rank{rank}'\n",
    "    if not os.path.isdir(merged_dir):\n",
    "        print(f'rank={rank}: merged model not found, skipping')\n",
    "        continue\n",
    "\n",
    "    print('\\n' + '=' * 50)\n",
    "    print(f'Evaluating rank={rank}')\n",
    "    print('=' * 50)\n",
    "\n",
    "    metrics = evaluate_model(merged_dir, test_inputs, max_new_tokens=1024)\n",
    "    metrics['rank'] = rank\n",
    "    metrics['alpha'] = rank * 2\n",
    "    eval_results[rank] = metrics\n",
    "\n",
    "    print(f'  JSON valid:   {metrics[\"json_valid_rate\"]*100:.0f}%')\n",
    "    print(f'  Has results:  {metrics[\"has_results_rate\"]*100:.0f}%')\n",
    "    print(f'  Results?3:    {metrics[\"results_3_rate\"]*100:.0f}%')\n",
    "    print(f'  Has recs:     {metrics[\"has_recs_rate\"]*100:.0f}%')\n",
    "    print(f'  Prob?1.0:     {metrics[\"prob_close_to_1_rate\"]*100:.0f}%')\n",
    "    print(f'  Hit max tok:  {metrics[\"hit_max_new_tokens_rate\"]*100:.0f}%')\n",
    "    print(f'  Avg time:     {metrics[\"avg_time_s\"]}s ({metrics[\"tok_per_s\"]} tok/s)')\n",
    "    print(f'  VRAM peak:    {metrics[\"vram_peak_gb\"]} GB')\n",
    "\n",
    "print('\\n' + '=' * 50)\n",
    "print(f'Evaluation complete! ({len(eval_results)} ranks)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 10. 汇总表格\n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for rank in sorted(eval_results.keys()):\n",
    "    m = eval_results[rank]\n",
    "    # 读取训练时的 GPU stats\n",
    "    train_gpu = {}\n",
    "    gpu_stats_path = f'{ABLATION_OUTPUT_ROOT}/rank{rank}/ablation_gpu_stats.json'\n",
    "    if os.path.exists(gpu_stats_path):\n",
    "        with open(gpu_stats_path) as f:\n",
    "            train_gpu = json.load(f)\n",
    "\n",
    "    # 读取 training loss\n",
    "    final_loss = None\n",
    "    state_path = f'{ABLATION_OUTPUT_ROOT}/rank{rank}/trainer_state.json'\n",
    "    if os.path.exists(state_path):\n",
    "        with open(state_path) as f:\n",
    "            state = json.load(f)\n",
    "        losses = [e['loss'] for e in state.get('log_history', []) if 'loss' in e]\n",
    "        if losses:\n",
    "            final_loss = losses[-1]\n",
    "\n",
    "    rows.append({\n",
    "        'Rank': rank,\n",
    "        'Alpha': rank * 2,\n",
    "        'Final Loss': round(final_loss, 4) if final_loss else 'N/A',\n",
    "        'JSON Valid%': f'{m[\"json_valid_rate\"]*100:.0f}%',\n",
    "        'Results≥3%': f'{m[\"results_3_rate\"]*100:.0f}%',\n",
    "        'Has Recs%': f'{m[\"has_recs_rate\"]*100:.0f}%',\n",
    "        'Prob≈1.0%': f'{m[\"prob_close_to_1_rate\"]*100:.0f}%',\n",
    "        'Infer(s)': m['avg_time_s'],\n",
    "        'Tok/s': m['tok_per_s'],\n",
    "        'Train VRAM(GB)': train_gpu.get('peak_allocated_gb', 'N/A'),\n",
    "        'Infer VRAM(GB)': m['vram_peak_gb'],\n",
    "        'Train Time(min)': train_gpu.get('training_time_min', 'N/A'),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print('\\n=== LoRA Rank Ablation Results ===')\n",
    "display(df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 11. 可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ranks = sorted(eval_results.keys())\n",
    "x = np.arange(len(ranks))\n",
    "labels = [str(r) for r in ranks]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# (1) JSON Valid Rate\n",
    "ax = axes[0, 0]\n",
    "vals = [eval_results[r]['json_valid_rate'] * 100 for r in ranks]\n",
    "bars = ax.bar(x, vals, color='#4CAF50')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('LoRA Rank')\n",
    "ax.set_ylabel('%')\n",
    "ax.set_title('JSON Format Valid Rate')\n",
    "ax.set_ylim(0, 105)\n",
    "for bar, v in zip(bars, vals):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{v:.0f}%', ha='center', fontsize=10)\n",
    "\n",
    "# (2) Results ≥ 3 Rate\n",
    "ax = axes[0, 1]\n",
    "vals = [eval_results[r]['results_3_rate'] * 100 for r in ranks]\n",
    "bars = ax.bar(x, vals, color='#2196F3')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('LoRA Rank')\n",
    "ax.set_ylabel('%')\n",
    "ax.set_title('Results ≥ 3 Conditions Rate')\n",
    "ax.set_ylim(0, 105)\n",
    "for bar, v in zip(bars, vals):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{v:.0f}%', ha='center', fontsize=10)\n",
    "\n",
    "# (3) Prob ≈ 1.0 Rate\n",
    "ax = axes[0, 2]\n",
    "vals = [eval_results[r]['prob_close_to_1_rate'] * 100 for r in ranks]\n",
    "bars = ax.bar(x, vals, color='#FF9800')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('LoRA Rank')\n",
    "ax.set_ylabel('%')\n",
    "ax.set_title('Probability Sum ≈ 1.0 Rate (±0.05)')\n",
    "ax.set_ylim(0, 105)\n",
    "for bar, v in zip(bars, vals):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{v:.0f}%', ha='center', fontsize=10)\n",
    "\n",
    "# (4) Training Loss\n",
    "ax = axes[1, 0]\n",
    "for rank in ranks:\n",
    "    state_path = f'{ABLATION_OUTPUT_ROOT}/rank{rank}/trainer_state.json'\n",
    "    if os.path.exists(state_path):\n",
    "        with open(state_path) as f:\n",
    "            state = json.load(f)\n",
    "        logs = [e for e in state.get('log_history', []) if 'loss' in e]\n",
    "        if logs:\n",
    "            steps = [e['step'] for e in logs]\n",
    "            losses = [e['loss'] for e in logs]\n",
    "            ax.plot(steps, losses, linewidth=1.5, label=f'rank={rank}')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss Curves')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (5) Training VRAM\n",
    "ax = axes[1, 1]\n",
    "train_vrams = []\n",
    "for rank in ranks:\n",
    "    stats_path = f'{ABLATION_OUTPUT_ROOT}/rank{rank}/ablation_gpu_stats.json'\n",
    "    if os.path.exists(stats_path):\n",
    "        with open(stats_path) as f:\n",
    "            stats = json.load(f)\n",
    "        train_vrams.append(stats.get('peak_allocated_gb', 0))\n",
    "    else:\n",
    "        train_vrams.append(0)\n",
    "bars = ax.bar(x, train_vrams, color='#9C27B0')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('LoRA Rank')\n",
    "ax.set_ylabel('GB')\n",
    "ax.set_title('Training Peak VRAM (GB)')\n",
    "for bar, v in zip(bars, train_vrams):\n",
    "    if v > 0:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, f'{v:.1f}', ha='center', fontsize=10)\n",
    "\n",
    "# (6) Inference Speed\n",
    "ax = axes[1, 2]\n",
    "vals = [eval_results[r]['tok_per_s'] for r in ranks]\n",
    "bars = ax.bar(x, vals, color='#F44336')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('LoRA Rank')\n",
    "ax.set_ylabel('Tokens/s')\n",
    "ax.set_title('Inference Throughput (tok/s)')\n",
    "for bar, v in zip(bars, vals):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{v:.0f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.suptitle('LoRA Rank Ablation Study — diagnosis_generator (Qwen3-1.7B)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('ablation_rank_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Chart saved to ablation_rank_results.png')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 12. 保存完整报告到 Google Drive\n",
    "import json, shutil\n",
    "\n",
    "report_dir = f'{DRIVE_ROOT}/docs/ablation'\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "# JSON 原始数据\n",
    "report_data = {}\n",
    "for rank, m in eval_results.items():\n",
    "    report_data[rank] = {k: v for k, v in m.items() if k != 'details'}\n",
    "\n",
    "json_path = f'{report_dir}/ablation_results.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(report_data, f, indent=2, ensure_ascii=False)\n",
    "print(f'JSON saved to {json_path}')\n",
    "\n",
    "# Markdown 报告\n",
    "md_lines = [\n",
    "    '# LoRA Rank Ablation Results',\n",
    "    '',\n",
    "    '## 实验配置',\n",
    "    f'- Agent: diagnosis_generator (Qwen3-1.7B)',\n",
    "    f'- Data: ~800 SFT samples',\n",
    "    f'- Epochs: 2, LR: 1e-4, Batch: 2×8=16',\n",
    "    f'- Alpha = Rank × 2 (固定比值 2.0)',\n",
    "    f'- 测试样本: {len(test_inputs)} 条',\n",
    "    '',\n",
    "    '## 结果对比',\n",
    "    '',\n",
    "    '| Rank | Alpha | Final Loss | JSON Valid | Results≥3 | Prob≈1.0 | Tok/s | Train VRAM(GB) |',\n",
    "    '|------|-------|-----------|-----------|----------|---------|-------|---------------|',\n",
    "]\n",
    "\n",
    "for rank in sorted(eval_results.keys()):\n",
    "    m = eval_results[rank]\n",
    "    # Read final loss\n",
    "    fl = 'N/A'\n",
    "    sp = f'{ABLATION_OUTPUT_ROOT}/rank{rank}/trainer_state.json'\n",
    "    if os.path.exists(sp):\n",
    "        with open(sp) as f:\n",
    "            st = json.load(f)\n",
    "        ls = [e['loss'] for e in st.get('log_history', []) if 'loss' in e]\n",
    "        if ls:\n",
    "            fl = f'{ls[-1]:.4f}'\n",
    "    # Read train VRAM\n",
    "    tv = 'N/A'\n",
    "    gp = f'{ABLATION_OUTPUT_ROOT}/rank{rank}/ablation_gpu_stats.json'\n",
    "    if os.path.exists(gp):\n",
    "        with open(gp) as f:\n",
    "            gs = json.load(f)\n",
    "        tv = f'{gs.get(\"peak_allocated_gb\", \"?\")}'\n",
    "\n",
    "    md_lines.append(\n",
    "        f'| {rank} | {rank*2} | {fl} | '\n",
    "        f'{m[\"json_valid_rate\"]*100:.0f}% | '\n",
    "        f'{m[\"results_3_rate\"]*100:.0f}% | '\n",
    "        f'{m[\"prob_close_to_1_rate\"]*100:.0f}% | '\n",
    "        f'{m[\"tok_per_s\"]} | {tv} |'\n",
    "    )\n",
    "\n",
    "md_path = f'{report_dir}/ablation_results.md'\n",
    "with open(md_path, 'w') as f:\n",
    "    f.write('\\n'.join(md_lines))\n",
    "print(f'Markdown saved to {md_path}')\n",
    "\n",
    "\n",
    "# Copy chart\n",
    "if os.path.exists('ablation_rank_results.png'):\n",
    "    shutil.copy('ablation_rank_results.png', f'{report_dir}/ablation_rank_results.png')\n",
    "    print(f'Chart copied to {report_dir}/')\n",
    "\n",
    "print('\\nAll done! Results saved to Google Drive.')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
