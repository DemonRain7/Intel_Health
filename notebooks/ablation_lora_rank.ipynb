{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Rank Ablation Study\n",
    "\n",
    "对 `diagnosis_generator`（Qwen3-1.7B）做 LoRA rank 消融实验：\n",
    "- 对比 rank = 8, 16, 32, 64, 128\n",
    "- alpha = rank × 2（保持比值 2.0 不变）\n",
    "- 其余超参固定（lr=1e-4, batch=2, grad_acc=8, epochs=2）\n",
    "\n",
    "评估指标：\n",
    "1. Training loss 曲线\n",
    "2. JSON 格式正确率\n",
    "3. 字段完整性（results/recommendations/recomm_short）\n",
    "4. 概率合理性（3 个疾病概率之和 ≈ 1.0）\n",
    "5. 推理速度\n",
    "6. 训练显存占用\n",
    "\n",
    "**注意**: 每个 rank 训练完后需要 **重启 Runtime** 释放显存，再训练下一个。\n",
    "建议按 rank 从小到大依次训练。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 0. Setup\nimport os\nrepo_dir = '/content/Intel_Health'\nif not os.path.exists(repo_dir):\n    !git clone https://github.com/DemonRain7/Intel_Health.git {repo_dir}\nelse:\n    !git -C {repo_dir} pull\n%cd {repo_dir}\n\n# Colab 已预装 torch，不要重装，否则会循环导入报错\n!pip -q install \"transformers>=4.46\" datasets peft accelerate bitsandbytes sentencepiece loguru\n\ntry:\n    from google.colab import userdata\n    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n    print(\"HF_TOKEN loaded from Colab Secrets\")\nexcept (ImportError, Exception):\n    from huggingface_hub import login\n    login()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1. Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/Code_Project/IntelHealth'\n",
    "SFT_DATA_DIR = f'{DRIVE_ROOT}/datasets/agent_sft/diagnosis_generator'\n",
    "ABLATION_OUTPUT_ROOT = f'{DRIVE_ROOT}/models/adapters/ablation'\n",
    "ABLATION_MERGED_ROOT = f'{DRIVE_ROOT}/models/merged/ablation'\n",
    "\n",
    "os.makedirs(ABLATION_OUTPUT_ROOT, exist_ok=True)\n",
    "os.makedirs(ABLATION_MERGED_ROOT, exist_ok=True)\n",
    "\n",
    "# 检查训练数据\n",
    "data_files = [f for f in os.listdir(SFT_DATA_DIR) if f.endswith('.jsonl')]\n",
    "print(f'SFT data dir: {SFT_DATA_DIR}')\n",
    "print(f'Data files: {data_files}')\n",
    "for f in data_files:\n",
    "    path = os.path.join(SFT_DATA_DIR, f)\n",
    "    with open(path) as fp:\n",
    "        count = sum(1 for _ in fp)\n",
    "    print(f'  {f}: {count} samples')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Training（每个 rank 跑一次，跑完重启 Runtime）\n",
    "\n",
    "**修改下面的 `CURRENT_RANK` 后运行此 cell 和下一个 cell。**\n",
    "\n",
    "训练顺序建议：8 → 16 → 32 → 64 → 128（每个跑完后重启 Runtime 再跑下一个）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2. 选择当前要训练的 rank\n",
    "# ============================\n",
    "# 每次只训练一个 rank，训练完重启 Runtime，修改此值再跑\n",
    "CURRENT_RANK = 64  # <-- 修改这里: 8, 16, 32, 64, 128\n",
    "# ============================\n",
    "\n",
    "CURRENT_ALPHA = CURRENT_RANK * 2\n",
    "MODEL_NAME = 'Qwen/Qwen3-1.7B'\n",
    "OUTPUT_DIR = f'{ABLATION_OUTPUT_ROOT}/rank{CURRENT_RANK}'\n",
    "\n",
    "# 检查是否已经训练过\n",
    "if os.path.isdir(OUTPUT_DIR) and os.listdir(OUTPUT_DIR):\n",
    "    print(f'WARNING: {OUTPUT_DIR} already exists and is not empty!')\n",
    "    print('Contents:', os.listdir(OUTPUT_DIR))\n",
    "    print('If you want to retrain, delete the directory first.')\n",
    "else:\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f'\\n=== Ablation Config ===')\n",
    "print(f'Rank:       {CURRENT_RANK}')\n",
    "print(f'Alpha:      {CURRENT_ALPHA}')\n",
    "print(f'Alpha/Rank: {CURRENT_ALPHA / CURRENT_RANK}')\n",
    "print(f'Model:      {MODEL_NAME}')\n",
    "print(f'Data:       {SFT_DATA_DIR}')\n",
    "print(f'Output:     {OUTPUT_DIR}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 3. Train\nimport subprocess, shlex, sys, time as _time, os\n\ncmd = [\n    'python', 'training/supervised_finetuning.py',\n    '--model_name_or_path',          MODEL_NAME,\n    '--tokenizer_name_or_path',      MODEL_NAME,\n    '--train_file_dir',              SFT_DATA_DIR,\n    '--output_dir',                  OUTPUT_DIR,\n    '--template_name',               'qwen',\n    '--do_train',\n    '--fp16',\n    '--gradient_checkpointing',\n    '--per_device_train_batch_size', '2',\n    '--gradient_accumulation_steps',  '8',\n    '--num_train_epochs',            '2',\n    '--learning_rate',               '1e-4',\n    '--lora_rank',                   str(CURRENT_RANK),\n    '--lora_alpha',                  str(CURRENT_ALPHA),\n    '--lora_dropout',                '0.05',\n    '--model_max_length',            '256',\n    '--logging_steps',               '5',\n    '--save_strategy',               'epoch',\n]\n\nprint(f'Training rank={CURRENT_RANK}, alpha={CURRENT_ALPHA}...')\nprint(f'Command: {\" \".join(shlex.quote(x) for x in cmd)}')\nprint('=' * 60)\n\n# Show actual train JSONL files to avoid accidental data contamination\ntrain_jsonl_files = sorted([f for f in os.listdir(SFT_DATA_DIR) if f.endswith('.jsonl')])\nprint(f'Training JSONL files ({len(train_jsonl_files)}): {train_jsonl_files}')\nif any('dapt' in f.lower() for f in train_jsonl_files):\n    print('[WARN] Found dapt JSONL file(s); this may contaminate rank ablation training')\n\n_t0 = _time.time()\nproc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)\nfor line in proc.stdout:\n    print(line, end='', flush=True)\nret = proc.wait()\n_elapsed = _time.time() - _t0\n\nif ret != 0:\n    raise RuntimeError(f'Training failed with exit code {ret}')\nprint(f'\\nTraining rank={CURRENT_RANK} complete in {_elapsed/60:.1f} min!')\nprint(f'Adapter saved to: {OUTPUT_DIR}')\nchild_gpu_stats_path = f'{OUTPUT_DIR}/gpu_stats.json'\nif os.path.exists(child_gpu_stats_path):\n    print(f'Child GPU stats found: {child_gpu_stats_path}')\nelse:\n    print(f'[WARN] Child GPU stats not found: {child_gpu_stats_path}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 4. Save GPU stats after training (read from child process)\n",
    "import json, os\n",
    "\n",
    "child_stats_path = f'{OUTPUT_DIR}/gpu_stats.json'\n",
    "gpu_stats = {\n",
    "    'rank': CURRENT_RANK,\n",
    "    'alpha': CURRENT_ALPHA,\n",
    "    'training_time_min': round(_elapsed / 60, 1),\n",
    "}\n",
    "\n",
    "if os.path.exists(child_stats_path):\n",
    "    with open(child_stats_path) as f:\n",
    "        child_stats = json.load(f)\n",
    "    gpu_stats.update({\n",
    "        'gpu_name': child_stats.get('gpu_name'),\n",
    "        'total_gb': child_stats.get('total_gb'),\n",
    "        'peak_allocated_gb': child_stats.get('peak_allocated_gb'),\n",
    "        'peak_reserved_gb': child_stats.get('peak_reserved_gb'),\n",
    "        'stats_source': 'training/supervised_finetuning.py',\n",
    "    })\n",
    "else:\n",
    "    print(f'[WARN] Child GPU stats missing: {child_stats_path}')\n",
    "    gpu_stats['stats_source'] = 'fallback_no_child_stats'\n",
    "\n",
    "stats_path = f'{OUTPUT_DIR}/ablation_gpu_stats.json'\n",
    "with open(stats_path, 'w') as f:\n",
    "    json.dump(gpu_stats, f, indent=2)\n",
    "print(f'GPU stats saved to {stats_path}')\n",
    "print(json.dumps(gpu_stats, indent=2))\n",
    "\n",
    "print('\\n>>> Training done. Restart runtime, set CURRENT_RANK, and run next rank.')\n",
    "print('>>> After all ranks are trained, go to Part B for merge + evaluation.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B: Training Metrics Tradeoff (Run After All Ranks Are Trained)\n",
    "\n",
    "This section only uses hard training metrics:\n",
    "- Training loss (first / final / best)\n",
    "- Peak training VRAM\n",
    "- Training runtime and throughput\n",
    "- Adapter size\n",
    "\n",
    "No inference-format metrics (such as JSON valid rate or results>=3) are used in this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 5. Discover available rank runs\n",
    "import os\n",
    "\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/Code_Project/IntelHealth'\n",
    "ABLATION_OUTPUT_ROOT = f'{DRIVE_ROOT}/models/adapters/ablation'\n",
    "RANKS = [8, 16, 32, 64, 128]\n",
    "\n",
    "trained_ranks = []\n",
    "print('Checking rank directories...')\n",
    "for rank in RANKS:\n",
    "    rank_dir = f'{ABLATION_OUTPUT_ROOT}/rank{rank}'\n",
    "    if not os.path.isdir(rank_dir):\n",
    "        print(f'  rank={rank:>3}: NOT FOUND ({rank_dir})')\n",
    "        continue\n",
    "\n",
    "    has_state = os.path.exists(f'{rank_dir}/trainer_state.json')\n",
    "    has_train_results = os.path.exists(f'{rank_dir}/train_results.json')\n",
    "    has_adapter = os.path.exists(f'{rank_dir}/adapter_config.json') and os.path.exists(f'{rank_dir}/adapter_model.safetensors')\n",
    "    has_gpu_stats = os.path.exists(f'{rank_dir}/ablation_gpu_stats.json') or os.path.exists(f'{rank_dir}/gpu_stats.json')\n",
    "\n",
    "    ready = has_state and has_train_results and has_adapter\n",
    "    status = 'READY' if ready else 'INCOMPLETE'\n",
    "    print(\n",
    "        f'  rank={rank:>3}: {status} '\n",
    "        f'(state={has_state}, train_results={has_train_results}, adapter={has_adapter}, gpu_stats={has_gpu_stats})'\n",
    "    )\n",
    "\n",
    "    if ready:\n",
    "        trained_ranks.append(rank)\n",
    "\n",
    "print(f'\\nReady ranks: {trained_ranks}')\n",
    "if not trained_ranks:\n",
    "    raise RuntimeError('No READY ranks found. Run Part A training first.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 6. Load per-rank training metrics\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def _read_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _loss_stats_from_state(state):\n",
    "    losses = [e['loss'] for e in state.get('log_history', []) if 'loss' in e]\n",
    "    if not losses:\n",
    "        return None, None, None\n",
    "    first_loss = losses[0]\n",
    "    final_loss = losses[-1]\n",
    "    best_loss = min(losses)\n",
    "    return first_loss, final_loss, best_loss\n",
    "\n",
    "rows = []\n",
    "for rank in trained_ranks:\n",
    "    rank_dir = f'{ABLATION_OUTPUT_ROOT}/rank{rank}'\n",
    "\n",
    "    state = _read_json(f'{rank_dir}/trainer_state.json')\n",
    "    train_results = _read_json(f'{rank_dir}/train_results.json')\n",
    "    adapter_cfg = _read_json(f'{rank_dir}/adapter_config.json')\n",
    "\n",
    "    gpu_stats_path = f'{rank_dir}/ablation_gpu_stats.json'\n",
    "    if not os.path.exists(gpu_stats_path):\n",
    "        gpu_stats_path = f'{rank_dir}/gpu_stats.json'\n",
    "    gpu_stats = _read_json(gpu_stats_path) if os.path.exists(gpu_stats_path) else {}\n",
    "\n",
    "    first_loss, final_loss, best_loss = _loss_stats_from_state(state)\n",
    "    loss_drop = (first_loss - final_loss) if (first_loss is not None and final_loss is not None) else None\n",
    "\n",
    "    adapter_path = f'{rank_dir}/adapter_model.safetensors'\n",
    "    adapter_size_mb = round(os.path.getsize(adapter_path) / (1024**2), 1) if os.path.exists(adapter_path) else None\n",
    "\n",
    "    runtime_min = None\n",
    "    if 'train_runtime' in train_results:\n",
    "        runtime_min = train_results['train_runtime'] / 60.0\n",
    "    elif 'training_time_min' in gpu_stats:\n",
    "        runtime_min = gpu_stats['training_time_min']\n",
    "\n",
    "    rows.append({\n",
    "        'Rank': rank,\n",
    "        'Alpha': int(adapter_cfg.get('lora_alpha', rank * 2)),\n",
    "        'First Loss': round(first_loss, 4) if first_loss is not None else None,\n",
    "        'Final Loss': round(final_loss, 4) if final_loss is not None else None,\n",
    "        'Best Loss': round(best_loss, 4) if best_loss is not None else None,\n",
    "        'Loss Drop': round(loss_drop, 4) if loss_drop is not None else None,\n",
    "        'Train Runtime (min)': round(runtime_min, 2) if runtime_min is not None else None,\n",
    "        'Train Samples/s': round(train_results.get('train_samples_per_second', 0), 3),\n",
    "        'Train Steps/s': round(train_results.get('train_steps_per_second', 0), 3),\n",
    "        'Peak VRAM (GB)': gpu_stats.get('peak_allocated_gb'),\n",
    "        'Peak Reserved VRAM (GB)': gpu_stats.get('peak_reserved_gb'),\n",
    "        'Adapter Size (MB)': adapter_size_mb,\n",
    "        'Global Steps': state.get('global_step'),\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(rows).sort_values('Rank').reset_index(drop=True)\n",
    "\n",
    "print('=== Training Metrics By Rank ===')\n",
    "display(metrics_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 7. Compute balance score + Pareto front (loss vs VRAM)\n",
    "import numpy as np\n",
    "\n",
    "if metrics_df.empty:\n",
    "    raise RuntimeError('metrics_df is empty')\n",
    "\n",
    "def _minmax(series):\n",
    "    s = series.astype(float)\n",
    "    lo, hi = s.min(), s.max()\n",
    "    if hi - lo < 1e-12:\n",
    "        return pd.Series([0.0] * len(s), index=s.index)\n",
    "    return (s - lo) / (hi - lo)\n",
    "\n",
    "metrics_df['Norm Final Loss'] = _minmax(metrics_df['Final Loss'])\n",
    "metrics_df['Norm Peak VRAM'] = _minmax(metrics_df['Peak VRAM (GB)'])\n",
    "\n",
    "# You can adjust these weights based on interview story\n",
    "W_LOSS = 0.5\n",
    "W_VRAM = 0.5\n",
    "metrics_df['Balance Score'] = (\n",
    "    W_LOSS * metrics_df['Norm Final Loss'] +\n",
    "    W_VRAM * metrics_df['Norm Peak VRAM']\n",
    ")\n",
    "\n",
    "pareto = []\n",
    "for i, row in metrics_df.iterrows():\n",
    "    dominated = False\n",
    "    for j, other in metrics_df.iterrows():\n",
    "        if i == j:\n",
    "            continue\n",
    "        not_worse = (\n",
    "            other['Final Loss'] <= row['Final Loss'] and\n",
    "            other['Peak VRAM (GB)'] <= row['Peak VRAM (GB)']\n",
    "        )\n",
    "        strictly_better = (\n",
    "            other['Final Loss'] < row['Final Loss'] or\n",
    "            other['Peak VRAM (GB)'] < row['Peak VRAM (GB)']\n",
    "        )\n",
    "        if not_worse and strictly_better:\n",
    "            dominated = True\n",
    "            break\n",
    "    pareto.append(not dominated)\n",
    "\n",
    "metrics_df['Pareto Optimal'] = pareto\n",
    "\n",
    "ranked_df = metrics_df.sort_values(\n",
    "    by=['Pareto Optimal', 'Balance Score', 'Rank'],\n",
    "    ascending=[False, True, True]\n",
    ").reset_index(drop=True)\n",
    "recommended = ranked_df.iloc[0]\n",
    "\n",
    "show_cols = [\n",
    "    'Rank', 'Alpha', 'Final Loss', 'Best Loss', 'Loss Drop',\n",
    "    'Peak VRAM (GB)', 'Train Runtime (min)', 'Train Samples/s',\n",
    "    'Adapter Size (MB)', 'Pareto Optimal', 'Balance Score'\n",
    "]\n",
    "\n",
    "print('=== Ranked by Training Tradeoff ===')\n",
    "display(ranked_df[show_cols])\n",
    "print(\n",
    "    f\"Recommended rank by training tradeoff: rank={int(recommended['Rank'])} \"\n",
    "    f\"(FinalLoss={recommended['Final Loss']}, PeakVRAM={recommended['Peak VRAM (GB)']}GB, \"\n",
    "    f\"Score={recommended['Balance Score']:.4f})\"\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 8. Visualize hard training metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plot_df = metrics_df.sort_values('Rank').reset_index(drop=True)\n",
    "x = np.arange(len(plot_df))\n",
    "labels = [str(r) for r in plot_df['Rank']]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# (1) Final vs Best loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(labels, plot_df['Final Loss'], marker='o', label='Final Loss')\n",
    "ax.plot(labels, plot_df['Best Loss'], marker='s', label='Best Loss')\n",
    "ax.set_title('Loss vs Rank')\n",
    "ax.set_xlabel('LoRA Rank')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# (2) Peak VRAM\n",
    "ax = axes[0, 1]\n",
    "bars = ax.bar(labels, plot_df['Peak VRAM (GB)'], color='#3f8efc')\n",
    "ax.set_title('Peak Training VRAM vs Rank')\n",
    "ax.set_xlabel('LoRA Rank')\n",
    "ax.set_ylabel('GB')\n",
    "for b, v in zip(bars, plot_df['Peak VRAM (GB)']):\n",
    "    ax.text(b.get_x() + b.get_width()/2, b.get_height() + 0.03, f'{v:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "# (3) Runtime\n",
    "ax = axes[1, 0]\n",
    "bars = ax.bar(labels, plot_df['Train Runtime (min)'], color='#f39c12')\n",
    "ax.set_title('Training Runtime vs Rank')\n",
    "ax.set_xlabel('LoRA Rank')\n",
    "ax.set_ylabel('Minutes')\n",
    "for b, v in zip(bars, plot_df['Train Runtime (min)']):\n",
    "    ax.text(b.get_x() + b.get_width()/2, b.get_height() + 0.05, f'{v:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "# (4) Pareto scatter: Loss vs VRAM\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(plot_df['Peak VRAM (GB)'], plot_df['Final Loss'], s=80, color='#808080', label='Rank')\n",
    "pareto_df = plot_df[plot_df['Pareto Optimal']]\n",
    "ax.scatter(pareto_df['Peak VRAM (GB)'], pareto_df['Final Loss'], s=140, color='#e74c3c', label='Pareto optimal')\n",
    "for _, r in plot_df.iterrows():\n",
    "    ax.text(r['Peak VRAM (GB)'] + 0.02, r['Final Loss'] + 0.002, f\"r{int(r['Rank'])}\", fontsize=9)\n",
    "ax.set_title('Loss-VRAM Pareto View')\n",
    "ax.set_xlabel('Peak VRAM (GB)')\n",
    "ax.set_ylabel('Final Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = 'ablation_rank_training_tradeoff.png'\n",
    "fig.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Plot saved to: {plot_path}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 9. Auto-generate interview talking points\n",
    "top3 = ranked_df.head(3)[['Rank', 'Final Loss', 'Peak VRAM (GB)', 'Train Runtime (min)', 'Balance Score']]\n",
    "\n",
    "print('=== Interview Talking Points (Training Metrics Only) ===')\n",
    "print(f\"1) We compared LoRA rank {sorted(metrics_df['Rank'].tolist())} under fixed training setup.\")\n",
    "print(\n",
    "    f\"2) Recommended rank={int(recommended['Rank'])} by balancing final loss and peak VRAM \"\n",
    "    f\"(weights: loss={W_LOSS}, vram={W_VRAM}).\"\n",
    ")\n",
    "print(\n",
    "    f\"3) Recommended rank metrics: final_loss={recommended['Final Loss']}, \"\n",
    "    f\"peak_vram={recommended['Peak VRAM (GB)']}GB, runtime={recommended['Train Runtime (min)']}min.\"\n",
    ")\n",
    "print('4) We used only hard training metrics for this decision (no format-dependent inference metric).')\n",
    "\n",
    "print('\\nTop-3 ranks by Balance Score:')\n",
    "display(top3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 10. Concise table for slides/interview\n",
    "interview_df = ranked_df[[\n",
    "    'Rank', 'Alpha', 'Final Loss', 'Best Loss', 'Peak VRAM (GB)',\n",
    "    'Train Runtime (min)', 'Train Samples/s', 'Adapter Size (MB)',\n",
    "    'Pareto Optimal', 'Balance Score'\n",
    "]].copy()\n",
    "\n",
    "display(interview_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 11. Optional: rank pick policy notes\n",
    "print('Rank selection policy used in this notebook:')\n",
    "print('- Keep training setup fixed across ranks.')\n",
    "print('- Compare Final Loss and Peak VRAM as primary axes.')\n",
    "print('- Use Pareto front + weighted balance score to pick final rank.')\n",
    "print('- Keep inference-format metrics out of this decision report.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 12. Save hard-metrics report to Google Drive\n",
    "import json, shutil\n",
    "\n",
    "report_dir = f'{DRIVE_ROOT}/docs/ablation'\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "report_payload = {\n",
    "    'metric_scope': 'training_only',\n",
    "    'weights': {'loss': W_LOSS, 'vram': W_VRAM},\n",
    "    'recommended_rank': int(recommended['Rank']),\n",
    "    'recommended_metrics': {\n",
    "        'final_loss': float(recommended['Final Loss']),\n",
    "        'best_loss': float(recommended['Best Loss']),\n",
    "        'peak_vram_gb': float(recommended['Peak VRAM (GB)']),\n",
    "        'train_runtime_min': float(recommended['Train Runtime (min)']),\n",
    "        'balance_score': float(recommended['Balance Score']),\n",
    "    },\n",
    "    'ranks': ranked_df.to_dict(orient='records'),\n",
    "}\n",
    "\n",
    "json_path = f'{report_dir}/ablation_train_tradeoff.json'\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(report_payload, f, indent=2, ensure_ascii=False)\n",
    "print(f'JSON saved to: {json_path}')\n",
    "\n",
    "md_lines = [\n",
    "    '# LoRA Rank Training Tradeoff Report',\n",
    "    '',\n",
    "    '- Metric scope: training-only (no inference-format metrics).',\n",
    "    f'- Recommended rank: **{int(recommended[\"Rank\"])}**',\n",
    "    f'- Balance weights: loss={W_LOSS}, vram={W_VRAM}',\n",
    "    '',\n",
    "    '## Core Metrics by Rank',\n",
    "    '',\n",
    "    '| Rank | Alpha | Final Loss | Best Loss | Peak VRAM(GB) | Runtime(min) | Samples/s | Balance Score | Pareto |',\n",
    "    '|---:|---:|---:|---:|---:|---:|---:|---:|:---:|',\n",
    "]\n",
    "\n",
    "for _, r in ranked_df.iterrows():\n",
    "    md_lines.append(\n",
    "        f\"| {int(r['Rank'])} | {int(r['Alpha'])} | {r['Final Loss']:.4f} | {r['Best Loss']:.4f} | \"\n",
    "        f\"{r['Peak VRAM (GB)']:.2f} | {r['Train Runtime (min)']:.2f} | {r['Train Samples/s']:.3f} | \"\n",
    "        f\"{r['Balance Score']:.4f} | {'Y' if r['Pareto Optimal'] else 'N'} |\"\n",
    "    )\n",
    "\n",
    "md_path = f'{report_dir}/ablation_train_tradeoff.md'\n",
    "with open(md_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(md_lines))\n",
    "print(f'Markdown saved to: {md_path}')\n",
    "\n",
    "if os.path.exists('ablation_rank_training_tradeoff.png'):\n",
    "    out_plot = f'{report_dir}/ablation_rank_training_tradeoff.png'\n",
    "    shutil.copy('ablation_rank_training_tradeoff.png', out_plot)\n",
    "    print(f'Plot copied to: {out_plot}')\n",
    "\n",
    "print('\\nDone. Training tradeoff report is ready for interview use.')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}