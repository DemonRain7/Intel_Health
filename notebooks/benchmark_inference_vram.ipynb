{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IntelHealth — Inference VRAM Benchmark (Colab)\n",
    "\n",
    "测量每个 Agent 模型在 GPU 上的推理显存占用和速度。\n",
    "\n",
    "- 逐个加载 merged 模型（或回退到 base model）\n",
    "- 记录：加载后显存、推理峰值显存、推理时间、输出 token 数\n",
    "- 生成对比表格和柱状图\n",
    "\n",
    "**前置条件**: merged 模型已上传到 Google Drive `models/merged/` 目录"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 0. Clone repo + install deps\nimport os\nrepo_dir = '/content/Intel_Health'\nif not os.path.exists(repo_dir):\n    !git clone https://github.com/DemonRain7/Intel_Health.git {repo_dir}\nelse:\n    !git -C {repo_dir} pull\n%cd {repo_dir}\n\n# Colab 已预装 torch，不要重装，否则会循环导入报错\n!pip -q install \"transformers>=4.46\" sentencepiece accelerate\n\n# HuggingFace login (for base model fallback)\ntry:\n    from google.colab import userdata\n    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n    print(\"HF_TOKEN loaded from Colab Secrets\")\nexcept (ImportError, Exception):\n    from huggingface_hub import login\n    login()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1. Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/Code_Project/IntelHealth'\n",
    "MERGED_MODELS_DIR = f'{DRIVE_ROOT}/models/merged'\n",
    "\n",
    "# 检查有哪些 merged 模型\n",
    "if os.path.isdir(MERGED_MODELS_DIR):\n",
    "    print('Merged models found:')\n",
    "    for d in sorted(os.listdir(MERGED_MODELS_DIR)):\n",
    "        full = os.path.join(MERGED_MODELS_DIR, d)\n",
    "        if os.path.isdir(full):\n",
    "            has_weights = any(f.endswith('.safetensors') or f.endswith('.bin') for f in os.listdir(full))\n",
    "            print(f'  {d} {\"✓\" if has_weights else \"(empty)\"}')\n",
    "else:\n",
    "    print(f'WARNING: {MERGED_MODELS_DIR} not found, will use base models only')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2. 配置\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Agent 定义: agent_name → (base_model_id, has_sft)\n",
    "AGENTS = {\n",
    "    \"symptom_normalizer\":     (\"Qwen/Qwen3-0.6B\", True),\n",
    "    \"symptom_quality_grader\": (\"Qwen/Qwen3-0.6B\", True),\n",
    "    \"rag_relevance_grader\":   (\"Qwen/Qwen3-0.6B\", True),\n",
    "    \"drug_evidence_grader\":   (\"Qwen/Qwen3-0.6B\", True),\n",
    "    \"diagnosis_generator\":    (\"Qwen/Qwen3-1.7B\", True),\n",
    "    \"drug_recommender\":       (\"Qwen/Qwen3-0.6B\", False),\n",
    "    \"diagnosis_reviewer\":     (\"Qwen/Qwen3-1.7B\", False),\n",
    "}\n",
    "\n",
    "# 测试 prompt（模拟真实 pipeline 输入）\n",
    "TEST_MESSAGES = [\n",
    "    {\"role\": \"system\", \"content\": \"你是医学助理，请将用户口语症状整理为专业、结构化描述。只输出JSON。\"},\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"身体部位: 背部\\n主要症状: 肩颈疼痛，脊椎僵硬\\n\"\n",
    "        \"其他症状: 洗碗久了腰就不舒服\\n严重程度: 3\\n持续时间: 超过4周\\n\\n\"\n",
    "        '{\"optimized_symptoms\": \"...\", \"rag_keywords\": [\"...\"]}'\n",
    "    )},\n",
    "]\n",
    "\n",
    "MAX_NEW_TOKENS = 200\n",
    "\n",
    "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "print(f'Agents to benchmark: {len(AGENTS)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3. Benchmark 函数\n",
    "\n",
    "def get_gpu_mem_mb():\n",
    "    return torch.cuda.memory_allocated() / 1024 / 1024\n",
    "\n",
    "def get_gpu_peak_mb():\n",
    "    return torch.cuda.max_memory_allocated() / 1024 / 1024\n",
    "\n",
    "def get_model_param_mb(model):\n",
    "    return sum(p.numel() * p.element_size() for p in model.parameters()) / 1024 / 1024\n",
    "\n",
    "def resolve_model_source(agent_name):\n",
    "    \"\"\"优先使用 merged 模型，不存在则回退到 base model。\"\"\"\n",
    "    merged_path = os.path.join(MERGED_MODELS_DIR, agent_name)\n",
    "    if os.path.isdir(merged_path):\n",
    "        has_weights = any(\n",
    "            f.endswith('.safetensors') or f.endswith('.bin')\n",
    "            for f in os.listdir(merged_path)\n",
    "        )\n",
    "        if has_weights:\n",
    "            return merged_path, 'merged'\n",
    "    base_model, _ = AGENTS[agent_name]\n",
    "    return base_model, 'base'\n",
    "\n",
    "\n",
    "def benchmark_one(agent_name):\n",
    "    \"\"\"对单个 agent 做推理 benchmark，返回结果 dict。\"\"\"\n",
    "    source, source_type = resolve_model_source(agent_name)\n",
    "    base_model, has_sft = AGENTS[agent_name]\n",
    "\n",
    "    # 清理显存\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # 加载模型\n",
    "    t0 = time.time()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(source, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        source,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='cuda',\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    load_time = time.time() - t0\n",
    "\n",
    "    params_mb = get_model_param_mb(model)\n",
    "    mem_after_load = get_gpu_mem_mb()\n",
    "\n",
    "    # 构建输入\n",
    "    text = tokenizer.apply_chat_template(TEST_MESSAGES, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "    input_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "    # 推理\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    t1 = time.time()\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    infer_time = time.time() - t1\n",
    "    output_tokens = output_ids.shape[1] - input_len\n",
    "    mem_peak_infer = get_gpu_peak_mb()\n",
    "\n",
    "    # 清理\n",
    "    del model, tokenizer, inputs, output_ids\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        'agent': agent_name,\n",
    "        'base_model': base_model,\n",
    "        'sft': 'Yes' if has_sft else 'No (base)',\n",
    "        'source_type': source_type,\n",
    "        'params_mb': round(params_mb, 1),\n",
    "        'load_time_s': round(load_time, 1),\n",
    "        'infer_time_s': round(infer_time, 1),\n",
    "        'output_tokens': output_tokens,\n",
    "        'tok_per_s': round(output_tokens / max(infer_time, 0.01), 1),\n",
    "        'mem_after_load_mb': round(mem_after_load, 1),\n",
    "        'mem_peak_infer_mb': round(mem_peak_infer, 1),\n",
    "    }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 4. 运行 Benchmark（逐个 agent）\n",
    "results = []\n",
    "\n",
    "for agent_name in AGENTS:\n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'Benchmarking: {agent_name}')\n",
    "    print(f'{\"=\"*50}')\n",
    "    try:\n",
    "        r = benchmark_one(agent_name)\n",
    "        results.append(r)\n",
    "        print(f'  Source:     {r[\"source_type\"]}')\n",
    "        print(f'  Params:     {r[\"params_mb\"]} MB')\n",
    "        print(f'  Load:       {r[\"load_time_s\"]}s')\n",
    "        print(f'  Inference:  {r[\"infer_time_s\"]}s ({r[\"output_tokens\"]} tokens, {r[\"tok_per_s\"]} tok/s)')\n",
    "        print(f'  VRAM load:  {r[\"mem_after_load_mb\"]} MB')\n",
    "        print(f'  VRAM peak:  {r[\"mem_peak_infer_mb\"]} MB')\n",
    "    except Exception as e:\n",
    "        print(f'  ERROR: {e}')\n",
    "        results.append({'agent': agent_name, 'error': str(e)})\n",
    "\n",
    "print(f'\\n{\"=\"*50}')\n",
    "print(f'All benchmarks complete! ({len(results)} agents)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 5. 结果表格\n",
    "import pandas as pd\n",
    "\n",
    "valid = [r for r in results if 'error' not in r]\n",
    "\n",
    "df = pd.DataFrame(valid)\n",
    "display_cols = ['agent', 'base_model', 'sft', 'source_type', 'params_mb',\n",
    "                'load_time_s', 'infer_time_s', 'output_tokens', 'tok_per_s',\n",
    "                'mem_after_load_mb', 'mem_peak_infer_mb']\n",
    "df = df[display_cols]\n",
    "df.columns = ['Agent', 'Base Model', 'SFT', 'Source', 'Params(MB)',\n",
    "              'Load(s)', 'Infer(s)', 'Tokens', 'Tok/s',\n",
    "              'VRAM Load(MB)', 'VRAM Peak(MB)']\n",
    "\n",
    "print('\\n=== Inference VRAM Benchmark ===')\n",
    "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'Total GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "print(f'Precision: float16')\n",
    "print(f'max_new_tokens: {MAX_NEW_TOKENS}')\n",
    "print()\n",
    "display(df)\n",
    "\n",
    "total_infer = sum(r['infer_time_s'] for r in valid)\n",
    "print(f'\\nPipeline 总推理时间（不含加载）: {total_infer:.1f}s')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 6. 可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "valid = [r for r in results if 'error' not in r]\n",
    "agents = [r['agent'].replace('_', '\\n') for r in valid]\n",
    "x = np.arange(len(agents))\n",
    "\n",
    "# 按模型大小着色: 0.6B=蓝, 1.7B=橙\n",
    "colors = ['#2196F3' if '0.6B' in r['base_model'] else '#FF9800' for r in valid]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# (1) VRAM 加载后\n",
    "ax = axes[0, 0]\n",
    "ax.bar(x, [r['mem_after_load_mb'] for r in valid], color=colors)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(agents, fontsize=8)\n",
    "ax.set_ylabel('MB')\n",
    "ax.set_title('VRAM After Model Load (MB)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# (2) VRAM 推理峰值\n",
    "ax = axes[0, 1]\n",
    "ax.bar(x, [r['mem_peak_infer_mb'] for r in valid], color=colors)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(agents, fontsize=8)\n",
    "ax.set_ylabel('MB')\n",
    "ax.set_title('VRAM Peak During Inference (MB)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# (3) 推理时间\n",
    "ax = axes[1, 0]\n",
    "ax.bar(x, [r['infer_time_s'] for r in valid], color=colors)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(agents, fontsize=8)\n",
    "ax.set_ylabel('Seconds')\n",
    "ax.set_title('Inference Time (s)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# (4) Token 吞吐量\n",
    "ax = axes[1, 1]\n",
    "ax.bar(x, [r['tok_per_s'] for r in valid], color=colors)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(agents, fontsize=8)\n",
    "ax.set_ylabel('Tokens/s')\n",
    "ax.set_title('Throughput (tok/s)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='#2196F3', label='Qwen3-0.6B'),\n",
    "                   Patch(facecolor='#FF9800', label='Qwen3-1.7B')]\n",
    "fig.legend(handles=legend_elements, loc='upper center', ncol=2, fontsize=11,\n",
    "           bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "plt.suptitle(f'IntelHealth Inference Benchmark — {torch.cuda.get_device_name(0)} (fp16)',\n",
    "             fontsize=13, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig('inference_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Chart saved to inference_benchmark.png')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 7. 保存结果到 Google Drive\n",
    "import json\n",
    "\n",
    "output_dir = f'{DRIVE_ROOT}/docs/benchmark'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# JSON\n",
    "json_path = f'{output_dir}/inference_benchmark.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "print(f'JSON saved to {json_path}')\n",
    "\n",
    "# Markdown 报告\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "\n",
    "lines = [\n",
    "    '# IntelHealth 推理显存 Benchmark',\n",
    "    '',\n",
    "    f'- **GPU**: {gpu_name} ({gpu_total:.1f} GB)',\n",
    "    f'- **精度**: float16',\n",
    "    f'- **max_new_tokens**: {MAX_NEW_TOKENS}',\n",
    "    '',\n",
    "    '| Agent | Base Model | SFT | Params(MB) | Load(s) | Infer(s) | Tokens | Tok/s | VRAM Load(MB) | VRAM Peak(MB) |',\n",
    "    '|-------|-----------|-----|-----------|--------|--------|--------|-------|--------------|--------------|',\n",
    "]\n",
    "for r in results:\n",
    "    if 'error' in r:\n",
    "        lines.append(f'| {r[\"agent\"]} | ERROR | | | | | | | | {r[\"error\"]} |')\n",
    "    else:\n",
    "        lines.append(\n",
    "            f'| {r[\"agent\"]} | {r[\"base_model\"]} | {r[\"sft\"]} | '\n",
    "            f'{r[\"params_mb\"]} | {r[\"load_time_s\"]} | {r[\"infer_time_s\"]} | '\n",
    "            f'{r[\"output_tokens\"]} | {r[\"tok_per_s\"]} | '\n",
    "            f'{r[\"mem_after_load_mb\"]} | {r[\"mem_peak_infer_mb\"]} |'\n",
    "        )\n",
    "\n",
    "md_path = f'{output_dir}/inference_benchmark.md'\n",
    "with open(md_path, 'w') as f:\n",
    "    f.write('\\n'.join(lines))\n",
    "print(f'Markdown saved to {md_path}')\n",
    "\n",
    "# 也拷贝图表\n",
    "import shutil\n",
    "if os.path.exists('inference_benchmark.png'):\n",
    "    shutil.copy('inference_benchmark.png', f'{output_dir}/inference_benchmark.png')\n",
    "    print(f'Chart copied to {output_dir}/inference_benchmark.png')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}