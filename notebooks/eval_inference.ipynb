{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IntelHealth Eval Inference (Base vs Finetuned)\n",
    "\n",
    "This notebook compares API inference quality/latency for two model settings and exports metrics JSON for Home charts."
   ]
  },
  {
   "cell_type": "code",
   "id": "uriq9v2rz4m",
   "source": "# Clone repo (skip if already cloned)\nimport os\nif not os.path.exists('Intel_Health'):\n    !git clone https://github.com/DemonRain7/Intel_Health.git\n%cd Intel_Health",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, re\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "API_BASE_URL = 'http://127.0.0.1:8081'  # change if needed\n",
    "ACCESS_TOKEN = 'YOUR_SUPABASE_ACCESS_TOKEN'\n",
    "\n",
    "BASE_PROFILE = 'fast'\n",
    "FINETUNED_PROFILE = 'balanced'\n",
    "\n",
    "EVAL_CASES = Path('data_clean/eval/eval_cases.jsonl')\n",
    "OUT_METRICS = Path('src/assets/metrics/training_metrics.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cases(path: Path):\n",
    "    rows = []\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line).get('input', {}))\n",
    "    return rows\n",
    "\n",
    "def extract_text(output_obj):\n",
    "    if not isinstance(output_obj, dict):\n",
    "        return ''\n",
    "    parts = []\n",
    "    for r in output_obj.get('results', []):\n",
    "        if isinstance(r, dict):\n",
    "            parts.append(str(r.get('condition', '')))\n",
    "            parts.append(str(r.get('description', '')))\n",
    "    parts.extend([str(x) for x in output_obj.get('recommendations', [])])\n",
    "    return ' '.join(parts)\n",
    "\n",
    "def keyword_coverage(case_input, output_obj):\n",
    "    text = extract_text(output_obj)\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    keywords = []\n",
    "    body_part = case_input.get('body_part')\n",
    "    if body_part:\n",
    "        keywords.append(str(body_part))\n",
    "    keywords.extend([str(x) for x in case_input.get('symptoms', [])])\n",
    "    other = str(case_input.get('other_symptoms', '')).strip()\n",
    "    if other:\n",
    "        keywords.extend([x for x in re.split(r'[,ï¼Œ\\s]+', other) if x])\n",
    "    if not keywords:\n",
    "        return 0.0\n",
    "    hit = sum(1 for k in keywords if k and k in text)\n",
    "    return hit / len(keywords)\n",
    "\n",
    "def is_valid_response(resp_json):\n",
    "    if not isinstance(resp_json, dict):\n",
    "        return False\n",
    "    results = resp_json.get('results')\n",
    "    recs = resp_json.get('recommendations')\n",
    "    return isinstance(results, list) and len(results) >= 1 and isinstance(recs, list)\n",
    "\n",
    "def run_batch(cases, profile_id):\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {ACCESS_TOKEN}',\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "    valid = 0\n",
    "    coverage_scores = []\n",
    "    latencies = []\n",
    "\n",
    "    for item in cases:\n",
    "        payload = dict(item)\n",
    "        payload['model_profile_id'] = profile_id\n",
    "\n",
    "        t0 = time.time()\n",
    "        resp = requests.post(f'{API_BASE_URL}/api/diagnoses', headers=headers, json=payload, timeout=120)\n",
    "        dt = time.time() - t0\n",
    "        latencies.append(dt)\n",
    "\n",
    "        if resp.status_code >= 400:\n",
    "            coverage_scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        data = resp.json()\n",
    "        output_obj = {\n",
    "            'results': data.get('results', []),\n",
    "            'recommendations': data.get('recommendations', []),\n",
    "        }\n",
    "        if is_valid_response(output_obj):\n",
    "            valid += 1\n",
    "        coverage_scores.append(keyword_coverage(item, output_obj))\n",
    "\n",
    "    n = max(1, len(cases))\n",
    "    return {\n",
    "        'json_valid_rate': valid / n,\n",
    "        'keyword_coverage': sum(coverage_scores) / n,\n",
    "        'latency_sec': sum(latencies) / n,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = load_cases(EVAL_CASES)\n",
    "base_metrics = run_batch(cases, BASE_PROFILE)\n",
    "finetuned_metrics = run_batch(cases, FINETUNED_PROFILE)\n",
    "\n",
    "metrics = {\n",
    "    'labels': ['base', 'finetuned'],\n",
    "    'json_valid_rate': [round(base_metrics['json_valid_rate'], 4), round(finetuned_metrics['json_valid_rate'], 4)],\n",
    "    'keyword_coverage': [round(base_metrics['keyword_coverage'], 4), round(finetuned_metrics['keyword_coverage'], 4)],\n",
    "    'latency_sec': [round(base_metrics['latency_sec'], 4), round(finetuned_metrics['latency_sec'], 4)]\n",
    "}\n",
    "\n",
    "OUT_METRICS.parent.mkdir(parents=True, exist_ok=True)\n",
    "OUT_METRICS.write_text(json.dumps(metrics, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "print('Saved metrics to', OUT_METRICS)\n",
    "print(json.dumps(metrics, ensure_ascii=False, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}