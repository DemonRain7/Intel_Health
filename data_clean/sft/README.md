# IntelHealth Agent SFT æ•°æ®ç”Ÿæˆä¸è®­ç»ƒæŒ‡å—

## 1. å¿«é€Ÿå¼€å§‹

### 1.1 å®‰è£…ä¾èµ–
```bash
pip install openai tqdm loguru
```

### 1.2 ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼ˆ100æ¡ï¼ŒæŸ¥çœ‹æˆæœ¬ï¼‰
```bash
# è®¾ç½® API Key
export OPENAI_API_KEY="your-api-key"

# ç”Ÿæˆ 100 æ¡ preprocess æ•°æ®ï¼Œæµ‹è¯•æˆæœ¬
python generate_sft_data.py --agent preprocess --num_samples 100 --test_cost
```

### 1.2.1 ä»…ç”Ÿæˆæœ¬åœ° seed æ•°æ®ï¼ˆä¸è°ƒç”¨ APIï¼‰
```bash
# ç”Ÿæˆ 100 æ¡ seed æ•°æ®ï¼ˆä¸æ¶ˆè€— APIï¼‰
python generate_sft_data.py --agent preprocess --num_samples 100 --seed_mode
```

### 1.3 ç”Ÿæˆå®Œæ•´æ•°æ®é›†
```bash
# ä¸º preprocess Agent ç”Ÿæˆ 5000 æ¡æ•°æ®
python generate_sft_data.py --agent preprocess --num_samples 5000

# ä¸ºæ‰€æœ‰ Agent ç”Ÿæˆæ•°æ®
python generate_sft_data.py --agent all --num_samples 2000
```

### 1.4 åŒæ—¶ç”Ÿæˆ seed + åˆæˆæ•°æ®
```bash
python generate_sft_data.py --agent all --num_samples 2000 --seed_mode --synthetic_mode
```

è¾“å‡ºæ–‡ä»¶ï¼š
- åˆæˆæ•°æ®ï¼š`{agent}_sft_data.jsonl`
- Seed æ•°æ®ï¼š`{agent}_seed.jsonl`

---

## 2. SFT vs å¢é‡é¢„è®­ç»ƒ vs LoRA è¯¦è§£

### 2.1 ä¸‰ç§è®­ç»ƒæ–¹å¼å¯¹æ¯”

| æ–¹å¼ | å…¨ç§° | è®­ç»ƒå†…å®¹ | æ•°æ®æ ¼å¼ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|---------|---------|
| **SFT** | Supervised Fine-Tuning | å­¦ä¹ "å¦‚ä½•å›ç­”" | å¯¹è¯æ ¼å¼ (instruction-response) | ç‰¹å®šä»»åŠ¡æŒ‡ä»¤éµå¾ª |
| **å¢é‡é¢„è®­ç»ƒ** | Continual Pre-training | å­¦ä¹ "é¢†åŸŸçŸ¥è¯†" | çº¯æ–‡æœ¬ (æ— æ ¼å¼è¦æ±‚) | é¢†åŸŸçŸ¥è¯†æ³¨å…¥ |
| **LoRA** | Low-Rank Adaptation | å‚æ•°é«˜æ•ˆå¾®è°ƒ | åŒä¸Šä¸¤ç§ | èµ„æºæœ‰é™æ—¶çš„ä»»ä½•è®­ç»ƒ |

### 2.2 ä»€ä¹ˆæ—¶å€™ç”¨ä»€ä¹ˆï¼Ÿ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è®­ç»ƒç­–ç•¥å†³ç­–æ ‘                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä½ çš„ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ
    â”‚
    â”œâ”€â–º è®©æ¨¡å‹å­¦ä¼š"åšæŸä¸ªä»»åŠ¡"ï¼ˆå¦‚ï¼šç”ŸæˆJSONæ ¼å¼è¯Šæ–­ï¼‰
    â”‚       â””â”€â–º ç”¨ SFTï¼ˆæœ‰ç›‘ç£å¾®è°ƒï¼‰
    â”‚
    â”œâ”€â–º è®©æ¨¡å‹"ç†è§£æŸä¸ªé¢†åŸŸ"ï¼ˆå¦‚ï¼šåŒ»å­¦çŸ¥è¯†ï¼‰
    â”‚       â””â”€â–º ç”¨ å¢é‡é¢„è®­ç»ƒï¼ˆContinual Pre-trainingï¼‰
    â”‚
    â””â”€â–º ä¸¤è€…éƒ½éœ€è¦
            â””â”€â–º å…ˆå¢é‡é¢„è®­ç»ƒï¼Œå† SFT

ä½ çš„èµ„æºæƒ…å†µï¼Ÿ
    â”‚
    â”œâ”€â–º GPU æ˜¾å­˜ < 16GB
    â”‚       â””â”€â–º å¿…é¡»ç”¨ LoRA
    â”‚
    â”œâ”€â–º GPU æ˜¾å­˜ 16-48GB
    â”‚       â””â”€â–º å°æ¨¡å‹(0.5B-3B) å¯å…¨é‡ï¼Œå¤§æ¨¡å‹ç”¨ LoRA
    â”‚
    â””â”€â–º GPU æ˜¾å­˜ > 48GB (A100ç­‰)
            â””â”€â–º å¯ä»¥è€ƒè™‘å…¨é‡å¾®è°ƒ
```

### 2.3 å¯¹äº IntelHealth å„ Agent çš„å»ºè®®

| Agent | æ¨èè®­ç»ƒæ–¹å¼ | ç†ç”± |
|-------|-------------|------|
| `preprocess` | **SFT + LoRA** | ä»»åŠ¡æ˜ç¡®ï¼ˆç»“æ„åŒ–è½¬æ¢ï¼‰ï¼Œéœ€è¦å­¦ä¼šç‰¹å®šè¾“å‡ºæ ¼å¼ |
| `critic` | **SFT + LoRA** | è¯„åˆ†ä»»åŠ¡ï¼Œéœ€è¦å­¦ä¼šæ‰“åˆ†é€»è¾‘ |
| `check_rag` | **SFT + LoRA** | ç›¸å…³æ€§åˆ¤æ–­ï¼Œä»»åŠ¡å¯¼å‘ |
| `diagnosis` | **å¢é‡é¢„è®­ç»ƒ + SFT + LoRA** | æ ¸å¿ƒä»»åŠ¡ï¼Œéœ€è¦åŒ»å­¦çŸ¥è¯† + è¯Šæ–­èƒ½åŠ› |
| `drug` | **å¢é‡é¢„è®­ç»ƒ + SFT + LoRA** | éœ€è¦è¯ç‰©çŸ¥è¯† + æ¨èèƒ½åŠ› |

---

## 3. å…·ä½“è®­ç»ƒæµç¨‹

### 3.1 ç®€å•ä»»åŠ¡ï¼ˆpreprocess, critic, check_ragï¼‰

**åªéœ€è¦ SFTï¼š**

```bash
# 1. ç”Ÿæˆæ•°æ®
python generate_sft_data.py --agent preprocess --num_samples 5000

# 2. è®­ç»ƒï¼ˆåœ¨ MyMedicalGPT ç›®å½•ä¸‹ï¼‰
cd ../../MyMedicalGPT

python supervised_finetuning.py \
    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
    --train_file_dir ../IntelHealth/data_clean/sft/ \
    --output_dir outputs/loras/preprocess-agent \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --num_train_epochs 3 \
    --learning_rate 2e-4 \
    --use_lora True \
    --lora_rank 64 \
    --lora_alpha 128
```

### 3.2 å¤æ‚ä»»åŠ¡ï¼ˆdiagnosis, drugï¼‰

**éœ€è¦å…ˆå¢é‡é¢„è®­ç»ƒï¼Œå† SFTï¼š**

```bash
# æ­¥éª¤ 1: å¢é‡é¢„è®­ç»ƒï¼ˆå­¦ä¹ åŒ»å­¦çŸ¥è¯†ï¼‰
python pretraining.py \
    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
    --train_file_dir data/medical_corpus/ \
    --output_dir outputs/pretrained/medical-qwen \
    --per_device_train_batch_size 4 \
    --num_train_epochs 1 \
    --use_lora True

# æ­¥éª¤ 2: åˆå¹¶ LoRA
python merge_peft_adapter.py \
    --base_model Qwen/Qwen2.5-0.5B-Instruct \
    --lora_model outputs/pretrained/medical-qwen \
    --output_dir outputs/merged/medical-qwen-pretrained

# æ­¥éª¤ 3: SFTï¼ˆå­¦ä¹ è¯Šæ–­ä»»åŠ¡ï¼‰
python supervised_finetuning.py \
    --model_name_or_path outputs/merged/medical-qwen-pretrained \
    --train_file_dir ../IntelHealth/data_clean/sft/ \
    --output_dir outputs/loras/diagnosis-agent \
    --use_lora True
```

---

## 4. LoRA è¯¦è§£

### 4.1 ä»€ä¹ˆæ˜¯ LoRAï¼Ÿ

LoRA (Low-Rank Adaptation) æ˜¯ä¸€ç§**å‚æ•°é«˜æ•ˆå¾®è°ƒ**æ–¹æ³•ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åŸå§‹æ¨¡å‹æƒé‡ W                         â”‚
â”‚                    (å†»ç»“ï¼Œä¸æ›´æ–°)                         â”‚
â”‚                         â”‚                                â”‚
â”‚                         â–¼                                â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚              â”‚    W + Î”W           â”‚                     â”‚
â”‚              â”‚                     â”‚                     â”‚
â”‚              â”‚  Î”W = A Ã— B         â”‚  â—„â”€â”€ åªè®­ç»ƒ A å’Œ B   â”‚
â”‚              â”‚  (ä½ç§©åˆ†è§£)          â”‚      å‚æ•°é‡å¾ˆå°      â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…¨é‡å¾®è°ƒ: æ›´æ–°æ‰€æœ‰å‚æ•° (æ•°åäº¿)
LoRA:     åªæ›´æ–° Aã€B çŸ©é˜µ (æ•°ç™¾ä¸‡ï¼Œçº¦ 1-5%)
```

### 4.2 LoRA å‚æ•°è¯´æ˜

```python
# åœ¨ supervised_finetuning.py ä¸­çš„å‚æ•°
--use_lora True              # å¯ç”¨ LoRA
--lora_rank 64               # ç§© (è¶Šå¤§èƒ½åŠ›è¶Šå¼ºï¼Œæ˜¾å­˜è¶Šå¤š)
--lora_alpha 128             # ç¼©æ”¾å› å­ (é€šå¸¸æ˜¯ rank çš„ 2 å€)
--lora_dropout 0.05          # Dropout
--lora_target_modules q_proj,k_proj,v_proj,o_proj  # è¦é€‚é…çš„å±‚
```

### 4.3 LoRA æ˜¾å­˜å¯¹æ¯”

| æ¨¡å‹å¤§å° | å…¨é‡å¾®è°ƒ | LoRA (rank=64) |
|---------|---------|----------------|
| 0.5B | ~8GB | ~4GB |
| 1.8B | ~16GB | ~8GB |
| 7B | ~60GB | ~16GB |
| 14B | ~120GB | ~24GB |

---

## 5. å¢é‡é¢„è®­ç»ƒ vs SFT æ•°æ®æ ¼å¼

### 5.1 å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼

**çº¯æ–‡æœ¬ï¼Œæ— ç‰¹å®šæ ¼å¼ï¼š**

```text
æ€¥æ€§èƒƒç‚æ˜¯æŒ‡ç”±å¤šç§åŸå› å¼•èµ·çš„æ€¥æ€§èƒƒé»è†œç‚ç—‡ã€‚ä¸´åºŠä¸Šå¸¸è§çš„ç—…å› åŒ…æ‹¬è¯ç‰©ã€åº”æ¿€ã€
ä¹™é†‡ã€ç¼ºè¡€ã€èƒ†æ±åæµç­‰ã€‚ä¸»è¦è¡¨ç°ä¸ºä¸Šè…¹éƒ¨ä¸é€‚ã€ç–¼ç—›ã€æ¶å¿ƒã€å‘•åç­‰ç—‡çŠ¶...
```

### 5.2 SFT è¾“å‡ºæ ¼å¼è¯´æ˜

é»˜è®¤è¾“å‡ºä¸º `instruction` æ ¼å¼ï¼ˆæ›´é€‚åˆç»Ÿä¸€æ•°æ®é›†ï¼‰ï¼š
```jsonl
{"instruction":"...","input":{...},"output":{...}}
```

å¦‚éœ€æ—§ç‰ˆå¯¹è¯æ ¼å¼ï¼š
```bash
python generate_sft_data.py --agent preprocess --num_samples 100 --output_format conversation
```

**æ–‡ä»¶æ ¼å¼**: `.txt` æˆ–æ¯è¡Œä¸€ä¸ªæ–‡æ¡£çš„ `.jsonl`

```jsonl
{"text": "æ€¥æ€§èƒƒç‚æ˜¯æŒ‡ç”±å¤šç§åŸå› å¼•èµ·çš„æ€¥æ€§èƒƒé»è†œç‚ç—‡..."}
{"text": "æ…¢æ€§æ”¯æ°”ç®¡ç‚æ˜¯æ°”ç®¡ã€æ”¯æ°”ç®¡é»è†œåŠå‘¨å›´ç»„ç»‡çš„æ…¢æ€§éç‰¹å¼‚æ€§ç‚ç—‡..."}
```

### 5.2 SFT æ•°æ®æ ¼å¼

**å¯¹è¯æ ¼å¼ï¼ˆä½ å·²æœ‰çš„æ ¼å¼ï¼‰ï¼š**

```jsonl
{"conversations": [{"from": "human", "value": "é—®é¢˜"}, {"from": "gpt", "value": "å›ç­”"}]}
```

---

## 6. å„ Agent æ•°æ®é‡å»ºè®®

| Agent | æœ€å°é‡ | æ¨èé‡ | è¯´æ˜ |
|-------|-------|-------|------|
| preprocess | 1,000 | 5,000 | ç»“æ„åŒ–è½¬æ¢ä»»åŠ¡ |
| critic | 500 | 2,000 | è¯„åˆ†ä»»åŠ¡ï¼Œéœ€è¦†ç›–å„åˆ†æ•°æ®µ |
| check_rag | 500 | 2,000 | ç›¸å…³æ€§åˆ¤æ–­ |
| diagnosis | 3,000 | 10,000+ | æ ¸å¿ƒä»»åŠ¡ï¼Œéœ€è¦æ›´å¤šæ•°æ® |
| drug | 2,000 | 5,000 | ç”¨è¯å»ºè®® |

---

## 7. è®­ç»ƒåæµ‹è¯•

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥ç”¨ä»¥ä¸‹æ–¹å¼æµ‹è¯•ï¼š

```python
# æµ‹è¯• preprocess agent
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("outputs/merged/preprocess-agent")
tokenizer = AutoTokenizer.from_pretrained("outputs/merged/preprocess-agent")

prompt = """è¯·å°†ä»¥ä¸‹ç—‡çŠ¶ä¿¡æ¯è½¬æ¢ä¸ºç»“æ„åŒ–çš„åŒ»å­¦æè¿°ï¼š
èº«ä½“éƒ¨ä½: å¤´éƒ¨
ä¸»è¦ç—‡çŠ¶: å¤´ç—›, å¤´æ™•
å…¶ä»–ç—‡çŠ¶: æ¶å¿ƒæƒ³å
ä¸¥é‡ç¨‹åº¦: 3
æŒç»­æ—¶é—´: 1To3Days

è¯·è¾“å‡º JSON æ ¼å¼ï¼š"""

inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=200)
print(tokenizer.decode(outputs[0]))
```

---

## 8. å¸¸è§é—®é¢˜

### Q: å¢é‡é¢„è®­ç»ƒä¹Ÿç”¨ LoRA å—ï¼Ÿ
**A:** å¯ä»¥ã€‚å¢é‡é¢„è®­ç»ƒçš„ç›®çš„æ˜¯æ³¨å…¥çŸ¥è¯†ï¼ŒLoRA åŒæ ·é€‚ç”¨ã€‚å¦‚æœèµ„æºæœ‰é™ï¼Œå»ºè®®ç”¨ LoRAã€‚

### Q: ä¸ºä»€ä¹ˆ diagnosis éœ€è¦å¢é‡é¢„è®­ç»ƒï¼Ÿ
**A:** å› ä¸ºè¯Šæ–­éœ€è¦åŒ»å­¦èƒŒæ™¯çŸ¥è¯†ã€‚åŸºç¡€æ¨¡å‹ï¼ˆå¦‚ Qwenï¼‰çš„åŒ»å­¦çŸ¥è¯†æœ‰é™ï¼Œå¢é‡é¢„è®­ç»ƒå¯ä»¥è¡¥å……è¿™éƒ¨åˆ†çŸ¥è¯†ã€‚

### Q: å¤šä¸ª Agent å¯ä»¥å…±äº«åŒä¸€ä¸ªåŸºç¡€æ¨¡å‹å—ï¼Ÿ
**A:** å¯ä»¥ã€‚å»ºè®®å…ˆç”¨åŒ»å­¦è¯­æ–™å¢é‡é¢„è®­ç»ƒä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œç„¶åä¸ºæ¯ä¸ª Agent å•ç‹¬åš SFTã€‚

### Q: ç”Ÿæˆçš„æ•°æ®è´¨é‡ä¸é«˜æ€ä¹ˆåŠï¼Ÿ
**A:**
1. å¢åŠ  few-shot ç¤ºä¾‹
2. äººå·¥å®¡æ ¸å’Œæ¸…æ´—
3. ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹ç”Ÿæˆï¼ˆå¦‚ GPT-4ï¼‰

## ĞÂÔö: build_agent_dataset.py
- ¹æÔòÓÅÏÈÉú³É seed Êı¾İ£¬Ö§³Ö output_format=conversations
- Ê¾Àı:
  python build_agent_dataset.py --agent preprocess --num_samples 500 --output_format conversations
