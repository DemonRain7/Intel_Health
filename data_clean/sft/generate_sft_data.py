# -*- coding: utf-8 -*-
"""
SFT æ•°æ®ç”Ÿæˆè„šæœ¬
ä½¿ç”¨ GPT-4.1 ä¸º IntelHealth å„ Agent ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆå¯é€šè¿‡ --model æŒ‡å®šï¼‰

Usage:
    # ç”Ÿæˆ 100 æ¡æµ‹è¯•æ•°æ®ï¼ˆæŸ¥çœ‹æˆæœ¬ï¼‰
    python generate_sft_data.py --agent symptom_quality_grader --num_samples 100 --test_cost

    # æ­£å¼ç”Ÿæˆ 5000 æ¡æ•°æ®
    python generate_sft_data.py --agent rag_relevance_grader --num_samples 5000

    # ç”Ÿæˆæ‰€æœ‰å¯ç”¨ Agent çš„æ•°æ®ï¼ˆä¸å« diagnosis/drugï¼‰
    python generate_sft_data.py --agent all --num_samples 2000

Available agents:
    - symptom_normalizer: ç—‡çŠ¶ç»“æ„åŒ– (preprocessAndGuess)
    - symptom_quality_grader: è´¨é‡è¯„åˆ† (Critic_preprocess)
    - rag_relevance_grader: RAG ç›¸å…³åº¦è¯„åˆ† (checkRAG)
    - drug_evidence_grader: è¯Šæ–­/ç”¨è¯è¯æ®è¯„åˆ† (drugRAGandScore)

Notes:
    - diagnosis_generator èµ° DAPT/QA æ•°æ®ï¼ˆtrain_sft.jsonlï¼‰ï¼Œä¸å†ç”¨ LLM åˆæˆç»“æ„åŒ– SFTã€‚
    - drug_recommender ä¸åš SFTï¼Œä»…èµ° RAG + Schema çº¦æŸã€‚
"""

import os
import json
import argparse
import random
import time
from typing import List, Dict, Any
from openai import OpenAI
from tqdm import tqdm
from loguru import logger

try:
    from jsonschema import validate, ValidationError
except Exception:
    validate = None
    class ValidationError(Exception):
        pass


# ==================== é…ç½® ====================

# èº«ä½“éƒ¨ä½åˆ—è¡¨
BODY_PARTS = [
    "å¤´éƒ¨", "çœ¼ç›", "è€³æœµ", "é¼»å­", "å£è…”", "å’½å–‰", "é¢ˆéƒ¨",
    "èƒ¸éƒ¨", "å¿ƒè„", "è‚ºéƒ¨", "è…¹éƒ¨", "èƒƒéƒ¨", "è‚è„", "è‚¾è„",
    "èƒŒéƒ¨", "è…°éƒ¨", "å››è‚¢", "æ‰‹è‡‚", "è…¿éƒ¨", "å…³èŠ‚", "çš®è‚¤",
    "å…¨èº«", "ç¥ç»ç³»ç»Ÿ", "æ³Œå°¿ç³»ç»Ÿ", "ç”Ÿæ®–ç³»ç»Ÿ"
]

# ç—‡çŠ¶ä¸¥é‡ç¨‹åº¦
SEVERITY_LEVELS = [1, 2, 3, 4, 5]

# æŒç»­æ—¶é—´é€‰é¡¹
DURATION_OPTIONS = [
    "lessThan24Hours", "1To3Days", "4To7Days", "1To2Weeks", "moreThan2Weeks"
]

DURATION_CHINESE = {
    "lessThan24Hours": "24å°æ—¶å†…",
    "1To3Days": "1è‡³3å¤©",
    "4To7Days": "4è‡³7å¤©",
    "1To2Weeks": "1è‡³2å‘¨",
    "moreThan2Weeks": "è¶…è¿‡2å‘¨"
}

# ç—‡çŠ¶ä¸ç–¾ç—…ç§å­æ•°æ®ï¼ˆç”¨äºæœ¬åœ°ç”Ÿæˆ seed æ ·æœ¬ï¼‰
SYMPTOMS_BY_BODY_PART = {
    "å¤´éƒ¨": ["å¤´ç—›", "å¤´æ™•", "è§†ç‰©æ¨¡ç³Š", "æ¶å¿ƒ"],
    "å’½å–‰": ["å’½ç—›", "å¹²å’³", "å–‰å’™ç—’", "å£°éŸ³å˜¶å“‘"],
    "èƒ¸éƒ¨": ["èƒ¸é—·", "èƒ¸ç—›", "æ°”çŸ­", "å¿ƒæ‚¸"],
    "è…¹éƒ¨": ["è‚šå­ç—›", "è…¹èƒ€", "æ¶å¿ƒ", "æ‹‰è‚šå­"],
    "èƒƒéƒ¨": ["èƒƒç—›", "åé…¸", "å—³æ°”", "æ¶å¿ƒ"],
    "çš®è‚¤": ["ç˜™ç—’", "çš®ç–¹", "çº¢è‚¿", "è„±å±‘"],
    "å…³èŠ‚": ["å…³èŠ‚ç—›", "è‚¿èƒ€", "æ™¨åƒµ", "æ´»åŠ¨å—é™"],
}

OTHER_SYMPTOMS_POOL = [
    "å¤œé—´åŠ é‡", "è¿›é£Ÿåæ˜æ˜¾", "ä¼´æœ‰ä½çƒ­", "å¶æœ‰ä¹åŠ›", "ç¡çœ å—å½±å“", "é£Ÿæ¬²ä¸‹é™"
]

SYMPTOM_MEDICAL_MAP = {
    "è‚šå­ç—›": "è…¹ç—›",
    "æ‹‰è‚šå­": "è…¹æ³»",
    "èƒƒç—›": "èƒƒéƒ¨ç–¼ç—›",
    "å–‰å’™ç—’": "å’½å–‰ç˜™ç—’",
    "å—³æ°”": "å—³æ°”",
    "åé…¸": "èƒƒé…¸åæµ",
    "å¹²å’³": "å¹²å’³",
    "èƒ¸é—·": "èƒ¸é—·",
    "æ°”çŸ­": "æ°”ä¿ƒ",
    "å¿ƒæ‚¸": "å¿ƒæ‚¸",
    "çš®ç–¹": "çš®ç–¹",
    "çº¢è‚¿": "çº¢è‚¿",
}

CONDITION_BY_BODY_PART = {
    "å¤´éƒ¨": ["åå¤´ç—›", "ç´§å¼ æ€§å¤´ç—›", "é¼»çª¦ç‚"],
    "å’½å–‰": ["ä¸Šå‘¼å¸é“æ„ŸæŸ“", "å’½ç‚", "è¿‡æ•æ€§å’½å–‰ç‚"],
    "èƒ¸éƒ¨": ["ä¸Šå‘¼å¸é“æ„ŸæŸ“", "æ€¥æ€§æ”¯æ°”ç®¡ç‚", "è‚ºéƒ¨æ„ŸæŸ“"],
    "è…¹éƒ¨": ["æ€¥æ€§èƒƒè‚ ç‚", "åŠŸèƒ½æ€§æ¶ˆåŒ–ä¸è‰¯", "æ€¥æ€§èƒƒç‚"],
    "èƒƒéƒ¨": ["æ€¥æ€§èƒƒç‚", "æ¶ˆåŒ–æ€§æºƒç–¡", "åæµæ€§é£Ÿç®¡ç‚"],
    "çš®è‚¤": ["è¿‡æ•æ€§çš®ç‚", "æ¹¿ç–¹", "æ¥è§¦æ€§çš®ç‚"],
    "å…³èŠ‚": ["éª¨å…³èŠ‚ç‚", "å…³èŠ‚ç‚", "è‚Œè‚‰åŠ³æŸ"],
}

DRUG_RECOMMENDATIONS = {
    "æ€¥æ€§èƒƒè‚ ç‚": [
        {"name": "è’™è„±çŸ³æ•£", "usage": "æ¯æ¬¡1è¢‹ï¼Œæ¯æ—¥3æ¬¡", "notes": "æ³¨æ„è¡¥æ¶²ï¼Œå­•å¦‡æ…ç”¨"},
        {"name": "å£æœè¡¥æ¶²ç›", "usage": "æŒ‰è¯´æ˜å†²æœ", "notes": "é˜²æ­¢è„±æ°´"},
    ],
    "æ€¥æ€§èƒƒç‚": [
        {"name": "å¥¥ç¾æ‹‰å”‘", "usage": "æ¯æ—¥1æ¬¡ï¼Œé¤å‰æœç”¨", "notes": "éµåŒ»å˜±ä½¿ç”¨"},
        {"name": "é“ç¢³é…¸é•", "usage": "æ¯æ¬¡1-2ç‰‡ï¼Œæ¯æ—¥3æ¬¡", "notes": "é¿å…ä¸å…¶ä»–è¯åŒæœ"},
    ],
    "ä¸Šå‘¼å¸é“æ„ŸæŸ“": [
        {"name": "å¯¹ä¹™é…°æ°¨åŸºé…š", "usage": "æŒ‰è¯´æ˜æœç”¨", "notes": "è‚åŠŸèƒ½ä¸å…¨æ…ç”¨"},
        {"name": "å³ç¾æ²™èŠ¬", "usage": "æ¯æ¬¡10-20mgï¼Œæ¯æ—¥3æ¬¡", "notes": "é¿å…ä¸å«é…’ç²¾è¯ç‰©åŒç”¨"},
    ],
    "è¿‡æ•æ€§çš®ç‚": [
        {"name": "æ°¯é›·ä»–å®š", "usage": "æ¯æ—¥1æ¬¡", "notes": "å—œç¡è€…æ…ç”¨"},
        {"name": "ç‚‰ç”˜çŸ³æ´—å‰‚", "usage": "å¤–ç”¨æ¯æ—¥2-3æ¬¡", "notes": "é¿å…ç ´æŸçš®è‚¤"},
    ],
}

# ç»Ÿä¸€ instruction æ¨¡æ¿


# JSON Schemas for validation (lightweight)
SCHEMAS = {
    "preprocess": {
        "type": "object",
        "required": ["input", "output"],
        "properties": {
            "input": {"type": "object"},
            "output": {
                "type": "object",
                "required": ["optimized_symptoms", "rag_keywords"],
                "properties": {
                    "optimized_symptoms": {"type": "string"},
                    "rag_keywords": {"type": "array", "items": {"type": "string"}}
                }
            }
        }
    },
    "critic": {
        "type": "object",
        "required": ["input", "output"],
        "properties": {
            "output": {
                "type": "object",
                "required": ["score", "comment", "isValid"],
                "properties": {
                    "score": {"type": "number"},
                    "comment": {"type": "string"},
                    "isValid": {"type": "boolean"}
                }
            }
        }
    },
    "check_rag": {
        "type": "object",
        "required": ["input", "output"],
        "properties": {
            "output": {
                "type": "object",
                "required": ["ragScore", "ragComment"],
                "properties": {
                    "ragScore": {"type": "number"},
                    "ragComment": {"type": "string"}
                }
            }
        }
    },
    "drug_evidence": {
        "type": "object",
        "required": ["input", "output"],
        "properties": {
            "output": {
                "type": "object",
                "required": ["diagnosisScore", "diagnosisComment"],
                "properties": {
                    "diagnosisScore": {"type": "number"},
                    "diagnosisComment": {"type": "string"}
                }
            }
        }
    },
    "diagnosis": {
        "type": "object",
        "required": ["input", "output"],
        "properties": {
            "output": {
                "type": "object",
                "required": ["results", "recommendations", "recomm_short"],
                "properties": {
                    "results": {"type": "array"},
                    "recommendations": {"type": "array"},
                    "recomm_short": {"type": "array"}
                }
            }
        }
    },
    "drug": {
        "type": "object",
        "required": ["input", "output"],
        "properties": {
            "output": {"type": "object", "required": ["drugs"]}
        }
    }
}

INSTRUCTIONS = {
    "preprocess": "è¯·å°†ä»¥ä¸‹ç”¨æˆ·ç—‡çŠ¶ä¿¡æ¯è½¬æ¢ä¸ºç»“æ„åŒ–çš„åŒ»å­¦æè¿°ï¼Œå¹¶æå–RAGæ£€ç´¢å…³é”®è¯ã€‚",
    "critic": "è¯·å¯¹ä»¥ä¸‹ç—‡çŠ¶ç»“æ„åŒ–ç»“æœè¿›è¡Œè´¨é‡è¯„åˆ†ã€‚",
    "check_rag": "è¯·è¯„ä¼°RAGè¿”å›çš„åŒ»å­¦çŸ¥è¯†ä¸ç”¨æˆ·ç—‡çŠ¶çš„åŒ¹é…åº¦ã€‚",
    "drug_evidence": "è¯·å¯¹è¯Šæ–­ç»“æœä¸è¯ç‰©è¯æ®è¿›è¡Œåˆç†æ€§è¯„åˆ†ã€‚",
    "diagnosis": "æ ¹æ®æ‚£è€…ç—‡çŠ¶å’ŒåŒ»å­¦èƒŒæ™¯çŸ¥è¯†ï¼Œç”Ÿæˆè¯Šæ–­ç»“æœã€‚",
    "drug": "æ ¹æ®è¯Šæ–­ç»“æœå’Œè¯ç‰©çŸ¥è¯†åº“ï¼Œç”Ÿæˆç”¨è¯å»ºè®®ã€‚",
}

# å¤–éƒ¨ Agent åç§° -> å†…éƒ¨ç”Ÿæˆå™¨åç§°
AGENT_ALIASES = {
    # æ–°å‘½å
    "symptom_normalizer": "preprocess",
    "symptom_quality_grader": "critic",
    "rag_relevance_grader": "check_rag",
    "drug_evidence_grader": "drug_evidence",
    "diagnosis_generator": "diagnosis",
    "drug_recommender": "drug",
    # å…¼å®¹æ—§å‘½å
    "preprocess": "preprocess",
    "critic": "critic",
    "check_rag": "check_rag",
    "drug_evidence": "drug_evidence",
    "diagnosis": "diagnosis",
    "drug": "drug",
}


def validate_item(agent: str, item: Dict[str, Any]) -> bool:
    """Validate a generated item with a lightweight JSON schema."""
    schema = SCHEMAS.get(agent)
    if not schema:
        return True
    if validate is None:
        return True
    try:
        validate(instance=item, schema=schema)
        return True
    except ValidationError as exc:
        logger.warning(f"Schema validation failed for agent {agent}: {exc}")
        return False

# ==================== Prompt æ¨¡æ¿ ====================

PROMPTS = {
    # Agent 1: ç—‡çŠ¶ç»“æ„åŒ– (preprocessAndGuess)
    "preprocess": {
        "system": """ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ•°æ®ç”ŸæˆåŠ©æ‰‹ã€‚ä½ éœ€è¦ç”Ÿæˆç”¨äºè®­ç»ƒ"ç—‡çŠ¶ç»“æ„åŒ–"æ¨¡å‹çš„æ•°æ®ã€‚

ä»»åŠ¡è¯´æ˜ï¼š
- è¾“å…¥ï¼šç”¨æˆ·çš„åŸå§‹ç—‡çŠ¶æè¿°ï¼ˆåŒ…æ‹¬èº«ä½“éƒ¨ä½ã€ä¸»è¦ç—‡çŠ¶ã€å…¶ä»–ç—‡çŠ¶ã€ä¸¥é‡ç¨‹åº¦ã€æŒç»­æ—¶é—´ï¼‰
- è¾“å‡ºï¼šç»“æ„åŒ–çš„åŒ»å­¦æè¿° (optimized_symptoms) å’Œ RAG æ£€ç´¢å…³é”®è¯ (rag_keywords)

ç”Ÿæˆè¦æ±‚ï¼š
1. optimized_symptoms åº”è¯¥æ˜¯ä¸“ä¸šã€å®Œæ•´çš„åŒ»å­¦ç—‡çŠ¶æè¿°ï¼ŒåŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯
2. rag_keywords å¿…é¡»æ›´å…·ä½“ï¼šåŒ…å«èº«ä½“éƒ¨ä½ + ç—‡çŠ¶ + å…³é”®ä¿®é¥°ï¼ˆä¾‹å¦‚â€œè…°éƒ¨å¤–ä¼¤ç–¼ç—›â€â€œè…°æ¤é—´ç›˜çªå‡ºæ”¾å°„ç—›â€ï¼‰
3. å°†å£è¯­åŒ–æè¿°è½¬æ¢ä¸ºä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚"è‚šå­ç—›"â†’"è…¹ç—›"ï¼‰
4. ä¸¥é‡ç¨‹åº¦ç”¨æ–‡å­—æè¿°ï¼ˆ1=è½»å¾®, 2=è¾ƒè½», 3=ä¸­ç­‰, 4=è¾ƒé‡, 5=ä¸¥é‡ï¼‰

è¯·ç”Ÿæˆå¤šæ ·åŒ–ã€çœŸå®çš„åŒ»ç–—åœºæ™¯æ•°æ®ã€‚""",

        "user_template": """è¯·ç”Ÿæˆ {batch_size} æ¡"ç—‡çŠ¶ç»“æ„åŒ–"è®­ç»ƒæ•°æ®ã€‚

æ¯æ¡æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
{{
  "input": {{
    "body_part": "èº«ä½“éƒ¨ä½",
    "symptoms": ["ç—‡çŠ¶1", "ç—‡çŠ¶2"],
    "other_symptoms": "å…¶ä»–ç—‡çŠ¶æè¿°",
    "severity": 1-5,
    "duration": "æŒç»­æ—¶é—´ä»£ç "
  }},
  "output": {{
    "optimized_symptoms": "ä¸“ä¸šçš„ç—‡çŠ¶æè¿°æ–‡æœ¬",
    "rag_keywords": ["å…³é”®è¯1", "å…³é”®è¯2", ...]
  }}
}}

è¦æ±‚ï¼š
1. ç”Ÿæˆå¤šæ ·åŒ–çš„ç—‡çŠ¶ç»„åˆï¼Œè¦†ç›–ä¸åŒèº«ä½“éƒ¨ä½
2. è¾“å…¥ä½¿ç”¨å£è¯­åŒ–æè¿°ï¼Œè¾“å‡ºä½¿ç”¨ä¸“ä¸šåŒ»å­¦æœ¯è¯­
3. rag_keywords å¿…é¡»ç»†åŒ–åˆ°ç—‡çŠ¶å±‚é¢ï¼Œä¸å…è®¸åªæœ‰â€œèº«ä½“éƒ¨ä½â€è¿™ç§æ³›åŒ–è¯
4. ç¡®ä¿ output æ˜¯åˆç†çš„åŒ»å­¦è½¬æ¢ç»“æœ
4. ç›´æ¥è¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŠ ä»»ä½•è§£é‡Šæ–‡å­—

è¯·è¿”å›ä¸€ä¸ªåŒ…å« {batch_size} æ¡æ•°æ®çš„ JSON æ•°ç»„ï¼š"""
    },

    # Agent 2: è´¨é‡è¯„åˆ† (Critic_preprocess)
    "critic": {
        "system": """ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ•°æ®ç”ŸæˆåŠ©æ‰‹ã€‚ä½ éœ€è¦ç”Ÿæˆç”¨äºè®­ç»ƒ"è¯Šæ–­è´¨é‡è¯„åˆ†"æ¨¡å‹çš„æ•°æ®ã€‚

ä»»åŠ¡è¯´æ˜ï¼š
- è¾“å…¥ï¼šç»“æ„åŒ–çš„ç—‡çŠ¶æè¿° (optimized_symptoms) å’Œå…³é”®è¯ (rag_keywords)
- è¾“å‡ºï¼šè´¨é‡è¯„åˆ† (score: 0-5) å’Œè¯„ä»·æ„è§ (comment)

è¯„åˆ†æ ‡å‡†ï¼ˆæ¯é¡¹1åˆ†ï¼Œæ»¡åˆ†5åˆ†ï¼‰ï¼š
1. optimized_symptoms æ ¼å¼è§„èŒƒï¼Œæ˜¯å®Œæ•´é€šé¡ºçš„åŒ»å­¦æè¿°
2. rag_keywords åŒ…å«èº«ä½“éƒ¨ä½å…³é”®è¯
3. rag_keywords åŒ…å«ä¸»è¦ç—‡çŠ¶å…³é”®è¯
4. æœ¯è¯­ä½¿ç”¨ä¸“ä¸šå‡†ç¡®
5. ä¿¡æ¯å®Œæ•´ï¼Œæ— é—æ¼é‡è¦å†…å®¹

è¯·ç”Ÿæˆå„ç§è´¨é‡ç­‰çº§ï¼ˆå¥½/ä¸­/å·®ï¼‰çš„æ ·æœ¬ã€‚""",

        "user_template": """è¯·ç”Ÿæˆ {batch_size} æ¡"è´¨é‡è¯„åˆ†"è®­ç»ƒæ•°æ®ã€‚

æ¯æ¡æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
{{
  "input": {{
    "optimized_symptoms": "ç—‡çŠ¶æè¿°",
    "rag_keywords": ["å…³é”®è¯1", "å…³é”®è¯2"]
  }},
  "output": {{
    "score": 0-5,
    "comment": "è¯„ä»·æ„è§",
    "isValid": true/false
  }}
}}

è¦æ±‚ï¼š
1. ç”Ÿæˆä¸åŒè´¨é‡ç­‰çº§çš„æ ·æœ¬ï¼ˆå·®:0-1åˆ†, ä¸­:2-3åˆ†, å¥½:4-5åˆ†ï¼‰
2. comment è¦å…·ä½“è¯´æ˜æ‰£åˆ†åŸå› æˆ–ä¼˜ç‚¹
3. score < 3 æ—¶ isValid ä¸º false
4. ç›´æ¥è¿”å› JSON æ•°ç»„

è¯·è¿”å›ä¸€ä¸ªåŒ…å« {batch_size} æ¡æ•°æ®çš„ JSON æ•°ç»„ï¼š"""
    },

    # Agent 3: RAG è¯„å®¡ (checkRAG)
    "check_rag": {
        "system": """ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ•°æ®ç”ŸæˆåŠ©æ‰‹ã€‚ä½ éœ€è¦ç”Ÿæˆç”¨äºè®­ç»ƒ"RAGå†…å®¹è¯„å®¡"æ¨¡å‹çš„æ•°æ®ã€‚

ä»»åŠ¡è¯´æ˜ï¼š
- è¾“å…¥ï¼šç”¨æˆ·ç—‡çŠ¶ (optimized_symptoms) å’Œ RAG æ£€ç´¢ç»“æœ (rag_docs)
- è¾“å‡ºï¼šç›¸å…³æ€§è¯„åˆ† (ragScore: 0-5) å’Œè¯„ä»·æ„è§ (ragComment)

è¯„åˆ†æ ‡å‡†ï¼ˆ0-5ï¼‰ï¼š
0: å®Œå…¨æ— å…³æˆ–æ˜æ˜¾çŸ›ç›¾
1: åŸºæœ¬æ— å…³ï¼Œä»…æœ‰æ³›åŒ–ä¿¡æ¯
2: å¼±ç›¸å…³ï¼ˆéƒ¨åˆ†å…³é”®è¯é‡åˆä½†ç¼ºå°‘æ ¸å¿ƒç—‡çŠ¶/éƒ¨ä½ï¼‰
3: éƒ¨åˆ†ç›¸å…³ï¼ˆè¦†ç›–éƒ¨åˆ†æ ¸å¿ƒç—‡çŠ¶ï¼Œä½†é—æ¼å…³é”®çº¿ç´¢ï¼‰
4: é«˜åº¦ç›¸å…³ï¼ˆè¦†ç›–æ ¸å¿ƒç—‡çŠ¶å¹¶æä¾›æœ‰ä»·å€¼èƒŒæ™¯ï¼‰
5: æé«˜ç›¸å…³ï¼ˆé«˜åº¦åŒ¹é…ä¸”åŒ…å«å…³é”®è¯Šæ–­æç¤º/é‰´åˆ«è¦ç‚¹ï¼‰

è¯·ç”Ÿæˆä¸åŒç›¸å…³åº¦çš„æ ·æœ¬ï¼Œå¹¶åŒ…å«è´Ÿæ ·æœ¬ã€‚""",

        "user_template": """è¯·ç”Ÿæˆ {batch_size} æ¡"RAGè¯„å®¡"è®­ç»ƒæ•°æ®ã€‚

æ¯æ¡æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
{{
  "input": {{
    "optimized_symptoms": "æ‚£è€…ç—‡çŠ¶æè¿°",
    "rag_docs": [
      {{"doc_id": "doc_001", "score": 0.78, "snippet": "æ£€ç´¢æ‘˜è¦ç‰‡æ®µ1"}},
      {{"doc_id": "doc_002", "score": 0.42, "snippet": "æ£€ç´¢æ‘˜è¦ç‰‡æ®µ2"}}
    ]
  }},
  "output": {{
    "ragScore": 0-5,
    "ragComment": "è¯„ä»·æ„è§"
  }}
}}

è¦æ±‚ï¼š
1. ç”Ÿæˆä¸åŒç›¸å…³åº¦çš„æ ·æœ¬ï¼ˆä½:0-2, ä¸­:3, é«˜:4-5ï¼‰
2. rag_docs è¦æ¨¡æ‹ŸçœŸå®çš„åŒ»å­¦çŸ¥è¯†åº“æ£€ç´¢ç»“æœ
3. åŒ…å«è´Ÿæ ·æœ¬ä¸è¾¹ç¼˜æ ·æœ¬ï¼ˆç›¸è¿‘ä½†ä¸åŒ¹é…ï¼‰
4. ç›´æ¥è¿”å› JSON æ•°ç»„

è¯·è¿”å›ä¸€ä¸ªåŒ…å« {batch_size} æ¡æ•°æ®çš„ JSON æ•°ç»„ï¼š"""
    },

    # Agent 4: è¯ç‰©è¯æ®è¯„å®¡ (drugRAGandScore)
    "drug_evidence": {
        "system": """ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ•°æ®ç”ŸæˆåŠ©æ‰‹ã€‚ä½ éœ€è¦ç”Ÿæˆç”¨äºè®­ç»ƒ"ç”¨è¯è¯æ®è¯„å®¡"æ¨¡å‹çš„æ•°æ®ã€‚

ä»»åŠ¡è¯´æ˜ï¼š
- è¾“å…¥ï¼šè¯Šæ–­ç»“æœ resultsï¼ˆå«ç–¾ç—…åä¸æ¦‚ç‡ï¼‰+ è¯ç‰©ç›¸å…³çš„ RAG æ£€ç´¢æ‘˜è¦ rag_docs
- è¾“å‡ºï¼šè¯Šæ–­/ç”¨è¯åˆç†æ€§è¯„åˆ† diagnosisScore (0-5) ä¸ç®€çŸ­æ„è§ diagnosisComment

è¯„åˆ†æ ‡å‡†ï¼ˆ0-5ï¼‰ï¼š
0: æ˜æ˜¾ä¸åˆç†ï¼ˆè¯æ®ä¸è¯Šæ–­æ— å…³æˆ–ç›¸çŸ›ç›¾ï¼‰
1: åŸºæœ¬æ— å…³ï¼Œä»…æœ‰æ³›åŒ–è¯ç‰©çŸ¥è¯†
2: å¼±ç›¸å…³ï¼ˆéƒ¨åˆ†è¯ç‰©/ç¦å¿Œæåˆ°ï¼Œä½†ä¸è¯Šæ–­å¥‘åˆåº¦ä½ï¼‰
3: éƒ¨åˆ†ç›¸å…³ï¼ˆæœ‰ä¸€å®šæ”¯æŒï¼Œä½†ç¼ºä¹å…³é”®è¯æ®ï¼‰
4: é«˜åº¦ç›¸å…³ï¼ˆè¯ç‰©/ç¦å¿Œä¸è¯Šæ–­åŒ¹é…ï¼Œè¯æ®å……åˆ†ï¼‰
5: æé«˜ç›¸å…³ï¼ˆè¯æ®å……åˆ†ä¸”åŒ…å«å…³é”®å®‰å…¨/ç¦å¿Œæç¤ºï¼‰

è¯·ç”Ÿæˆä¸åŒç›¸å…³åº¦çš„æ ·æœ¬ï¼Œå¹¶åŒ…å«è´Ÿæ ·æœ¬ã€‚""",

        "user_template": """è¯·ç”Ÿæˆ {batch_size} æ¡"ç”¨è¯è¯æ®è¯„å®¡"è®­ç»ƒæ•°æ®ã€‚

æ¯æ¡æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
{{
  "input": {{
    "results": [
      {{"condition": "ç–¾ç—…1", "probability": 0.xx, "description": "ç–¾ç—…æè¿°"}},
      {{"condition": "ç–¾ç—…2", "probability": 0.yy, "description": "ç–¾ç—…æè¿°"}}
    ],
    "rag_docs": [
      {{"doc_id": "drug_doc_001", "score": 0.72, "snippet": "è¯ç†/ç¦å¿Œç›¸å…³æ‘˜è¦1"}},
      {{"doc_id": "drug_doc_002", "score": 0.40, "snippet": "è¯ç†/ç¦å¿Œç›¸å…³æ‘˜è¦2"}}
    ]
  }},
  "output": {{
    "diagnosisScore": 0-5,
    "diagnosisComment": "è¯„ä»·æ„è§"
  }}
}}

è¦æ±‚ï¼š
1. ç”Ÿæˆä¸åŒç›¸å…³åº¦çš„æ ·æœ¬ï¼ˆä½:0-2, ä¸­:3, é«˜:4-5ï¼‰
2. rag_docs è¦æ¨¡æ‹ŸçœŸå®è¯ç‰©çŸ¥è¯†åº“æ£€ç´¢ç»“æœ
3. åŒ…å«è´Ÿæ ·æœ¬ä¸è¾¹ç¼˜æ ·æœ¬ï¼ˆç›¸è¿‘ä½†ä¸åŒ¹é…ï¼‰
4. ç›´æ¥è¿”å› JSON æ•°ç»„

è¯·è¿”å›ä¸€ä¸ªåŒ…å« {batch_size} æ¡æ•°æ®çš„ JSON æ•°ç»„ï¼š"""
    },

    # Agent 5: è¯Šæ–­ç”Ÿæˆ (generateDiagnosis) - æ ¸å¿ƒä»»åŠ¡
    "diagnosis": {
        "system": """ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ•°æ®ç”ŸæˆåŠ©æ‰‹ã€‚ä½ éœ€è¦ç”Ÿæˆç”¨äºè®­ç»ƒ"è¯Šæ–­ç”Ÿæˆ"æ¨¡å‹çš„æ•°æ®ã€‚

ä»»åŠ¡è¯´æ˜ï¼š
- è¾“å…¥ï¼šæ‚£è€…ç—‡çŠ¶ (optimized_symptoms) å’ŒåŒ»å­¦èƒŒæ™¯ (ragContext)
- è¾“å‡ºï¼šè¯Šæ–­ç»“æœï¼ˆ3ä¸ªå¯èƒ½ç–¾ç—…åŠæ¦‚ç‡ï¼‰ã€å¥åº·å»ºè®®ã€ç®€åŒ–å»ºè®®

è¾“å‡ºæ ¼å¼ï¼š
- results: 3ä¸ªè¯Šæ–­ç»“æœï¼Œæ¯ä¸ªåŒ…å« condition(ç–¾ç—…å), probability(æ¦‚ç‡), description(æè¿°)
- recommendations: 3æ¡è¯¦ç»†å¥åº·å»ºè®®ï¼ˆæ¯æ¡>=15å­—ï¼‰
- recomm_short: 10æ¡ç®€åŒ–å»ºè®®ï¼ˆæ¯æ¡<=10å­—ï¼‰

è¯·ç¡®ä¿è¯Šæ–­ç»“æœç¬¦åˆåŒ»å­¦å¸¸è¯†ã€‚""",

        "user_template": """è¯·ç”Ÿæˆ {batch_size} æ¡"è¯Šæ–­ç”Ÿæˆ"è®­ç»ƒæ•°æ®ã€‚

æ¯æ¡æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
{{
  "input": {{
    "optimized_symptoms": "æ‚£è€…çš„è¯¦ç»†ç—‡çŠ¶æè¿°",
    "ragContext": "ç›¸å…³çš„åŒ»å­¦èƒŒæ™¯çŸ¥è¯†"
  }},
  "output": {{
    "results": [
      {{"condition": "ç–¾ç—…1", "probability": 0.xx, "description": "ç–¾ç—…æè¿°"}},
      {{"condition": "ç–¾ç—…2", "probability": 0.yy, "description": "ç–¾ç—…æè¿°"}},
      {{"condition": "ç–¾ç—…3", "probability": 0.zz, "description": "ç–¾ç—…æè¿°"}}
    ],
    "recommendations": ["è¯¦ç»†å»ºè®®1(>=15å­—)", "è¯¦ç»†å»ºè®®2", "è¯¦ç»†å»ºè®®3"],
    "recomm_short": ["ç®€åŒ–1", "ç®€åŒ–2", "ç®€åŒ–3", "ç®€åŒ–4", "ç®€åŒ–5", "ç®€åŒ–6", "ç®€åŒ–7", "ç®€åŒ–8", "ç®€åŒ–9", "ç®€åŒ–10"]
  }}
}}

è¦æ±‚ï¼š
1. è¯Šæ–­ç»“æœè¦ç¬¦åˆåŒ»å­¦å¸¸è¯†ï¼Œæ¦‚ç‡ä¹‹å’Œçº¦ä¸º1
2. æ¶µç›–å¸¸è§ç—…å’Œä¸€äº›ç‰¹æ®Šæƒ…å†µ
3. å»ºè®®è¦å…·ä½“å¯æ“ä½œ
4. ç›´æ¥è¿”å› JSON æ•°ç»„
5. å¿…é¡»è¾“å‡ºä¸¥æ ¼åˆæ³• JSONï¼ˆåªå…è®¸åŒå¼•å·ï¼Œä¸è¦æ³¨é‡Šã€ä¸è¦ Markdownã€ä¸è¦é¢å¤–æ–‡å­—ï¼‰
6. ç¦æ­¢ä½¿ç”¨çœç•¥å·/å ä½ç¬¦ï¼ˆä¾‹å¦‚ "..." æˆ– "recommendations": [...]ï¼‰
7. recommendations å¿…é¡»æ­£å¥½ 3 æ¡ï¼Œrecomm_short å¿…é¡»æ­£å¥½ 10 æ¡

è¯·è¿”å›ä¸€ä¸ªåŒ…å« {batch_size} æ¡æ•°æ®çš„ JSON æ•°ç»„ï¼š"""
    },

    # Agent 5: ç”¨è¯å»ºè®® (generateDrugRecommendations)
    "drug": {
        "system": """ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ•°æ®ç”ŸæˆåŠ©æ‰‹ã€‚ä½ éœ€è¦ç”Ÿæˆç”¨äºè®­ç»ƒ"ç”¨è¯å»ºè®®"æ¨¡å‹çš„æ•°æ®ã€‚

ä»»åŠ¡è¯´æ˜ï¼š
- è¾“å…¥ï¼šè¯Šæ–­ç»“æœ (condition) å’Œè¯ç‰©çŸ¥è¯†åº“å†…å®¹ (drugRagContext)
- è¾“å‡ºï¼šç”¨è¯å»ºè®®åˆ—è¡¨

è¾“å‡ºæ ¼å¼ï¼š
æ¯ä¸ªè¯ç‰©åŒ…å«ï¼šname(è¯å), usage(ç”¨æ³•ç”¨é‡), notes(æ³¨æ„äº‹é¡¹)

è¯·ç¡®ä¿ç”¨è¯å»ºè®®ç¬¦åˆåŒ»å­¦è§„èŒƒã€‚""",

        "user_template": """è¯·ç”Ÿæˆ {batch_size} æ¡"ç”¨è¯å»ºè®®"è®­ç»ƒæ•°æ®ã€‚

æ¯æ¡æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
{{
  "input": {{
    "condition": "è¯Šæ–­çš„ç–¾ç—…åç§°",
    "drugRagContext": "è¯ç‰©çŸ¥è¯†åº“æ£€ç´¢ç»“æœ"
  }},
  "output": {{
    "drugs": [{{
      "condition": "ç–¾ç—…åç§°",
      "recommended_drugs": [
        {{"name": "è¯å“å", "usage": "ç”¨æ³•ç”¨é‡", "notes": "æ³¨æ„äº‹é¡¹"}},
        {{"name": "è¯å“å2", "usage": "ç”¨æ³•ç”¨é‡", "notes": "æ³¨æ„äº‹é¡¹"}}
      ]
    }}]
  }}
}}

è¦æ±‚ï¼š
1. è¯ç‰©åç§°ä½¿ç”¨æ ‡å‡†å
2. ç”¨æ³•ç”¨é‡è¦å…·ä½“
3. æ³¨æ„äº‹é¡¹è¦åŒ…å«ç¦å¿Œç—‡
4. ç›´æ¥è¿”å› JSON æ•°ç»„
5. å¿…é¡»è¾“å‡ºä¸¥æ ¼åˆæ³• JSONï¼ˆåªå…è®¸åŒå¼•å·ï¼Œä¸è¦æ³¨é‡Šã€ä¸è¦ Markdownã€ä¸è¦é¢å¤–æ–‡å­—ï¼‰
6. ç¦æ­¢ä½¿ç”¨çœç•¥å·/å ä½ç¬¦ï¼ˆä¾‹å¦‚ "..." æˆ– "recommended_drugs": [...]ï¼‰

è¯·è¿”å›ä¸€ä¸ªåŒ…å« {batch_size} æ¡æ•°æ®çš„ JSON æ•°ç»„ï¼š"""
    }
}


# ==================== æ•°æ®ç”Ÿæˆç±» ====================

class SFTDataGenerator:
    def __init__(
        self,
        api_key: str,
        base_url: str = None,
        model: str = "gpt-4.1",
        input_price_per_million: float = 0.0,
        output_price_per_million: float = 0.0
    ):
        """
        åˆå§‹åŒ–æ•°æ®ç”Ÿæˆå™¨

        Args:
            api_key: OpenAI API Key
            base_url: API Base URL (å¯é€‰ï¼Œç”¨äºä»£ç†)
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        """
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model
        self.total_tokens = 0
        self.total_cost = 0.0

        # ä»·æ ¼ï¼ˆper 1M tokensï¼‰ï¼Œå¯é€šè¿‡å‘½ä»¤è¡Œä¼ å…¥
        self.input_price = float(input_price_per_million) / 1_000_000
        self.output_price = float(output_price_per_million) / 1_000_000

    def generate_batch(self, agent: str, batch_size: int = 10, max_retries: int = 2) -> List[Dict]:
        """
        ç”Ÿæˆä¸€æ‰¹æ•°æ®

        Args:
            agent: Agent ç±»å‹
            batch_size: æ¯æ‰¹ç”Ÿæˆæ•°é‡

        Returns:
            ç”Ÿæˆçš„æ•°æ®åˆ—è¡¨
        """
        if agent not in PROMPTS:
            raise ValueError(f"Unknown agent: {agent}")

        prompt_config = PROMPTS[agent]

        for attempt in range(max_retries + 1):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": prompt_config["system"]},
                        {"role": "user", "content": prompt_config["user_template"].format(batch_size=batch_size)}
                    ],
                    temperature=0.8,
                    max_tokens=4000,
                )
                # ç»Ÿè®¡ token ä½¿ç”¨
                usage = response.usage
                input_tokens = usage.prompt_tokens
                output_tokens = usage.completion_tokens
                self.total_tokens += input_tokens + output_tokens

                # è®¡ç®—æˆæœ¬
                cost = input_tokens * self.input_price + output_tokens * self.output_price
                self.total_cost += cost

                # è§£æè¿”å›çš„ JSON
                content = response.choices[0].message.content.strip()

                # å°è¯•æå– JSON æ•°ç»„
                if content.startswith("```"):
                    content = content.split("```")[1]
                    if content.startswith("json"):
                        content = content[4:]

                data = json.loads(content)
                data_list = data if isinstance(data, list) else [data]
                filtered = [d for d in data_list if validate_item(agent, d)]
                if filtered:
                    return filtered
                logger.warning("Schema validation produced empty batch, retrying...")
            except json.JSONDecodeError as e:
                logger.warning(f"JSON è§£æå¤±è´¥ (attempt {attempt + 1}): {e}")
                logger.debug(f"åŸå§‹å†…å®¹: {content[:500]}...")
            except Exception as e:
                logger.error(f"API è°ƒç”¨å¤±è´¥ (attempt {attempt + 1}): {e}")

            if attempt < max_retries:
                time.sleep(0.5)
                continue
            return []

    def generate_dataset(
        self,
        agent: str,
        num_samples: int,
        batch_size: int = 10,
        output_file: str = None,
        output_format: str = "instruction"
    ) -> List[Dict]:
        """
        ç”Ÿæˆå®Œæ•´æ•°æ®é›†

        Args:
            agent: Agent ç±»å‹
            num_samples: æ€»æ ·æœ¬æ•°
            batch_size: æ¯æ‰¹å¤§å°
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            æ‰€æœ‰ç”Ÿæˆçš„æ•°æ®
        """
        all_data = []
        num_batches = (num_samples + batch_size - 1) // batch_size

        logger.info(f"å¼€å§‹ç”Ÿæˆ {agent} æ•°æ®é›†: {num_samples} æ¡ï¼Œåˆ† {num_batches} æ‰¹")

        for i in tqdm(range(num_batches), desc=f"Generating {agent}"):
            current_batch_size = min(batch_size, num_samples - len(all_data))

            batch_data = self.generate_batch(agent, current_batch_size)
            all_data.extend(batch_data)

            # é¿å… rate limit
            if i < num_batches - 1:
                time.sleep(0.5)

        logger.info(f"ç”Ÿæˆå®Œæˆ: {len(all_data)} æ¡æ•°æ®")
        logger.info(f"æ€» Token: {self.total_tokens:,}")
        logger.info(f"é¢„ä¼°æˆæœ¬: ${self.total_cost:.4f}")

        # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼å¹¶ä¿å­˜
        if output_file:
            training_data = self.convert_to_training_format(all_data, agent, output_format=output_format)
            self.save_jsonl(training_data, output_file)
            logger.info(f"å·²ä¿å­˜åˆ°: {output_file}")

        return all_data

    def convert_to_training_format(self, data: List[Dict], agent: str, output_format: str = "instruction") -> List[Dict]:
        """
        è½¬æ¢ä¸º SFT è®­ç»ƒæ ¼å¼

        Args:
            data: åŸå§‹æ•°æ®
            agent: Agent ç±»å‹

        Returns:
            è®­ç»ƒæ ¼å¼æ•°æ®
        """
        if output_format == "conversations":
            output_format = "conversation"
        if output_format not in {"instruction", "conversation"}:
            raise ValueError(f"Unknown output_format: {output_format}")

        training_data = []

        for item in data:
            if "input" not in item or "output" not in item:
                continue

            if output_format == "conversation":
                human_content = self._build_human_prompt(item["input"], agent)
                gpt_content = json.dumps(item["output"], ensure_ascii=False)
                training_item = {
                    "conversations": [
                        {"from": "human", "value": human_content},
                        {"from": "gpt", "value": gpt_content}
                    ]
                }
            else:
                training_item = {
                    "instruction": INSTRUCTIONS.get(agent, ""),
                    "input": item["input"],
                    "output": item["output"],
                }
            training_data.append(training_item)

        return training_data

    def _build_human_prompt(self, input_data: Dict, agent: str) -> str:
        """æ ¹æ® agent ç±»å‹æ„å»ºç”¨æˆ·è¾“å…¥"""
        if agent == "preprocess":
            return f"""è¯·å°†ä»¥ä¸‹ç—‡çŠ¶ä¿¡æ¯è½¬æ¢ä¸ºç»“æ„åŒ–çš„åŒ»å­¦æè¿°ï¼š
èº«ä½“éƒ¨ä½: {input_data.get('body_part', '')}
ä¸»è¦ç—‡çŠ¶: {', '.join(input_data.get('symptoms', []))}
å…¶ä»–ç—‡çŠ¶: {input_data.get('other_symptoms', '')}
ä¸¥é‡ç¨‹åº¦: {input_data.get('severity', 3)}
æŒç»­æ—¶é—´: {input_data.get('duration', '')}

è¯·è¾“å‡º JSON æ ¼å¼ï¼š{{"optimized_symptoms": "...", "rag_keywords": [...]}}"""

        elif agent == "critic":
            return f"""è¯·å¯¹ä»¥ä¸‹ç—‡çŠ¶ç»“æ„åŒ–ç»“æœè¿›è¡Œè¯„åˆ†ï¼š
{json.dumps(input_data, ensure_ascii=False, indent=2)}

è¯·è¾“å‡º JSON æ ¼å¼ï¼š{{"score": 0-5, "comment": "...", "isValid": true/false}}"""

        elif agent == "check_rag":
            return f"""è¯·è¯„ä¼° RAG è¿”å›å†…å®¹ä¸ç—‡çŠ¶çš„ç›¸å…³æ€§ï¼š
ã€ç—‡çŠ¶ã€‘{input_data.get('optimized_symptoms', '')}
ã€RAGæ£€ç´¢æ‘˜è¦ã€‘{json.dumps(input_data.get('rag_docs', []), ensure_ascii=False)}

è¯·è¾“å‡º JSON æ ¼å¼ï¼š{{"ragScore": 0-5, "ragComment": "..."}}"""

        elif agent == "diagnosis":
            return f"""è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆè¯Šæ–­ç»“æœï¼š
ã€ç—‡çŠ¶ã€‘{input_data.get('optimized_symptoms', '')}
ã€åŒ»å­¦èƒŒæ™¯ã€‘{input_data.get('ragContext', '')}

è¯·è¾“å‡ºè¯Šæ–­ JSONã€‚"""

        elif agent == "drug":
            return f"""è¯·ä¸ºä»¥ä¸‹ç–¾ç—…ç”Ÿæˆç”¨è¯å»ºè®®ï¼š
ã€è¯Šæ–­ã€‘{input_data.get('condition', '')}
ã€è¯ç‰©çŸ¥è¯†ã€‘{input_data.get('drugRagContext', '')}

è¯·è¾“å‡ºç”¨è¯å»ºè®® JSONã€‚"""

        elif agent == "drug_evidence":
            return f"""è¯·è¯„ä¼°è¯Šæ–­ä¸è¯ç‰©è¯æ®çš„ç›¸å…³æ€§ï¼š
ã€è¯Šæ–­ç»“æœã€‘{json.dumps(input_data.get('results', []), ensure_ascii=False)}
ã€RAGæ£€ç´¢æ‘˜è¦ã€‘{json.dumps(input_data.get('rag_docs', []), ensure_ascii=False)}

è¯·è¾“å‡º JSON æ ¼å¼ï¼š{{"diagnosisScore": 0-5, "diagnosisComment": "..."}}"""

        return json.dumps(input_data, ensure_ascii=False)

    @staticmethod
    def save_jsonl(data: List[Dict], filepath: str):
        """ä¿å­˜ä¸º JSONL æ ¼å¼"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')


def _medicalize(symptom: str) -> str:
    return SYMPTOM_MEDICAL_MAP.get(symptom, symptom)


def _severity_text(severity: int) -> str:
    return ["è½»å¾®", "è¾ƒè½»", "ä¸­ç­‰", "è¾ƒé‡", "ä¸¥é‡"][max(0, min(4, severity - 1))]


def _pick_body_part(rng: random.Random) -> str:
    return rng.choice(list(SYMPTOMS_BY_BODY_PART.keys()))


def generate_seed_samples(agent: str, num_samples: int, seed: int = 42) -> List[Dict]:
    rng = random.Random(seed + hash(agent) % 1000)
    samples: List[Dict[str, Any]] = []

    for _ in range(num_samples):
        body_part = _pick_body_part(rng)
        symptoms = rng.sample(SYMPTOMS_BY_BODY_PART[body_part], k=2)
        other_symptoms = rng.choice(OTHER_SYMPTOMS_POOL)
        severity = rng.choice(SEVERITY_LEVELS)
        duration = rng.choice(DURATION_OPTIONS)
        duration_text = DURATION_CHINESE.get(duration, duration)

        if agent == "preprocess":
            medical_symptoms = [_medicalize(s) for s in symptoms]
            optimized_symptoms = (
                f"æ‚£è€…ä¸»è¯‰{body_part}å‡ºç°{ 'ã€'.join(medical_symptoms) }ï¼Œ"
                f"ä¼´æœ‰{other_symptoms}ï¼Œç—‡çŠ¶ç¨‹åº¦{_severity_text(severity)}ï¼ŒæŒç»­{duration_text}ã€‚"
            )
            rag_keywords = [f"{body_part}{s}" for s in medical_symptoms]
            rag_keywords.append(f"{body_part}{other_symptoms}")
            rag_keywords.extend([_severity_text(severity), duration_text])
            samples.append({
                "input": {
                    "body_part": body_part,
                    "symptoms": symptoms,
                    "other_symptoms": other_symptoms,
                    "severity": severity,
                    "duration": duration,
                },
                "output": {
                    "optimized_symptoms": optimized_symptoms,
                    "rag_keywords": rag_keywords,
                }
            })

        elif agent == "critic":
            medical_symptoms = [_medicalize(s) for s in symptoms]
            optimized_symptoms = (
                f"æ‚£è€…{body_part}å‡ºç°{ 'ã€'.join(medical_symptoms) }ï¼Œ"
                f"ç¨‹åº¦{_severity_text(severity)}ï¼ŒæŒç»­{duration_text}ã€‚"
            )
            rag_keywords = [body_part] + medical_symptoms
            quality = rng.choice(["good", "medium", "bad"])
            if quality == "bad":
                optimized_symptoms = "è‚šå­ç—›ï¼Œæ„Ÿè§‰ä¸èˆ’æœ"
                rag_keywords = ["ä¸èˆ’æœ"]
                score = 1
            elif quality == "medium":
                rag_keywords = medical_symptoms
                score = 3
            else:
                score = 5
            samples.append({
                "input": {
                    "optimized_symptoms": optimized_symptoms,
                    "rag_keywords": rag_keywords,
                },
                "output": {
                    "score": score,
                    "comment": "ç»“æ„åŒ–è´¨é‡è¯„ä¼°ç»“æœã€‚",
                    "isValid": score >= 3,
                }
            })

        elif agent == "check_rag":
            optimized_symptoms = f"æ‚£è€…{body_part}å‡ºç°{_medicalize(symptoms[0])}ï¼Œä¼´{other_symptoms}"
            r = rng.random()
            if r < 0.15:
                rag_docs = [
                    {"doc_id": "doc_009", "score": 0.18, "snippet": "çš®è‚¤è¿‡æ•å¸¸è§è¡¨ç°ä¸ºçš®ç–¹ã€ç˜™ç—’ï¼Œä¸å½“å‰ç—‡çŠ¶æ— å…³ã€‚"}
                ]
                rag_score = 0
                rag_comment = "å†…å®¹ä¸ç—‡çŠ¶æ— å…³ã€‚"
            elif r < 0.3:
                rag_docs = [
                    {"doc_id": "doc_012", "score": 0.24, "snippet": "ä¸€èˆ¬æ€§å¥åº·ç§‘æ™®å†…å®¹ï¼Œä¸å½“å‰ç—‡çŠ¶å…³è”å¾ˆå¼±ã€‚"}
                ]
                rag_score = 1
                rag_comment = "å‡ ä¹æ— å…³ï¼Œä»…æœ‰æ³›åŒ–ä¿¡æ¯ã€‚"
            elif r < 0.5:
                rag_docs = [
                    {"doc_id": "doc_015", "score": 0.30, "snippet": f"{body_part}ä¸é€‚å¯èƒ½ä¸ç–²åŠ³ç›¸å…³ï¼Œä½†ç¼ºå°‘å…·ä½“ç—‡çŠ¶æè¿°ã€‚"}
                ]
                rag_score = 2
                rag_comment = "ç›¸å…³æ€§è¾ƒå¼±ï¼Œç¼ºå°‘æ ¸å¿ƒç—‡çŠ¶ã€‚"
            elif r < 0.7:
                rag_docs = [
                    {"doc_id": "doc_021", "score": 0.52, "snippet": f"{body_part}ç›¸å…³ç–¾ç—…å¯èƒ½åŒ…æ‹¬{CONDITION_BY_BODY_PART[body_part][1]}ï¼Œä½†ä¸ä¸»è¦ç—‡çŠ¶åŒ¹é…ä¸å®Œæ•´ã€‚"}
                ]
                rag_score = 3
                rag_comment = "éƒ¨åˆ†ç›¸å…³ï¼Œä½†ç¼ºå°‘å…³é”®çº¿ç´¢ã€‚"
            elif r < 0.9:
                rag_docs = [
                    {"doc_id": "doc_001", "score": 0.78, "snippet": f"{body_part}ç›¸å…³ç–¾ç—…å¯èƒ½åŒ…æ‹¬{CONDITION_BY_BODY_PART[body_part][0]}ï¼Œå¸¸è§è¡¨ç°ä¸º{_medicalize(symptoms[0])}ç­‰ã€‚"}
                ]
                rag_score = 4
                rag_comment = "RAGå†…å®¹ä¸ç—‡çŠ¶ç›¸å…³ï¼Œå…·æœ‰å‚è€ƒä»·å€¼ã€‚"
            else:
                rag_docs = [
                    {"doc_id": "doc_002", "score": 0.90, "snippet": f"{body_part}{_medicalize(symptoms[0])}æ˜¯{CONDITION_BY_BODY_PART[body_part][0]}çš„å…¸å‹è¡¨ç°ï¼Œå¹¶éœ€ä¸{CONDITION_BY_BODY_PART[body_part][2]}é‰´åˆ«ã€‚"}
                ]
                rag_score = 5
                rag_comment = "é«˜åº¦ç›¸å…³ï¼Œä¸”åŒ…å«å…³é”®é‰´åˆ«è¦ç‚¹ã€‚"
            samples.append({
                "input": {
                    "optimized_symptoms": optimized_symptoms,
                    "rag_docs": rag_docs,
                },
                "output": {
                    "ragScore": rag_score,
                    "ragComment": rag_comment,
                }
            })

        elif agent == "diagnosis":
            optimized_symptoms = f"æ‚£è€…{body_part}å‡ºç°{_medicalize(symptoms[0])}ï¼Œä¼´{other_symptoms}ï¼ŒæŒç»­{duration_text}ã€‚"
            rag_context = f"{body_part}ç›¸å…³å¸¸è§ç–¾ç—…åŒ…æ‹¬{', '.join(CONDITION_BY_BODY_PART[body_part])}ã€‚"
            conditions = CONDITION_BY_BODY_PART[body_part]
            samples.append({
                "input": {
                    "optimized_symptoms": optimized_symptoms,
                    "ragContext": rag_context,
                },
                "output": {
                    "results": [
                        {"condition": conditions[0], "probability": 0.6, "description": "ä¸ç—‡çŠ¶åŒ¹é…åº¦è¾ƒé«˜çš„å¸¸è§ç–¾ç—…ã€‚"},
                        {"condition": conditions[1], "probability": 0.25, "description": "å¯èƒ½æ€§ä¸­ç­‰ï¼Œéœ€è¦ç»“åˆè¿›ä¸€æ­¥æ£€æŸ¥ã€‚"},
                        {"condition": conditions[2], "probability": 0.15, "description": "å­˜åœ¨ä¸€å®šå¯èƒ½ï¼Œéœ€é‰´åˆ«è¯Šæ–­ã€‚"},
                    ],
                    "recommendations": [
                        "å»ºè®®ä¿æŒæ¸…æ·¡é¥®é£Ÿï¼Œé¿å…è¾›è¾£åˆºæ¿€é£Ÿç‰©ã€‚",
                        "æ³¨æ„ä¼‘æ¯å¹¶è§‚å¯Ÿç—‡çŠ¶å˜åŒ–ï¼Œå¦‚åŠ é‡åŠæ—¶å°±åŒ»ã€‚",
                        "å¦‚ç—‡çŠ¶æŒç»­è¶…è¿‡ä¸€å‘¨å»ºè®®åˆ°æ­£è§„åŒ»é™¢æ£€æŸ¥ã€‚",
                    ],
                    "recomm_short": ["æ¸…æ·¡é¥®é£Ÿ", "æ³¨æ„ä¼‘æ¯", "åŠæ—¶å°±åŒ»", "è§‚å¯Ÿå˜åŒ–", "é¿å…åˆºæ¿€", "ä¿æŒæ°´åˆ†", "è§„å¾‹ä½œæ¯", "é€‚åº¦æ´»åŠ¨", "æŒ‰æ—¶å¤è¯Š", "éµåŒ»å˜±"],
                }
            })

        elif agent == "drug_evidence":
            conditions = CONDITION_BY_BODY_PART[body_part]
            results = [
                {"condition": conditions[0], "probability": 0.6, "description": "ä¸ç—‡çŠ¶åŒ¹é…åº¦è¾ƒé«˜çš„å¸¸è§ç–¾ç—…ã€‚"},
                {"condition": conditions[1], "probability": 0.3, "description": "å¯èƒ½æ€§ä¸­ç­‰ï¼Œéœ€è¦ç»“åˆè¿›ä¸€æ­¥æ£€æŸ¥ã€‚"},
            ]
            r = rng.random()
            if r < 0.15:
                rag_docs = [
                    {"doc_id": "drug_doc_008", "score": 0.18, "snippet": "çš®è‚¤å¤–ç”¨è¯ç‰©çš„æ³¨æ„äº‹é¡¹ï¼Œä¸å½“å‰è¯Šæ–­æ— å…³ã€‚"},
                ]
                diagnosis_score = 0
                diagnosis_comment = "è¯æ®ä¸è¯Šæ–­æ— å…³ã€‚"
            elif r < 0.3:
                rag_docs = [
                    {"doc_id": "drug_doc_009", "score": 0.25, "snippet": "ä¸€èˆ¬æ€§ç”¨è¯å®‰å…¨è¯´æ˜ï¼Œä¸å½“å‰è¯Šæ–­è”ç³»å¾ˆå¼±ã€‚"},
                ]
                diagnosis_score = 1
                diagnosis_comment = "å‡ ä¹æ— å…³ï¼Œä»…æœ‰æ³›åŒ–è¯ç‰©ä¿¡æ¯ã€‚"
            elif r < 0.5:
                rag_docs = [
                    {"doc_id": "drug_doc_010", "score": 0.32, "snippet": "å¯¹ç—‡è¯çš„å¸¸è§ä¸è‰¯ååº”æ¦‚è¿°ï¼Œä½†æœªæåŠå…·ä½“ç–¾ç—…ã€‚"},
                ]
                diagnosis_score = 2
                diagnosis_comment = "ç›¸å…³æ€§å¼±ï¼Œç¼ºå°‘å…³é”®ç”¨è¯ä¿¡æ¯ã€‚"
            elif r < 0.7:
                rag_docs = [
                    {"doc_id": "drug_doc_013", "score": 0.55, "snippet": f"{conditions[1]}å¯èƒ½ä½¿ç”¨çš„è¯ç‰©æè¦ï¼Œä½†ç¼ºä¹ç¦å¿Œæˆ–é€‚åº”è¯ç»†èŠ‚ã€‚"},
                ]
                diagnosis_score = 3
                diagnosis_comment = "éƒ¨åˆ†ç›¸å…³ï¼Œä½†è¯æ®ä¸å……åˆ†ã€‚"
            elif r < 0.9:
                rag_docs = [
                    {"doc_id": "drug_doc_001", "score": 0.76, "snippet": f"{conditions[0]}çš„å¸¸ç”¨æ²»ç–—è¯ç‰©åŒ…æ‹¬å¯¹ç—‡è¯å’ŒåŸºç¡€æ”¯æŒæ²»ç–—ã€‚"},
                ]
                diagnosis_score = 4
                diagnosis_comment = "è¯æ®ä¸è¯Šæ–­ç»“æœç›¸å…³ï¼Œåˆç†ã€‚"
            else:
                rag_docs = [
                    {"doc_id": "drug_doc_002", "score": 0.88, "snippet": f"{conditions[0]}çš„ä¸€çº¿ç”¨è¯å»ºè®®åŠç¦å¿Œæç¤ºï¼Œé€‚ç”¨äºå½“å‰è¯Šæ–­ã€‚"},
                ]
                diagnosis_score = 5
                diagnosis_comment = "é«˜åº¦ç›¸å…³ï¼ŒåŒ…å«å…³é”®å®‰å…¨ä¿¡æ¯ã€‚"
            samples.append({
                "input": {
                    "results": results,
                    "rag_docs": rag_docs,
                },
                "output": {
                    "diagnosisScore": diagnosis_score,
                    "diagnosisComment": diagnosis_comment,
                }
            })

        elif agent == "drug":
            condition = rng.choice(CONDITION_BY_BODY_PART[body_part])
            drug_list = DRUG_RECOMMENDATIONS.get(condition, [
                {"name": "è¯·å’¨è¯¢åŒ»ç”Ÿ", "usage": "æ ¹æ®å…·ä½“æƒ…å†µç”¨è¯", "notes": "éœ€ä¸“ä¸šåŒ»ç”ŸæŒ‡å¯¼"}
            ])
            rag_context = f"{condition}å¸¸ç”¨è¯ç‰©åŒ…æ‹¬ï¼š{', '.join([d['name'] for d in drug_list])}ã€‚"
            samples.append({
                "input": {
                    "condition": condition,
                    "drugRagContext": rag_context,
                },
                "output": {
                    "drugs": [{
                        "condition": condition,
                        "recommended_drugs": drug_list,
                    }]
                }
            })

    return samples


# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(description="SFT æ•°æ®ç”Ÿæˆè„šæœ¬")
    parser.add_argument("--agent", type=str, required=True,
                        choices=[
                            "symptom_normalizer", "symptom_quality_grader", "rag_relevance_grader",
                            "drug_evidence_grader",
                            "preprocess", "critic", "check_rag", "drug_evidence",
                            "diagnosis_generator", "drug_recommender",
                            "diagnosis", "drug",
                            "all"
                        ],
                        help="è¦ç”Ÿæˆæ•°æ®çš„ Agent ç±»å‹")
    parser.add_argument("--num_samples", type=int, default=100,
                        help="ç”Ÿæˆæ ·æœ¬æ•°é‡")
    parser.add_argument("--batch_size", type=int, default=10,
                        help="æ¯æ‰¹ç”Ÿæˆæ•°é‡")
    parser.add_argument("--api_key", type=str, default=None,
                        help="OpenAI API Key (ä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡ OPENAI_API_KEY è®¾ç½®)")
    parser.add_argument("--base_url", type=str, default=None,
                        help="API Base URL (å¯é€‰)")
    parser.add_argument("--model", type=str, default="gpt-4.1",
                        help="ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤ gpt-4.1ï¼Œé¿å… gpt-4oï¼‰")
    parser.add_argument("--price_in", type=float, default=0.0,
                        help="è¾“å…¥tokenä»·æ ¼ï¼ˆæ¯1M tokensï¼‰ï¼Œç”¨äºæˆæœ¬ä¼°ç®—")
    parser.add_argument("--price_out", type=float, default=0.0,
                        help="è¾“å‡ºtokenä»·æ ¼ï¼ˆæ¯1M tokensï¼‰ï¼Œç”¨äºæˆæœ¬ä¼°ç®—")
    parser.add_argument("--output_dir", type=str, default="./",
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--resume", action="store_true",
                        help="å¦‚æœè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™ç»§ç»­è¿½åŠ ç”Ÿæˆ")
    parser.add_argument("--test_cost", action="store_true",
                        help="æµ‹è¯•æ¨¡å¼ï¼Œåªæ‰“å°é¢„ä¼°æˆæœ¬")
    parser.add_argument("--seed_mode", action="store_true",
                        help="å¯ç”¨æœ¬åœ° seed æ•°æ®ç”Ÿæˆï¼ˆä¸è°ƒç”¨ APIï¼‰")
    parser.add_argument("--synthetic_mode", action="store_true",
                        help="å¯ç”¨ API åˆæˆæ•°æ®ç”Ÿæˆ")
    parser.add_argument("--seed", type=int, default=42,
                        help="seed æ¨¡å¼éšæœºç§å­")
    parser.add_argument("--output_format", type=str, default="instruction",
                        choices=["instruction", "conversation"],
                        help="è¾“å‡ºæ ¼å¼ï¼šinstruction æˆ– conversation")

    args = parser.parse_args()

    if not args.seed_mode and not args.synthetic_mode:
        args.synthetic_mode = True

    generator = None
    if args.synthetic_mode:
        api_key = args.api_key or os.environ.get("OPENAI_API_KEY")
        if not api_key:
            logger.error("è¯·æä¾› OpenAI API Key (--api_key æˆ–ç¯å¢ƒå˜é‡ OPENAI_API_KEY)")
            return
        generator = SFTDataGenerator(
            api_key=api_key,
            base_url=args.base_url,
            model=args.model,
            input_price_per_million=args.price_in,
            output_price_per_million=args.price_out
        )

    # ç¡®å®šè¦ç”Ÿæˆçš„ Agent åˆ—è¡¨
    if args.agent == "all":
        agents = ["symptom_quality_grader", "rag_relevance_grader", "drug_evidence_grader"]
    else:
        agents = [args.agent]

    # ç”Ÿæˆæ•°æ®
    for agent in agents:
        resolved_agent = AGENT_ALIASES.get(agent)
        if not resolved_agent:
            logger.error(f"Unknown agent: {agent}")
            continue
        output_file = os.path.join(args.output_dir, f"{agent}_sft_data.jsonl")
        seed_output_file = os.path.join(args.output_dir, f"{agent}_seed.jsonl")

        logger.info(f"\n{'='*50}")
        logger.info(f"ç”Ÿæˆ {agent} æ•°æ®é›†")
        logger.info(f"{'='*50}")

        if args.seed_mode:
            seed_data = generate_seed_samples(resolved_agent, args.num_samples, seed=args.seed)
            seed_training = []
            if generator:
                seed_training = generator.convert_to_training_format(seed_data, resolved_agent, output_format=args.output_format)
            else:
                temp_gen = SFTDataGenerator(api_key="seed-only", base_url=args.base_url, model=args.model)
                seed_training = temp_gen.convert_to_training_format(seed_data, resolved_agent, output_format=args.output_format)
            if args.resume and os.path.exists(seed_output_file):
                with open(seed_output_file, 'a', encoding='utf-8') as f:
                    for item in seed_training:
                        f.write(json.dumps(item, ensure_ascii=False) + '\n')
            else:
                SFTDataGenerator.save_jsonl(seed_training, seed_output_file)
            logger.info(f"Seed æ•°æ®å·²ä¿å­˜åˆ°: {seed_output_file}")

        data = []
        if args.synthetic_mode and generator:
            if args.resume and os.path.exists(output_file):
                # ä¼°ç®—å·²å­˜åœ¨æ ·æœ¬æ•°
                with open(output_file, 'r', encoding='utf-8') as f:
                    existing_count = sum(1 for _ in f)
                remaining = max(args.num_samples - existing_count, 0)
                if remaining == 0:
                    logger.info(f"{output_file} å·²å­˜åœ¨ {existing_count} æ¡ï¼Œè·³è¿‡ç”Ÿæˆ")
                else:
                    logger.info(f"{output_file} å·²å­˜åœ¨ {existing_count} æ¡ï¼Œç»§ç»­ç”Ÿæˆ {remaining} æ¡")
                    data = generator.generate_dataset(
                        agent=resolved_agent,
                        num_samples=remaining,
                        batch_size=args.batch_size,
                        output_file=None,
                        output_format=args.output_format
                    )
                    if data:
                        training_data = generator.convert_to_training_format(
                            data, resolved_agent, output_format=args.output_format
                        )
                        with open(output_file, 'a', encoding='utf-8') as f:
                            for item in training_data:
                                f.write(json.dumps(item, ensure_ascii=False) + '\n')
            else:
                data = generator.generate_dataset(
                    agent=resolved_agent,
                    num_samples=args.num_samples,
                    batch_size=args.batch_size,
                    output_file=output_file,
                    output_format=args.output_format
                )

        if args.test_cost and generator:
            logger.info(f"\nğŸ“Š æˆæœ¬ä¼°ç®—:")
            logger.info(f"   - æ€» Token: {generator.total_tokens:,}")
            logger.info(f"   - é¢„ä¼°æˆæœ¬: ${generator.total_cost:.4f}")
            logger.info(f"   - æ¯æ¡æˆæœ¬: ${generator.total_cost/max(len(data),1):.6f}")

            # æ¨ç®— 5000 æ¡çš„æˆæœ¬
            estimated_5k = (5000 / max(args.num_samples, 1)) * generator.total_cost
            logger.info(f"   - 5000æ¡é¢„ä¼°: ${estimated_5k:.2f}")


if __name__ == "__main__":
    main()
