{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IntelHealth Agent SFT æ•°æ®ç”Ÿæˆ\n",
    "\n",
    "ä½¿ç”¨ GPT-4o-mini ä¸ºå„ä¸ª Agent ç”Ÿæˆè®­ç»ƒæ•°æ®\n",
    "\n",
    "## æµç¨‹\n",
    "1. é…ç½® API Key\n",
    "2. æµ‹è¯•ç”Ÿæˆ 100 æ¡ï¼ŒæŸ¥çœ‹æˆæœ¬\n",
    "3. ç¡®è®¤åæ‰¹é‡ç”Ÿæˆå®Œæ•´æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å®‰è£…ä¾èµ– & å¯¼å…¥åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–ï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰\n",
    "# !pip install openai tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict\n",
    "from openai import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "print(\"âœ… åº“å¯¼å…¥æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. é…ç½® API Key\n",
    "\n",
    "åœ¨ä¸‹é¢å¡«å…¥ä½ çš„ OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ é…ç½®åŒº ============\n",
    "\n",
    "# OpenAI API Key (å¡«å…¥ä½ çš„ key)\n",
    "OPENAI_API_KEY = \"sk-proj-xxxxx\"  # <-- æ›¿æ¢æˆä½ çš„ API Key\n",
    "\n",
    "# æ¨¡å‹é€‰æ‹©\n",
    "MODEL = \"gpt-4o-mini\"  # ä¾¿å®œä¸”æ•ˆæœä¸é”™\n",
    "\n",
    "# API Base URL (å¦‚æœç”¨ä»£ç†çš„è¯å–æ¶ˆæ³¨é‡Š)\n",
    "# BASE_URL = \"https://api.openai.com/v1\"  # é»˜è®¤\n",
    "BASE_URL = None  # ä½¿ç”¨é»˜è®¤\n",
    "\n",
    "# è¾“å‡ºç›®å½•\n",
    "OUTPUT_DIR = \"./\"\n",
    "\n",
    "# ================================\n",
    "\n",
    "# åˆå§‹åŒ–å®¢æˆ·ç«¯\n",
    "client = OpenAI(api_key=OPENAI_API_KEY, base_url=BASE_URL)\n",
    "\n",
    "# æµ‹è¯•è¿æ¥\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"è¯´'è¿æ¥æˆåŠŸ'\"}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    print(f\"âœ… API è¿æ¥æˆåŠŸ: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ API è¿æ¥å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å®šä¹‰ Prompt æ¨¡æ¿\n",
    "\n",
    "æ¯ä¸ª Agent æœ‰ä¸åŒçš„ Prompt æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Prompt æ¨¡æ¿ ============\n",
    "\n",
    "PROMPTS = {\n",
    "    # Agent 1: ç—‡çŠ¶ç»“æ„åŒ– (preprocessAndGuess)\n",
    "    \"preprocess\": {\n",
    "        \"name\": \"ç—‡çŠ¶ç»“æ„åŒ– (preprocessAndGuess)\",\n",
    "        \"system\": \"\"\"ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ•°æ®ç”ŸæˆåŠ©æ‰‹ã€‚ä½ éœ€è¦ç”Ÿæˆç”¨äºè®­ç»ƒ\"ç—‡çŠ¶ç»“æ„åŒ–\"æ¨¡å‹çš„æ•°æ®ã€‚\n",
    "\n",
    "ä»»åŠ¡è¯´æ˜ï¼š\n",
    "- è¾“å…¥ï¼šç”¨æˆ·çš„åŸå§‹ç—‡çŠ¶æè¿°ï¼ˆåŒ…æ‹¬èº«ä½“éƒ¨ä½ã€ä¸»è¦ç—‡çŠ¶ã€å…¶ä»–ç—‡çŠ¶ã€ä¸¥é‡ç¨‹åº¦ã€æŒç»­æ—¶é—´ï¼‰\n",
    "- è¾“å‡ºï¼šç»“æ„åŒ–çš„åŒ»å­¦æè¿° (optimized_symptoms) å’Œ RAG æ£€ç´¢å…³é”®è¯ (rag_keywords)\n",
    "\n",
    "ç”Ÿæˆè¦æ±‚ï¼š\n",
    "1. optimized_symptoms åº”è¯¥æ˜¯ä¸“ä¸šã€å®Œæ•´çš„åŒ»å­¦ç—‡çŠ¶æè¿°ï¼ŒåŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯\n",
    "2. rag_keywords åº”è¯¥æ˜¯åŒ»å­¦å…³é”®è¯æ•°ç»„ï¼Œç”¨äºåç»­æ£€ç´¢ï¼Œè‡³å°‘åŒ…å«èº«ä½“éƒ¨ä½å’Œä¸»è¦ç—‡çŠ¶\n",
    "3. å°†å£è¯­åŒ–æè¿°è½¬æ¢ä¸ºä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚\"è‚šå­ç—›\"â†’\"è…¹ç—›\"ï¼‰\n",
    "4. ä¸¥é‡ç¨‹åº¦ç”¨æ–‡å­—æè¿°ï¼ˆ1=è½»å¾®, 2=è¾ƒè½», 3=ä¸­ç­‰, 4=è¾ƒé‡, 5=ä¸¥é‡ï¼‰\n",
    "\n",
    "è¯·ç”Ÿæˆå¤šæ ·åŒ–ã€çœŸå®çš„åŒ»ç–—åœºæ™¯æ•°æ®ã€‚\"\"\",\n",
    "\n",
    "        \"user_template\": \"\"\"è¯·ç”Ÿæˆ {batch_size} æ¡\"ç—‡çŠ¶ç»“æ„åŒ–\"è®­ç»ƒæ•°æ®ã€‚\n",
    "\n",
    "æ¯æ¡æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š\n",
    "{{\n",
    "  \"input\": {{\n",
    "    \"body_part\": \"èº«ä½“éƒ¨ä½\",\n",
    "    \"symptoms\": [\"ç—‡çŠ¶1\", \"ç—‡çŠ¶2\"],\n",
    "    \"other_symptoms\": \"å…¶ä»–ç—‡çŠ¶æè¿°\",\n",
    "    \"severity\": 1-5,\n",
    "    \"duration\": \"æŒç»­æ—¶é—´ä»£ç (lessThan24Hours/1To3Days/4To7Days/1To2Weeks/moreThan2Weeks)\"\n",
    "  }},\n",
    "  \"output\": {{\n",
    "    \"optimized_symptoms\": \"ä¸“ä¸šçš„ç—‡çŠ¶æè¿°æ–‡æœ¬\",\n",
    "    \"rag_keywords\": [\"å…³é”®è¯1\", \"å…³é”®è¯2\", ...]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "1. ç”Ÿæˆå¤šæ ·åŒ–çš„ç—‡çŠ¶ç»„åˆï¼Œè¦†ç›–ä¸åŒèº«ä½“éƒ¨ä½\n",
    "2. è¾“å…¥ä½¿ç”¨å£è¯­åŒ–æè¿°ï¼Œè¾“å‡ºä½¿ç”¨ä¸“ä¸šåŒ»å­¦æœ¯è¯­\n",
    "3. ç¡®ä¿ output æ˜¯åˆç†çš„åŒ»å­¦è½¬æ¢ç»“æœ\n",
    "4. ç›´æ¥è¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŠ ä»»ä½•è§£é‡Šæ–‡å­—æˆ– markdown æ ‡è®°\n",
    "\n",
    "è¯·è¿”å›ä¸€ä¸ªåŒ…å« {batch_size} æ¡æ•°æ®çš„ JSON æ•°ç»„ï¼š\"\"\"\n",
    "    },\n",
    "\n",
    "    # Agent 2: è´¨é‡è¯„åˆ† (Critic_preprocess)\n",
    "    \"critic\": {\n",
    "        \"name\": \"è´¨é‡è¯„åˆ† (Critic_preprocess)\",\n",
    "        \"system\": \"\"\"ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ•°æ®ç”ŸæˆåŠ©æ‰‹ã€‚ä½ éœ€è¦ç”Ÿæˆç”¨äºè®­ç»ƒ\"è¯Šæ–­è´¨é‡è¯„åˆ†\"æ¨¡å‹çš„æ•°æ®ã€‚\n",
    "\n",
    "ä»»åŠ¡è¯´æ˜ï¼š\n",
    "- è¾“å…¥ï¼šç»“æ„åŒ–çš„ç—‡çŠ¶æè¿° (optimized_symptoms) å’Œå…³é”®è¯ (rag_keywords)\n",
    "- è¾“å‡ºï¼šè´¨é‡è¯„åˆ† (score: 0-5) å’Œè¯„ä»·æ„è§ (comment)\n",
    "\n",
    "è¯„åˆ†æ ‡å‡†ï¼ˆæ¯é¡¹1åˆ†ï¼Œæ»¡åˆ†5åˆ†ï¼‰ï¼š\n",
    "1. optimized_symptoms æ ¼å¼è§„èŒƒï¼Œæ˜¯å®Œæ•´é€šé¡ºçš„åŒ»å­¦æè¿°\n",
    "2. rag_keywords åŒ…å«èº«ä½“éƒ¨ä½å…³é”®è¯\n",
    "3. rag_keywords åŒ…å«ä¸»è¦ç—‡çŠ¶å…³é”®è¯\n",
    "4. æœ¯è¯­ä½¿ç”¨ä¸“ä¸šå‡†ç¡®\n",
    "5. ä¿¡æ¯å®Œæ•´ï¼Œæ— é—æ¼é‡è¦å†…å®¹\n",
    "\n",
    "è¯·ç”Ÿæˆå„ç§è´¨é‡ç­‰çº§ï¼ˆå¥½/ä¸­/å·®ï¼‰çš„æ ·æœ¬ã€‚\"\"\",\n",
    "\n",
    "        \"user_template\": \"\"\"è¯·ç”Ÿæˆ {batch_size} æ¡\"è´¨é‡è¯„åˆ†\"è®­ç»ƒæ•°æ®ã€‚\n",
    "\n",
    "æ¯æ¡æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š\n",
    "{{\n",
    "  \"input\": {{\n",
    "    \"optimized_symptoms\": \"ç—‡çŠ¶æè¿°\",\n",
    "    \"rag_keywords\": [\"å…³é”®è¯1\", \"å…³é”®è¯2\"]\n",
    "  }},\n",
    "  \"output\": {{\n",
    "    \"score\": 0-5,\n",
    "    \"comment\": \"è¯„ä»·æ„è§\",\n",
    "    \"isValid\": true/false\n",
    "  }}\n",
    "}}\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "1. ç”Ÿæˆä¸åŒè´¨é‡ç­‰çº§çš„æ ·æœ¬ï¼ˆå·®:0-1åˆ†, ä¸­:2-3åˆ†, å¥½:4-5åˆ†ï¼‰ï¼Œæ¯”ä¾‹çº¦ 2:3:5\n",
    "2. comment è¦å…·ä½“è¯´æ˜æ‰£åˆ†åŸå› æˆ–ä¼˜ç‚¹\n",
    "3. score < 3 æ—¶ isValid ä¸º false\n",
    "4. ç›´æ¥è¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŠ ä»»ä½•è§£é‡Šæ–‡å­—æˆ– markdown æ ‡è®°\n",
    "\n",
    "è¯·è¿”å›ä¸€ä¸ªåŒ…å« {batch_size} æ¡æ•°æ®çš„ JSON æ•°ç»„ï¼š\"\"\"\n",
    "    },\n",
    "\n",
    "    # Agent 3: RAG è¯„å®¡ (checkRAG)\n",
    "    \"check_rag\": {\n",
    "        \"name\": \"RAGè¯„å®¡ (checkRAG)\",\n",
    "        \"system\": \"\"\"ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ•°æ®ç”ŸæˆåŠ©æ‰‹ã€‚ä½ éœ€è¦ç”Ÿæˆç”¨äºè®­ç»ƒ\"RAGå†…å®¹è¯„å®¡\"æ¨¡å‹çš„æ•°æ®ã€‚\n",
    "\n",
    "ä»»åŠ¡è¯´æ˜ï¼š\n",
    "- è¾“å…¥ï¼šç”¨æˆ·ç—‡çŠ¶ (optimized_symptoms) å’Œ RAG æ£€ç´¢ç»“æœ (ragContext)\n",
    "- è¾“å‡ºï¼šç›¸å…³æ€§è¯„åˆ† (ragScore: 0-5) å’Œè¯„ä»·æ„è§ (ragComment)\n",
    "\n",
    "è¯„åˆ†æ ‡å‡†ï¼ˆæ¯é¡¹1åˆ†ï¼‰ï¼š\n",
    "1. RAG å†…å®¹ä¸ç—‡çŠ¶ç›¸å…³\n",
    "2. åŒ…å«æœ‰ç”¨çš„åŒ»å­¦èƒŒæ™¯çŸ¥è¯†\n",
    "3. ä¿¡æ¯å‡†ç¡®æ— æ˜æ˜¾é”™è¯¯\n",
    "4. å¯¹è¯Šæ–­æœ‰å‚è€ƒä»·å€¼\n",
    "5. å†…å®¹è¡¨è¿°æ¸…æ™°\n",
    "\n",
    "è¯·ç”Ÿæˆä¸åŒç›¸å…³åº¦çš„æ ·æœ¬ã€‚\"\"\",\n",
    "\n",
    "        \"user_template\": \"\"\"è¯·ç”Ÿæˆ {batch_size} æ¡\"RAGè¯„å®¡\"è®­ç»ƒæ•°æ®ã€‚\n",
    "\n",
    "æ¯æ¡æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š\n",
    "{{\n",
    "  \"input\": {{\n",
    "    \"optimized_symptoms\": \"æ‚£è€…ç—‡çŠ¶æè¿°\",\n",
    "    \"ragContext\": \"RAGæ£€ç´¢è¿”å›çš„åŒ»å­¦èƒŒæ™¯çŸ¥è¯†ï¼ˆå¯ä»¥æ˜¯ç›¸å…³çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸å¤ªç›¸å…³çš„ï¼‰\"\n",
    "  }},\n",
    "  \"output\": {{\n",
    "    \"ragScore\": 0-5,\n",
    "    \"ragComment\": \"è¯„ä»·æ„è§\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "1. ç”Ÿæˆä¸åŒç›¸å…³åº¦çš„æ ·æœ¬ï¼ˆä½:0-2, ä¸­:3, é«˜:4-5ï¼‰ï¼Œæ¯”ä¾‹çº¦ 2:3:5\n",
    "2. ragContext è¦æ¨¡æ‹ŸçœŸå®çš„åŒ»å­¦çŸ¥è¯†åº“æ£€ç´¢ç»“æœ\n",
    "3. åŒ…å«ä¸€äº›ä¸å¤ªç›¸å…³çš„è´Ÿæ ·æœ¬\n",
    "4. ç›´æ¥è¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŠ ä»»ä½•è§£é‡Šæ–‡å­—æˆ– markdown æ ‡è®°\n",
    "\n",
    "è¯·è¿”å›ä¸€ä¸ªåŒ…å« {batch_size} æ¡æ•°æ®çš„ JSON æ•°ç»„ï¼š\"\"\"\n",
    "    },\n",
    "\n",
    "    # Agent 4: è¯Šæ–­ç”Ÿæˆ (generateDiagnosis) - æ ¸å¿ƒä»»åŠ¡\n",
    "    \"diagnosis\": {\n",
    "        \"name\": \"è¯Šæ–­ç”Ÿæˆ (generateDiagnosis)\",\n",
    "        \"system\": \"\"\"ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ•°æ®ç”ŸæˆåŠ©æ‰‹ã€‚ä½ éœ€è¦ç”Ÿæˆç”¨äºè®­ç»ƒ\"è¯Šæ–­ç”Ÿæˆ\"æ¨¡å‹çš„æ•°æ®ã€‚\n",
    "\n",
    "ä»»åŠ¡è¯´æ˜ï¼š\n",
    "- è¾“å…¥ï¼šæ‚£è€…ç—‡çŠ¶ (optimized_symptoms) å’ŒåŒ»å­¦èƒŒæ™¯ (ragContext)\n",
    "- è¾“å‡ºï¼šè¯Šæ–­ç»“æœï¼ˆ3ä¸ªå¯èƒ½ç–¾ç—…åŠæ¦‚ç‡ï¼‰ã€å¥åº·å»ºè®®ã€ç®€åŒ–å»ºè®®\n",
    "\n",
    "è¾“å‡ºæ ¼å¼ï¼š\n",
    "- results: 3ä¸ªè¯Šæ–­ç»“æœï¼Œæ¯ä¸ªåŒ…å« condition(ç–¾ç—…å), probability(æ¦‚ç‡), description(æè¿°)\n",
    "- recommendations: 3æ¡è¯¦ç»†å¥åº·å»ºè®®ï¼ˆæ¯æ¡>=15å­—ï¼‰\n",
    "- recomm_short: 10æ¡ç®€åŒ–å»ºè®®ï¼ˆæ¯æ¡<=10å­—ï¼‰\n",
    "\n",
    "è¯·ç¡®ä¿è¯Šæ–­ç»“æœç¬¦åˆåŒ»å­¦å¸¸è¯†ã€‚\"\"\",\n",
    "\n",
    "        \"user_template\": \"\"\"è¯·ç”Ÿæˆ {batch_size} æ¡\"è¯Šæ–­ç”Ÿæˆ\"è®­ç»ƒæ•°æ®ã€‚\n",
    "\n",
    "æ¯æ¡æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š\n",
    "{{\n",
    "  \"input\": {{\n",
    "    \"optimized_symptoms\": \"æ‚£è€…çš„è¯¦ç»†ç—‡çŠ¶æè¿°\",\n",
    "    \"ragContext\": \"ç›¸å…³çš„åŒ»å­¦èƒŒæ™¯çŸ¥è¯†\"\n",
    "  }},\n",
    "  \"output\": {{\n",
    "    \"results\": [\n",
    "      {{\"condition\": \"ç–¾ç—…1\", \"probability\": 0.xx, \"description\": \"ç–¾ç—…æè¿°\"}},\n",
    "      {{\"condition\": \"ç–¾ç—…2\", \"probability\": 0.yy, \"description\": \"ç–¾ç—…æè¿°\"}},\n",
    "      {{\"condition\": \"ç–¾ç—…3\", \"probability\": 0.zz, \"description\": \"ç–¾ç—…æè¿°\"}}\n",
    "    ],\n",
    "    \"recommendations\": [\"è¯¦ç»†å»ºè®®1(>=15å­—)\", \"è¯¦ç»†å»ºè®®2\", \"è¯¦ç»†å»ºè®®3\"],\n",
    "    \"recomm_short\": [\"ç®€åŒ–1\", \"ç®€åŒ–2\", \"ç®€åŒ–3\", \"ç®€åŒ–4\", \"ç®€åŒ–5\", \"ç®€åŒ–6\", \"ç®€åŒ–7\", \"ç®€åŒ–8\", \"ç®€åŒ–9\", \"ç®€åŒ–10\"]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "1. è¯Šæ–­ç»“æœè¦ç¬¦åˆåŒ»å­¦å¸¸è¯†ï¼Œæ¦‚ç‡ä¹‹å’Œçº¦ä¸º1\n",
    "2. æ¶µç›–å¸¸è§ç—…å’Œä¸€äº›ç‰¹æ®Šæƒ…å†µ\n",
    "3. å»ºè®®è¦å…·ä½“å¯æ“ä½œ\n",
    "4. ç›´æ¥è¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŠ ä»»ä½•è§£é‡Šæ–‡å­—æˆ– markdown æ ‡è®°\n",
    "\n",
    "è¯·è¿”å›ä¸€ä¸ªåŒ…å« {batch_size} æ¡æ•°æ®çš„ JSON æ•°ç»„ï¼š\"\"\"\n",
    "    },\n",
    "\n",
    "    # Agent 5: ç”¨è¯å»ºè®® (generateDrugRecommendations)\n",
    "    \"drug\": {\n",
    "        \"name\": \"ç”¨è¯å»ºè®® (generateDrugRecommendations)\",\n",
    "        \"system\": \"\"\"ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ•°æ®ç”ŸæˆåŠ©æ‰‹ã€‚ä½ éœ€è¦ç”Ÿæˆç”¨äºè®­ç»ƒ\"ç”¨è¯å»ºè®®\"æ¨¡å‹çš„æ•°æ®ã€‚\n",
    "\n",
    "ä»»åŠ¡è¯´æ˜ï¼š\n",
    "- è¾“å…¥ï¼šè¯Šæ–­ç»“æœ (condition) å’Œè¯ç‰©çŸ¥è¯†åº“å†…å®¹ (drugRagContext)\n",
    "- è¾“å‡ºï¼šç”¨è¯å»ºè®®åˆ—è¡¨\n",
    "\n",
    "è¾“å‡ºæ ¼å¼ï¼š\n",
    "æ¯ä¸ªè¯ç‰©åŒ…å«ï¼šname(è¯å), usage(ç”¨æ³•ç”¨é‡), notes(æ³¨æ„äº‹é¡¹)\n",
    "\n",
    "è¯·ç¡®ä¿ç”¨è¯å»ºè®®ç¬¦åˆåŒ»å­¦è§„èŒƒã€‚\"\"\",\n",
    "\n",
    "        \"user_template\": \"\"\"è¯·ç”Ÿæˆ {batch_size} æ¡\"ç”¨è¯å»ºè®®\"è®­ç»ƒæ•°æ®ã€‚\n",
    "\n",
    "æ¯æ¡æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š\n",
    "{{\n",
    "  \"input\": {{\n",
    "    \"condition\": \"è¯Šæ–­çš„ç–¾ç—…åç§°\",\n",
    "    \"drugRagContext\": \"è¯ç‰©çŸ¥è¯†åº“æ£€ç´¢ç»“æœ\"\n",
    "  }},\n",
    "  \"output\": {{\n",
    "    \"drugs\": [{{\n",
    "      \"condition\": \"ç–¾ç—…åç§°\",\n",
    "      \"recommended_drugs\": [\n",
    "        {{\"name\": \"è¯å“å\", \"usage\": \"ç”¨æ³•ç”¨é‡\", \"notes\": \"æ³¨æ„äº‹é¡¹\"}},\n",
    "        {{\"name\": \"è¯å“å2\", \"usage\": \"ç”¨æ³•ç”¨é‡\", \"notes\": \"æ³¨æ„äº‹é¡¹\"}}\n",
    "      ]\n",
    "    }}]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "1. è¯ç‰©åç§°ä½¿ç”¨æ ‡å‡†å\n",
    "2. ç”¨æ³•ç”¨é‡è¦å…·ä½“\n",
    "3. æ³¨æ„äº‹é¡¹è¦åŒ…å«ç¦å¿Œç—‡\n",
    "4. ç›´æ¥è¿”å› JSON æ•°ç»„ï¼Œä¸è¦åŠ ä»»ä½•è§£é‡Šæ–‡å­—æˆ– markdown æ ‡è®°\n",
    "\n",
    "è¯·è¿”å›ä¸€ä¸ªåŒ…å« {batch_size} æ¡æ•°æ®çš„ JSON æ•°ç»„ï¼š\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"âœ… å·²åŠ è½½ {len(PROMPTS)} ä¸ª Agent çš„ Prompt æ¨¡æ¿\")\n",
    "for key, value in PROMPTS.items():\n",
    "    print(f\"   - {key}: {value['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ•°æ®ç”Ÿæˆå‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini ä»·æ ¼ (per 1M tokens)\n",
    "INPUT_PRICE = 0.15 / 1_000_000\n",
    "OUTPUT_PRICE = 0.60 / 1_000_000\n",
    "\n",
    "# ç»Ÿè®¡å˜é‡\n",
    "total_tokens = 0\n",
    "total_cost = 0.0\n",
    "\n",
    "def generate_batch(agent: str, batch_size: int = 10) -> List[Dict]:\n",
    "    \"\"\"ç”Ÿæˆä¸€æ‰¹æ•°æ®\"\"\"\n",
    "    global total_tokens, total_cost\n",
    "    \n",
    "    if agent not in PROMPTS:\n",
    "        raise ValueError(f\"Unknown agent: {agent}\")\n",
    "    \n",
    "    prompt_config = PROMPTS[agent]\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt_config[\"system\"]},\n",
    "                {\"role\": \"user\", \"content\": prompt_config[\"user_template\"].format(batch_size=batch_size)}\n",
    "            ],\n",
    "            temperature=0.8,\n",
    "            max_tokens=4000,\n",
    "        )\n",
    "        \n",
    "        # ç»Ÿè®¡ token ä½¿ç”¨\n",
    "        usage = response.usage\n",
    "        input_tokens = usage.prompt_tokens\n",
    "        output_tokens = usage.completion_tokens\n",
    "        total_tokens += input_tokens + output_tokens\n",
    "        \n",
    "        # è®¡ç®—æˆæœ¬\n",
    "        cost = input_tokens * INPUT_PRICE + output_tokens * OUTPUT_PRICE\n",
    "        total_cost += cost\n",
    "        \n",
    "        # è§£æè¿”å›çš„ JSON\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # å°è¯•æå– JSON æ•°ç»„\n",
    "        if content.startswith(\"```\"):\n",
    "            content = content.split(\"```\")[1]\n",
    "            if content.startswith(\"json\"):\n",
    "                content = content[4:]\n",
    "        \n",
    "        data = json.loads(content)\n",
    "        return data if isinstance(data, list) else [data]\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âš ï¸ JSON è§£æå¤±è´¥: {e}\")\n",
    "        print(f\"   åŸå§‹å†…å®¹: {content[:200]}...\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ API è°ƒç”¨å¤±è´¥: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def build_human_prompt(input_data: Dict, agent: str) -> str:\n",
    "    \"\"\"æ ¹æ® agent ç±»å‹æ„å»ºç”¨æˆ·è¾“å…¥\"\"\"\n",
    "    if agent == \"preprocess\":\n",
    "        return f\"\"\"è¯·å°†ä»¥ä¸‹ç—‡çŠ¶ä¿¡æ¯è½¬æ¢ä¸ºç»“æ„åŒ–çš„åŒ»å­¦æè¿°ï¼š\n",
    "èº«ä½“éƒ¨ä½: {input_data.get('body_part', '')}\n",
    "ä¸»è¦ç—‡çŠ¶: {', '.join(input_data.get('symptoms', []))}\n",
    "å…¶ä»–ç—‡çŠ¶: {input_data.get('other_symptoms', '')}\n",
    "ä¸¥é‡ç¨‹åº¦: {input_data.get('severity', 3)}\n",
    "æŒç»­æ—¶é—´: {input_data.get('duration', '')}\n",
    "\n",
    "è¯·è¾“å‡º JSON æ ¼å¼ï¼š{{\"optimized_symptoms\": \"...\", \"rag_keywords\": [...]}}\"\"\"\n",
    "\n",
    "    elif agent == \"critic\":\n",
    "        return f\"\"\"è¯·å¯¹ä»¥ä¸‹ç—‡çŠ¶ç»“æ„åŒ–ç»“æœè¿›è¡Œè¯„åˆ†ï¼š\n",
    "{json.dumps(input_data, ensure_ascii=False, indent=2)}\n",
    "\n",
    "è¯·è¾“å‡º JSON æ ¼å¼ï¼š{{\"score\": 0-5, \"comment\": \"...\", \"isValid\": true/false}}\"\"\"\n",
    "\n",
    "    elif agent == \"check_rag\":\n",
    "        return f\"\"\"è¯·è¯„ä¼° RAG è¿”å›å†…å®¹ä¸ç—‡çŠ¶çš„ç›¸å…³æ€§ï¼š\n",
    "ã€ç—‡çŠ¶ã€‘{input_data.get('optimized_symptoms', '')}\n",
    "ã€RAGå†…å®¹ã€‘{input_data.get('ragContext', '')}\n",
    "\n",
    "è¯·è¾“å‡º JSON æ ¼å¼ï¼š{{\"ragScore\": 0-5, \"ragComment\": \"...\"}}\"\"\"\n",
    "\n",
    "    elif agent == \"diagnosis\":\n",
    "        return f\"\"\"è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆè¯Šæ–­ç»“æœï¼š\n",
    "ã€ç—‡çŠ¶ã€‘{input_data.get('optimized_symptoms', '')}\n",
    "ã€åŒ»å­¦èƒŒæ™¯ã€‘{input_data.get('ragContext', '')}\n",
    "\n",
    "è¯·è¾“å‡ºè¯Šæ–­ JSONã€‚\"\"\"\n",
    "\n",
    "    elif agent == \"drug\":\n",
    "        return f\"\"\"è¯·ä¸ºä»¥ä¸‹ç–¾ç—…ç”Ÿæˆç”¨è¯å»ºè®®ï¼š\n",
    "ã€è¯Šæ–­ã€‘{input_data.get('condition', '')}\n",
    "ã€è¯ç‰©çŸ¥è¯†ã€‘{input_data.get('drugRagContext', '')}\n",
    "\n",
    "è¯·è¾“å‡ºç”¨è¯å»ºè®® JSONã€‚\"\"\"\n",
    "\n",
    "    return json.dumps(input_data, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def convert_to_training_format(data: List[Dict], agent: str) -> List[Dict]:\n",
    "    \"\"\"è½¬æ¢ä¸º SFT è®­ç»ƒæ ¼å¼\"\"\"\n",
    "    training_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        if \"input\" not in item or \"output\" not in item:\n",
    "            continue\n",
    "        \n",
    "        human_content = build_human_prompt(item[\"input\"], agent)\n",
    "        gpt_content = json.dumps(item[\"output\"], ensure_ascii=False)\n",
    "        \n",
    "        training_item = {\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": human_content},\n",
    "                {\"from\": \"gpt\", \"value\": gpt_content}\n",
    "            ]\n",
    "        }\n",
    "        training_data.append(training_item)\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "\n",
    "def save_jsonl(data: List[Dict], filepath: str):\n",
    "    \"\"\"ä¿å­˜ä¸º JSONL æ ¼å¼\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else \".\", exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    print(f\"âœ… å·²ä¿å­˜åˆ°: {filepath}\")\n",
    "\n",
    "\n",
    "print(\"âœ… å‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ğŸ§ª æµ‹è¯•é˜¶æ®µï¼šç”Ÿæˆ 100 æ¡æ•°æ®\n",
    "\n",
    "å…ˆç”Ÿæˆå°‘é‡æ•°æ®ï¼ŒæŸ¥çœ‹ï¼š\n",
    "1. æ•°æ®è´¨é‡\n",
    "2. æˆæœ¬ä¼°ç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ æµ‹è¯•é…ç½® ============\n",
    "\n",
    "# é€‰æ‹©è¦æµ‹è¯•çš„ Agent\n",
    "TEST_AGENT = \"preprocess\"  # å¯é€‰: preprocess, critic, check_rag, diagnosis, drug\n",
    "\n",
    "# æµ‹è¯•æ•°é‡\n",
    "TEST_NUM_SAMPLES = 100\n",
    "\n",
    "# æ¯æ‰¹ç”Ÿæˆæ•°é‡ï¼ˆå»ºè®® 5-10ï¼‰\n",
    "TEST_BATCH_SIZE = 10\n",
    "\n",
    "# ================================\n",
    "\n",
    "print(f\"ğŸ“‹ æµ‹è¯•é…ç½®:\")\n",
    "print(f\"   Agent: {TEST_AGENT} ({PROMPTS[TEST_AGENT]['name']})\")\n",
    "print(f\"   æ•°é‡: {TEST_NUM_SAMPLES} æ¡\")\n",
    "print(f\"   æ‰¹æ¬¡å¤§å°: {TEST_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡ç½®ç»Ÿè®¡\n",
    "total_tokens = 0\n",
    "total_cost = 0.0\n",
    "\n",
    "# å¼€å§‹ç”Ÿæˆ\n",
    "print(f\"\\nğŸš€ å¼€å§‹ç”Ÿæˆ {TEST_AGENT} æµ‹è¯•æ•°æ®...\\n\")\n",
    "\n",
    "all_data = []\n",
    "num_batches = (TEST_NUM_SAMPLES + TEST_BATCH_SIZE - 1) // TEST_BATCH_SIZE\n",
    "\n",
    "for i in tqdm(range(num_batches), desc=\"ç”Ÿæˆä¸­\"):\n",
    "    current_batch_size = min(TEST_BATCH_SIZE, TEST_NUM_SAMPLES - len(all_data))\n",
    "    batch_data = generate_batch(TEST_AGENT, current_batch_size)\n",
    "    all_data.extend(batch_data)\n",
    "    \n",
    "    # é¿å… rate limit\n",
    "    if i < num_batches - 1:\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print(f\"\\nâœ… ç”Ÿæˆå®Œæˆ: {len(all_data)} æ¡æ•°æ®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹æˆæœ¬ç»Ÿè®¡\n",
    "print(\"=\"*50)\n",
    "print(\"ğŸ“Š æˆæœ¬ç»Ÿè®¡\")\n",
    "print(\"=\"*50)\n",
    "print(f\"æ€» Token æ•°: {total_tokens:,}\")\n",
    "print(f\"æµ‹è¯•æˆæœ¬: ${total_cost:.4f}\")\n",
    "print(f\"æ¯æ¡æˆæœ¬: ${total_cost/max(len(all_data),1):.6f}\")\n",
    "print()\n",
    "print(\"ğŸ“ˆ æˆæœ¬é¢„ä¼°:\")\n",
    "cost_per_sample = total_cost / max(len(all_data), 1)\n",
    "print(f\"   1,000 æ¡: ${cost_per_sample * 1000:.2f}\")\n",
    "print(f\"   5,000 æ¡: ${cost_per_sample * 5000:.2f}\")\n",
    "print(f\"  10,000 æ¡: ${cost_per_sample * 10000:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹ç”Ÿæˆçš„æ•°æ®æ ·ä¾‹\n",
    "print(\"ğŸ“ æ•°æ®æ ·ä¾‹ (å‰3æ¡):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, item in enumerate(all_data[:3]):\n",
    "    print(f\"\\n--- æ ·ä¾‹ {i+1} ---\")\n",
    "    print(json.dumps(item, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æµ‹è¯•æ•°æ®\n",
    "test_output_file = f\"{OUTPUT_DIR}/{TEST_AGENT}_test_100.jsonl\"\n",
    "\n",
    "# è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼\n",
    "training_data = convert_to_training_format(all_data, TEST_AGENT)\n",
    "\n",
    "# ä¿å­˜\n",
    "save_jsonl(training_data, test_output_file)\n",
    "\n",
    "print(f\"\\nğŸ“„ è®­ç»ƒæ ¼å¼æ ·ä¾‹:\")\n",
    "print(json.dumps(training_data[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. âœ… ç¡®è®¤åï¼šæ‰¹é‡ç”Ÿæˆå®Œæ•´æ•°æ®é›†\n",
    "\n",
    "å¦‚æœæµ‹è¯•æ•°æ®è´¨é‡æ»¡æ„ï¼Œæˆæœ¬å¯æ¥å—ï¼Œè¿è¡Œä»¥ä¸‹ä»£ç ç”Ÿæˆå®Œæ•´æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ æ­£å¼ç”Ÿæˆé…ç½® ============\n",
    "\n",
    "# é€‰æ‹©è¦ç”Ÿæˆçš„ Agentï¼ˆå¯ä»¥æ˜¯åˆ—è¡¨ï¼‰\n",
    "AGENTS_TO_GENERATE = [\"preprocess\"]  # å¯æ”¹ä¸º [\"preprocess\", \"critic\", \"diagnosis\"] ç­‰\n",
    "\n",
    "# æ¯ä¸ª Agent ç”Ÿæˆçš„æ•°é‡\n",
    "NUM_SAMPLES = 1000  # å»ºè®®å…ˆä» 1000 å¼€å§‹\n",
    "\n",
    "# æ¯æ‰¹ç”Ÿæˆæ•°é‡\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# ====================================\n",
    "\n",
    "print(f\"ğŸ“‹ æ­£å¼ç”Ÿæˆé…ç½®:\")\n",
    "print(f\"   Agents: {AGENTS_TO_GENERATE}\")\n",
    "print(f\"   æ¯ä¸ª Agent: {NUM_SAMPLES} æ¡\")\n",
    "print(f\"   é¢„ä¼°æ€»æˆæœ¬: ${cost_per_sample * NUM_SAMPLES * len(AGENTS_TO_GENERATE):.2f}\")\n",
    "print()\n",
    "print(\"âš ï¸  ç¡®è®¤æ— è¯¯åï¼Œè¿è¡Œä¸‹ä¸€ä¸ªå•å…ƒæ ¼å¼€å§‹ç”Ÿæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ å¼€å§‹æ­£å¼ç”Ÿæˆ\n",
    "\n",
    "for agent in AGENTS_TO_GENERATE:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ğŸš€ ç”Ÿæˆ {agent} ({PROMPTS[agent]['name']}) æ•°æ®é›†\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # é‡ç½®ç»Ÿè®¡\n",
    "    total_tokens = 0\n",
    "    total_cost = 0.0\n",
    "    \n",
    "    # ç”Ÿæˆæ•°æ®\n",
    "    all_data = []\n",
    "    num_batches = (NUM_SAMPLES + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    for i in tqdm(range(num_batches), desc=f\"ç”Ÿæˆ {agent}\"):\n",
    "        current_batch_size = min(BATCH_SIZE, NUM_SAMPLES - len(all_data))\n",
    "        batch_data = generate_batch(agent, current_batch_size)\n",
    "        all_data.extend(batch_data)\n",
    "        \n",
    "        if i < num_batches - 1:\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    # è½¬æ¢å¹¶ä¿å­˜\n",
    "    training_data = convert_to_training_format(all_data, agent)\n",
    "    output_file = f\"{OUTPUT_DIR}/{agent}_sft_data.jsonl\"\n",
    "    save_jsonl(training_data, output_file)\n",
    "    \n",
    "    # ç»Ÿè®¡\n",
    "    print(f\"\\nğŸ“Š {agent} ç»Ÿè®¡:\")\n",
    "    print(f\"   ç”Ÿæˆæ•°é‡: {len(training_data)} æ¡\")\n",
    "    print(f\"   æ€» Token: {total_tokens:,}\")\n",
    "    print(f\"   æˆæœ¬: ${total_cost:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"âœ… æ‰€æœ‰æ•°æ®ç”Ÿæˆå®Œæˆï¼\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æ•°æ®éªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶\n",
    "import glob\n",
    "\n",
    "jsonl_files = glob.glob(f\"{OUTPUT_DIR}/*.jsonl\")\n",
    "\n",
    "print(\"ğŸ“ ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for filepath in jsonl_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"   {os.path.basename(filepath)}: {len(lines)} æ¡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éšæœºæŸ¥çœ‹ä¸€æ¡æ•°æ®\n",
    "import random\n",
    "\n",
    "# é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶\n",
    "if jsonl_files:\n",
    "    selected_file = jsonl_files[0]\n",
    "    print(f\"ğŸ“„ éšæœºæŸ¥çœ‹ {os.path.basename(selected_file)} ä¸­çš„ä¸€æ¡æ•°æ®:\\n\")\n",
    "    \n",
    "    with open(selected_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    random_line = random.choice(lines)\n",
    "    data = json.loads(random_line)\n",
    "    print(json.dumps(data, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ä¸‹ä¸€æ­¥ï¼šè®­ç»ƒæ¨¡å‹\n",
    "\n",
    "æ•°æ®ç”Ÿæˆå®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®­ç»ƒæ¨¡å‹ï¼š\n",
    "\n",
    "```bash\n",
    "cd ../../MyMedicalGPT\n",
    "\n",
    "python supervised_finetuning.py \\\n",
    "    --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \\\n",
    "    --train_file_dir ../IntelHealth/data_clean/sft/ \\\n",
    "    --output_dir outputs/loras/preprocess-agent \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --use_lora True \\\n",
    "    --lora_rank 64 \\\n",
    "    --lora_alpha 128\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
